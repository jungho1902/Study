# 전이 학습 및 미세 조정 (Transfer Learning & Fine-tuning)

**전이 학습(Transfer Learning)**은 특정 문제(Task A)를 해결하기 위해 학습된 모델의 지식(가중치 등)을 다른 관련 문제(Task B)를 해결하는 데 재사용하는 머신러닝 기법입니다. 특히 컴퓨터 비전 분야에서는 대규모 데이터셋(예: ImageNet)으로 사전 훈련된(pre-trained) 모델을 기반으로, 더 적은 데이터셋을 가진 특정 과제에 적용할 때 매우 효과적입니다.

**미세 조정(Fine-tuning)**은 전이 학습의 구체적인 구현 방법 중 하나로, 사전 훈련된 모델의 일부 또는 전체 가중치를 새로운 데이터셋에 맞게 조금 더 학습(재조정)하는 과정을 의미합니다.

## 1. 왜 전이 학습을 사용하는가?

- **데이터 부족 문제 해결:** 처음부터 딥러닝 모델을 학습시키려면 수십만 장 이상의 대규모 데이터셋이 필요하지만, 대부분의 실제 문제에서는 이 정도의 데이터를 구하기 어렵습니다. 전이 학습을 사용하면 상대적으로 적은 데이터로도 높은 성능을 달성할 수 있습니다.
- **학습 시간 단축:** 사전 훈련된 모델은 이미 이미지의 기본적인 특징(선, 질감, 형태 등)을 학습한 상태이므로, 새로운 과제를 훨씬 빠르게 학습할 수 있습니다.
- **성능 향상:** 잘 훈련된 모델의 지식을 활용하므로, 작은 데이터셋으로 밑바닥부터 학습(from scratch)하는 것보다 일반적으로 더 높은 성능을 보입니다.

## 2. 전이 학습의 주요 접근법

사전 훈련된 모델(주로 CNN)은 보통 두 부분으로 나뉩니다.
- **특징 추출기 (Feature Extractor):** 이미지의 저수준(low-level) 특징부터 고수준(high-level) 특징까지 추출하는 합성곱 레이어(Convolutional Layers) 부분.
- **분류기 (Classifier):** 추출된 특징을 바탕으로 이미지를 분류하는 완전 연결 레이어(Fully-Connected Layers) 부분.

새로운 데이터셋의 크기와 사전 훈련에 사용된 데이터셋과의 유사도에 따라 다음과 같은 전략을 선택할 수 있습니다.

### 2.1. 특징 추출기로 사용 (Feature Extraction)

- **상황:** 새로운 데이터셋이 작고, 사전 훈련 데이터셋과 유사할 때.
- **방법:**
  1. 사전 훈련된 모델의 합성곱 부분(특징 추출기)은 **고정(freeze)**시킵니다. 즉, 가중치를 업데이트하지 않습니다.
  2. 모델의 분류기 부분만 새로운 과제에 맞는 분류기로 교체합니다.
  3. 교체된 분류기 부분만 새로운 데이터셋으로 학습시킵니다.
- **장점:** 학습해야 할 파라미터가 적어 매우 빠르고, 과대적합의 위험이 적습니다.

### 2.2. 미세 조정 (Fine-tuning)

- **상황:** 새로운 데이터셋이 충분히 크고, 사전 훈련 데이터셋과 유사할 때.
- **방법:**
  1. 사전 훈련된 모델의 가중치로 초기화합니다.
  2. 모델의 분류기 부분을 새로운 과제에 맞게 교체합니다.
  3. 모델의 **전체 또는 일부 레이어**를 새로운 데이터셋으로 **재학습**시킵니다. 이때, 보통 매우 작은 학습률(learning rate)을 사용합니다.
- **세부 전략:**
  - **전체 모델 미세 조정:** 모델의 모든 레이어를 재학습시킵니다. 데이터가 충분할 때 효과적입니다.
  - **일부 레이어만 미세 조정:** 모델의 하위 레이어(입력에 가까운)는 일반적인 특징을 학습하므로 고정시키고, 상위 레이어(출력에 가까운)만 재학습시킵니다. 이는 데이터가 아주 많지는 않을 때 과대적합을 방지하는 데 도움이 됩니다.

## 3. 컴퓨터 비전의 주요 사전 훈련 모델

전이 학습에 널리 사용되는 ImageNet 사전 훈련 모델들은 다음과 같습니다.

- **VGGNet (VGG16, VGG19):** 구조가 간단하여 이해하기 쉽지만, 파라미터 수가 많아 무겁습니다.
- **ResNet (ResNet50, ResNet101):** 잔차 연결(Residual Connection)을 도입하여 깊은 신경망의 학습을 가능하게 했으며, 가장 널리 사용되는 모델 중 하나입니다.
- **Inception (GoogLeNet):** 다양한 크기의 필터를 병렬로 사용하여 효율적인 특징 추출을 수행합니다.
- **EfficientNet:** 모델의 깊이(depth), 너비(width), 해상도(resolution)를 균형 있게 조절하여 정확도와 효율성을 모두 높인 모델입니다.
- **Vision Transformer (ViT):** NLP 분야의 트랜스포머를 이미지 처리에 적용한 모델로, 대규모 데이터셋에서 CNN을 능가하는 성능을 보입니다.

## 4. 미세 조정 실전 팁

- **데이터셋 크기를 고려하세요:** 데이터가 적을수록 더 적은 레이어를 미세 조정하거나 특징 추출 방식으로 접근하는 것이 좋습니다.
- **학습률을 낮게 설정하세요:** 사전 훈련된 가중치는 이미 좋은 지점을 알고 있으므로, 큰 폭으로 변경되지 않도록 작은 학습률(e.g., 1e-4, 1e-5)을 사용해야 합니다.
- **점진적으로 동결을 해제하세요 (Progressive Unfreezing):** 처음에는 분류기만 학습시키고, 그 다음 상위 몇 개의 합성곱 레이어를 학습시키는 식으로 점진적으로 학습 범위를 넓혀나가는 것도 좋은 전략입니다.
