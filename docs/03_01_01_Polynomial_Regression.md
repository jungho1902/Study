# 3.1.1. 다항 회귀 (Polynomial Regression)

**다항 회귀(Polynomial Regression)**는 독립 변수와 종속 변수 간의 관계가 **비선형(non-linear)**일 때 사용하는 회귀 분석 기법입니다. 선형 회귀가 데이터를 가장 잘 설명하는 '직선'을 찾는 것이라면, 다항 회귀는 '곡선'을 찾아 데이터의 복잡한 패턴을 모델링합니다.

## 1. 다항 회귀의 필요성

현실 세계의 많은 데이터는 단순한 직선 관계로 설명하기 어렵습니다. 예를 들어, '온도'에 따른 '아이스크림 판매량'은 특정 온도까지 증가하다가 너무 더워지면 오히려 감소하는 곡선 형태를 띨 수 있습니다.

이러한 비선형 관계를 선형 회귀로 모델링하면, 모델의 예측 성능이 매우 낮아지는 **과소적합(Underfitting)** 문제가 발생합니다. 다항 회귀는 이러한 한계를 극복하기 위해 사용됩니다.

## 2. 다항 회귀의 원리

다항 회귀는 기존의 독립 변수(feature)를 **다항식으로 변환**하여 새로운 특성을 만들어내고, 이 새로운 특성들을 입력으로 하는 **선형 회귀 모델**을 만드는 방식입니다.

예를 들어, 독립 변수 `x`가 하나인 단순 선형 회귀의 가설 함수는 다음과 같습니다.
> **H(x) = Wx + b**

이 모델을 2차 다항 회귀로 확장하면, 기존의 `x` 외에 `x²`이라는 새로운 특성을 추가합니다.
> **H(x) = W₂x² + W₁x + b**

여기서 `x`를 `x₁`, `x²`을 `x₂`로 치환하면, 이 식은 `x₁`과 `x₂`라는 두 개의 특성을 가진 **다중 선형 회귀** 문제와 동일해집니다. 즉, 다항 회귀는 특성을 다항식으로 변환할 뿐, 본질적으로는 선형 회귀 알고리즘을 사용합니다.

- **1차 다항식 (선형 회귀):** `y = ax + b`
- **2차 다항식:** `y = ax² + bx + c`
- **3차 다항식:** `y = ax³ + bx² + cx + d`

차수(degree)를 높일수록 더 복잡한 형태의 곡선을 만들 수 있습니다.

## 3. 다항 회귀 구현

다항 회귀는 보통 다음과 같은 순서로 구현됩니다.
1. **특성 변환**: 주어진 독립 변수 `x`를 원하는 차수의 다항 특성(`x`, `x²`, `x³`, ...)으로 변환합니다. `scikit-learn`에서는 `PolynomialFeatures` 클래스를 사용하여 이 과정을 쉽게 처리할 수 있습니다.
2. **선형 회귀 모델 학습**: 변환된 다항 특성을 입력으로 하여 선형 회귀 모델(`LinearRegression`)을 학습시킵니다.

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import numpy as np

# 1. 데이터 생성 (비선형 관계)
X = np.array([[1], [2], [3], [4]])
y = np.array([2, 5, 10, 17]) # y = x^2 + 1

# 2. 2차 다항 특성으로 변환
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# X_poly의 결과: [[1., 1., 1.], [1., 2., 4.], [1., 3., 9.], [1., 4., 16.]]
# (1, x, x^2 순서로 특성이 생성됨)

# 3. 선형 회귀 모델 학습
model = LinearRegression()
model.fit(X_poly, y)

# 4. 예측
# x=5일 때 y 예측
X_test = np.array([[5]])
X_test_poly = poly.transform(X_test)
prediction = model.predict(X_test_poly) # 약 26.0 (5^2 + 1)
```

## 4. 과대적합 (Overfitting) 문제

다항 회귀에서 차수(degree)는 매우 중요한 하이퍼파라미터입니다.
- **차수가 너무 낮으면**: 모델이 데이터의 패턴을 제대로 학습하지 못해 **과소적합(Underfitting)**이 발생합니다.
- **차수가 너무 높으면**: 모델이 훈련 데이터에만 과도하게 최적화되어, 노이즈까지 학습해버리는 **과대적합(Overfitting)**이 발생합니다. 과대적합된 모델은 새로운 데이터에 대한 예측 성능이 급격히 떨어지는 일반화(Generalization) 실패 문제를 겪습니다.

따라서 데이터에 가장 적합한 차수를 찾는 것이 중요하며, 이를 위해 훈련 데이터와 테스트 데이터를 분리하여 모델의 일반화 성능을 평가하거나, 교차 검증(Cross-Validation)을 사용하는 것이 일반적입니다. 과대적합을 방지하기 위해 **규제(Regularization)**가 있는 회귀 모델(Ridge, Lasso)을 함께 사용하는 것도 좋은 방법입니다.
