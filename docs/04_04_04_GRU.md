# GRU (Gated Recurrent Unit)

## 1. GRU의 등장 배경

**GRU(Gated Recurrent Unit)**는 2014년 조경현 교수를 비롯한 연구팀이 제안한 모델로, LSTM의 복잡한 구조를 개선하고 단순화한 RNN의 한 종류입니다.

LSTM은 장기 의존성 문제를 효과적으로 해결했지만, 3개의 게이트와 별도의 셀 상태(Cell State)를 가지는 등 구조가 다소 복잡하고 계산량이 많다는 단점이 있었습니다. GRU는 LSTM과 유사한 성능을 유지하면서도, 더 적은 파라미터와 간단한 구조로 계산 효율성을 높이는 것을 목표로 합니다.

## 2. GRU의 구조

GRU는 LSTM의 **셀 상태(Cell State)**와 **은닉 상태(Hidden State)**를 하나의 **은닉 상태($h_t$)**로 통합하고, 게이트의 수도 3개에서 2개로 줄였습니다.

GRU의 핵심 구성요소는 **리셋 게이트(Reset Gate)**와 **업데이트 게이트(Update Gate)**입니다.

### 2.1. 리셋 게이트 (Reset Gate, $r_t$)
- **역할:** 과거의 정보($h_{t-1}$)를 얼마나 '무시'할지, 즉 '리셋'할지를 결정합니다.
- **동작:** 이전 은닉 상태($h_{t-1}$)와 현재 입력($x_t$)을 받아 시그모이드 함수를 통과시켜 0과 1 사이의 값을 출력합니다. 이 값은 후보 은닉 상태($\tilde{h}_t$)를 계산할 때 이전 은닉 상태에 곱해집니다.
- **결과:** 리셋 게이트의 출력이 0에 가까우면, 과거의 정보를 거의 무시하고 현재 입력($x_t$)만으로 새로운 정보를 만듭니다. 1에 가까우면 과거 정보를 그대로 활용합니다.
  $$ r_t = \sigma(W_r \cdot [h_{t-1}, x_t]) $$

### 2.2. 업데이트 게이트 (Update Gate, $z_t$)
- **역할:** 과거 정보($h_{t-1}$)와 현재 정보($\tilde{h}_t$)를 어떤 비율로 조합하여 최종 은닉 상태($h_t$)를 만들지 결정합니다. 이는 LSTM의 망각 게이트와 입력 게이트의 역할을 합친 것과 유사합니다.
- **동작:**
  - 시그모이드 함수를 통해 0과 1 사이의 값을 출력합니다.
  - 이 값($z_t$)은 이전 은닉 상태($h_{t-1}$)를 얼마나 유지할지를 결정하고, $(1-z_t)$는 새로운 후보 정보($\tilde{h}_t$)를 얼마나 반영할지를 결정합니다.
  $$ z_t = \sigma(W_z \cdot [h_{t-1}, x_t]) $$

## 3. 은닉 상태 업데이트 과정

1.  **리셋 게이트($r_t$) 계산:** 과거 정보를 얼마나 무시할지 결정합니다.
2.  **업데이트 게이트($z_t$) 계산:** 과거와 현재 정보의 조합 비율을 결정합니다.
3.  **후보 은닉 상태($\tilde{h}_t$) 계산:** 리셋 게이트를 이용해 과거 정보의 반영 정도를 조절하여 새로운 후보 정보를 만듭니다.
    $$ \tilde{h}_t = \tanh(W_h \cdot [r_t * h_{t-1}, x_t]) $$
4.  **최종 은닉 상태($h_t$) 계산:** 업데이트 게이트를 이용해 이전 은닉 상태와 후보 은닉 상태를 조합합니다.
    $$ h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t $$
    - $z_t$가 1에 가까우면, 현재 후보 정보($\tilde{h}_t$)를 더 많이 반영합니다 (새로운 정보 업데이트).
    - $z_t$가 0에 가까우면, 과거 정보($h_{t-1}$)를 더 많이 유지합니다 (과거 정보 보존).

## 4. LSTM vs. GRU

| 특징 | LSTM | GRU |
|---|---|---|
| **게이트 수** | 3개 (망각, 입력, 출력) | 2개 (리셋, 업데이트) |
| **상태 벡터**| 셀 상태, 은닉 상태 (2개) | 은닉 상태 (1개) |
| **파라미터 수** | 많음 | 적음 |
| **계산량** | 많음 | 적음 |
| **성능** | 데이터셋이 매우 크고 복잡할 때 약간 더 좋은 경향 | 대부분의 경우 LSTM과 유사한 성능을 보임 |

**결론:**
GRU는 LSTM보다 구조가 간단하고 계산 효율성이 높아, 더 빠르게 학습할 수 있다는 장점이 있습니다. 어떤 모델이 항상 우월하다고 말하기는 어려우며, 해결하고자 하는 문제와 데이터의 특성에 따라 적절한 모델을 선택하는 것이 중요합니다. 일반적으로는 먼저 GRU를 시도해보고, 더 높은 성능이 필요할 경우 LSTM을 고려하는 접근 방식이 많이 사용됩니다.
