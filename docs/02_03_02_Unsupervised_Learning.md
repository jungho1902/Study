# 비지도학습 (Unsupervised Learning)

## 1. 비지도학습이란?

**비지도학습(Unsupervised Learning)**은 지도학습과 달리 **'정답' 즉, 레이블(Label)이 없는 데이터**를 사용하여 학습하는 머신러닝 방식입니다. '선생님'의 지도 없이 데이터 스스로 내재된 구조, 패턴, 관계를 찾아내도록 하는 것이 목표입니다.

- **핵심 목표:** 데이터의 숨겨진 특성을 발견하고, 데이터 자체를 더 잘 이해하는 데 중점을 둡니다. 입력 데이터(X)는 있지만, 예측해야 할 출력 데이터(y)는 주어지지 않습니다.

### 비유: 처음 보는 과일들을 분류하기
비지도학습은 마치 처음 보는 과일들이 담긴 상자를 받고, "비슷한 것들끼리 묶어보세요"라는 요청을 받은 것과 같습니다.
- **상자 속 과일들:** 레이블 없는 데이터
- **분류 기준:** 색깔, 모양, 크기, 질감 등 데이터의 고유한 특성

어떤 과일이 '사과'인지, '오렌지'인지 정답을 모르지만, 우리는 색깔이나 모양 같은 기준을 스스로 정해 빨갛고 동그란 그룹, 주황색의 동그란 그룹 등으로 나눌 수 있습니다. 비지도학습은 이처럼 데이터의 유사성을 기반으로 그룹을 형성하거나 구조를 파악합니다.

## 2. 비지도학습의 주요 과제

비지도학습은 데이터의 숨겨진 구조를 파악하기 위해 다양한 기법을 사용합니다. 대표적인 과제는 다음과 같습니다.

### 2.1. 군집화 (Clustering)
**군집화**는 레이블이 없는 데이터들을 비슷한 특성을 가진 것들끼리 **그룹(Cluster)으로 묶는** 작업입니다. 데이터 포인트 간의 유사성을 기반으로 동작하며, 그룹의 개수나 형태에 대한 사전 정보 없이 데이터 자체의 구조를 드러내는 데 목적이 있습니다.

- **목표:** 군집 내 데이터 간의 유사도는 최대화하고, 군집 간의 유사도는 최소화하는 것입니다.
- **주요 알고리즘:**
  - **K-평균 (K-Means)**
  - **계층적 군집화 (Hierarchical Clustering)**
  - **DBSCAN**

### 2.2. 차원 축소 (Dimensionality Reduction)
**차원 축소**는 데이터의 **특성(Feature) 수를 줄이는** 과정입니다. 여기서 '차원'은 데이터의 특성 개수를 의미합니다. 수백, 수천 개의 특성을 가진 고차원 데이터를 중요한 정보는 최대한 유지하면서 다루기 쉬운 저차원 데이터로 변환합니다.

- **필요성 (차원의 저주, Curse of Dimensionality):** 데이터의 차원이 증가할수록 데이터 공간의 부피가 기하급수적으로 커져, 데이터 간의 거리가 멀어지고 밀도가 희박해집니다. 이는 모델의 성능 저하와 과대적합을 유발할 수 있는데, 차원 축소는 이러한 문제를 완화하는 데 도움을 줍니다.
- **목표:**
  - **시각화:** 3차원 이상의 데이터를 2차원이나 3차원으로 축소하여 시각적으로 표현하고 인사이트를 얻습니다.
  - **노이즈 제거 및 성능 향상:** 불필요하거나 중복되는 특성을 제거하여 모델 학습을 더 효율적으로 만들고 예측 성능을 높입니다.
- **주요 알고리즘:**
  - **주성분 분석 (PCA)**
  - **t-SNE**

### 2.3. 연관 규칙 학습 (Association Rule Learning)
**연관 규칙 학습**은 대규모 데이터셋에서 항목들 간의 **흥미로운 관계나 규칙을 발견**하는 기법입니다. "If A then B" 형태의 규칙을 찾아냅니다.

- **목표:** "A를 구매한 고객은 B도 구매할 가능성이 높다"와 같은 형태의 유용한 연관 규칙을 찾아냅니다.
- **주요 알고리즘:** Apriori, Eclat, FP-Growth 등이 있으며, 규칙의 유용성을 판단하기 위해 지지도(Support), 신뢰도(Confidence), 향상도(Lift)와 같은 지표를 사용합니다.

---

## 3. 핵심 요약 (Key Takeaways)
- **정의:** 비지도학습은 **레이블이 없는 데이터**에서 스스로 패턴과 구조를 찾아내는 학습 방식입니다.
- **주요 과제:** 비슷한 데이터끼리 묶는 **군집화**, 데이터의 특성 수를 줄이는 **차원 축소**, 항목 간의 관계를 찾는 **연관 규칙 학습** 등이 있습니다.
- **핵심 동기:** 데이터 자체에 대한 깊은 이해와 탐색(Exploration)을 목적으로 하며, 종종 지도학습의 전처리 단계로 사용되기도 합니다.
- **한계:** '정답'이 없기 때문에 모델의 성능을 객관적으로 평가하기 어렵고, 결과에 대한 해석이 필요합니다.

지도학습과 비지도학습이 데이터로부터 정적인 패턴을 찾는 데 중점을 둔다면, 순차적인 '행동'과 그에 따른 '보상'을 통해 최적의 전략을 학습하는 또 다른 패러다임이 있습니다. 바로 **강화학습(Reinforcement Learning)**입니다.

---

## 예제 및 풀이 (Examples and Solutions)

### 예제 1: 군집화 (Clustering)
- **목표:** 비슷한 특성을 가진 데이터끼리 그룹으로 묶습니다.
- **구체적 사례:**
  - **고객 세분화 (Customer Segmentation):** 온라인 쇼핑몰의 고객들을 '구매 금액', '방문 빈도', '주요 구매 카테고리' 등의 특성을 바탕으로 'VIP 고객', '알뜰 구매 고객', '신규 고객' 등의 그룹으로 나눕니다. 이를 통해 각 그룹에 맞는 맞춤형 마케팅(쿠폰, 추천 상품 등)을 제공할 수 있습니다.
  - **이미지 분할 (Image Segmentation):** 의료 영상(예: MRI)에서 비슷한 픽셀 값들을 묶어 종양, 장기 등 특정 영역을 배경과 분리해 냅니다.

### 예제 2: 차원 축소 (Dimensionality Reduction)
- **목표:** 데이터의 특성(차원) 수를 줄여 시각화하거나 모델 성능을 높입니다.
- **구체적 사례:**
  - **데이터 시각화:** 수십 개의 특성을 가진 고객 만족도 설문조사 데이터를 2개의 대표적인 축(주성분)으로 축소하여, 2D 산점도에 고객들의 분포를 시각적으로 표현하고 그룹 간의 관계를 파악합니다.
  - **특성 공학 (Feature Engineering):** 수백 개의 센서 데이터 중 서로 상관관계가 높고 중복되는 특성들을 PCA를 통해 몇 개의 핵심 특성으로 압축하여, 머신러닝 모델의 학습 속도를 높이고 과대적합을 방지합니다.

### 예제 3: 연관 규칙 학습 (Association Rule Learning)
- **목표:** 데이터 내에서 항목들 간의 'A이면 B이다' 형태의 관계를 찾아냅니다.
- **구체적 사례:**
  - **장바구니 분석:** 대형마트의 거래 기록을 분석하여 `IF (기저귀) THEN (맥주)` 라는 규칙을 발견합니다. 이 규칙의 신뢰도가 높다면, 두 상품을 가까운 곳에 진열하여 연관 구매를 유도하는 전략을 사용할 수 있습니다.
  - **콘텐츠 추천:** OTT 서비스에서 '오징어 게임을 본 사용자들은 종종 기묘한 이야기도 시청했다'는 규칙을 찾아내어, 오징어 게임을 다 본 사용자에게 기묘한 이야기를 추천해줍니다.
