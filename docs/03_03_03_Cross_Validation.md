# êµì°¨ ê²€ì¦ (Cross-Validation)

ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ì „í†µì ì¸ ë°©ë²•ì¸ **í›ˆë ¨-í…ŒìŠ¤íŠ¸ ë¶„í• (Train-Test Split)**ì€ ë°ì´í„°ë¥¼ í•œ ë²ˆë§Œ ë¶„í• í•˜ì—¬ í‰ê°€í•˜ê¸° ë•Œë¬¸ì— ì—¬ëŸ¬ í•œê³„ì ì„ ê°€ì§‘ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ êµ¬ì„±ì— ë”°ë¼ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ê³¼ëŒ€ ë˜ëŠ” ê³¼ì†Œí‰ê°€ë  ìˆ˜ ìˆìœ¼ë©°, íŠ¹íˆ ë°ì´í„°ê°€ ì œí•œì ì¼ ë•Œ ì´ëŸ° ë¬¸ì œê°€ ë”ìš± ì‹¬ê°í•´ì§‘ë‹ˆë‹¤.

**êµì°¨ ê²€ì¦(Cross-Validation)**ì€ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ **ëª¨ë¸ í‰ê°€ ê¸°ë²•**ìœ¼ë¡œ, ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ë²ˆ ë‹¤ë¥´ê²Œ ë¶„í• í•˜ì—¬ í•™ìŠµê³¼ í‰ê°€ë¥¼ ë°˜ë³µ ìˆ˜í–‰í•¨ìœ¼ë¡œì¨, ëª¨ë¸ì˜ ì„±ëŠ¥ì„ **ë” ì•ˆì •ì ì´ê³  ì‹ ë¢°ì„± ìˆê²Œ** ì¶”ì •í•˜ëŠ” í†µê³„ì  ë°©ë²•ë¡ ì…ë‹ˆë‹¤.

---

## 1. êµì°¨ ê²€ì¦ì˜ í•„ìš”ì„±ê³¼ ì›ë¦¬

### 1.1. ê¸°ì¡´ í‰ê°€ ë°©ì‹ì˜ í•œê³„

**ë‹¨ìˆœ í›ˆë ¨-í…ŒìŠ¤íŠ¸ ë¶„í• ì˜ ë¬¸ì œì :**
- **ë¶„í•  ì˜ì¡´ì„±**: ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ë¶„í• í•˜ëŠëƒì— ë”°ë¼ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ê°€ í¬ê²Œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **ë°ì´í„° ë‚­ë¹„**: í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ëŠ” ëª¨ë¸ í•™ìŠµì— í™œìš©ë˜ì§€ ì•Šì•„ ì œí•œëœ ë°ì´í„°ë¥¼ ë¹„íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤.
- **í¸í–¥ëœ í‰ê°€**: ìš°ì—°íˆ ì‰¬ìš´(ë˜ëŠ” ì–´ë ¤ìš´) ìƒ˜í”Œë§Œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— í¬í•¨ë˜ë©´ ëª¨ë¸ì˜ ì§„ì§œ ì„±ëŠ¥ì„ íŒŒì•…í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.

### 1.2. êµì°¨ ê²€ì¦ì˜ í•µì‹¬ ì•„ì´ë””ì–´

**ë¶„ì‚°-í¸í–¥ íŠ¸ë ˆì´ë“œì˜¤í”„(Bias-Variance Tradeoff) ê´€ì ì—ì„œ:**
- **í¸í–¥(Bias) ê°ì†Œ**: ì—¬ëŸ¬ ë²ˆì˜ ë‹¤ë¥¸ ë¶„í• ë¡œ í‰ê°€í•˜ì—¬ íŠ¹ì • ë¶„í• ì— ì˜ì¡´í•˜ëŠ” í¸í–¥ì„ ì¤„ì…ë‹ˆë‹¤.
- **ë¶„ì‚°(Variance) ì¸¡ì •**: ê° í´ë“œë³„ ì„±ëŠ¥ì˜ ì°¨ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì•ˆì •ì„±ì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ìˆ˜í•™ì  í‘œí˜„:**
ë§Œì•½ ëª¨ë¸ì˜ ì§„ì§œ ì„±ëŠ¥ì„ $\mu$ë¼ê³  í•˜ê³ , kë²ˆì˜ êµì°¨ ê²€ì¦ìœ¼ë¡œ ì–»ì€ ì„±ëŠ¥ì„ $s_1, s_2, ..., s_k$ë¼ê³  í•˜ë©´:

$$\bar{s} = \frac{1}{k}\sum_{i=1}^{k} s_i \approx \mu$$

$$\sigma_s^2 = \frac{1}{k-1}\sum_{i=1}^{k} (s_i - \bar{s})^2$$

ì—¬ê¸°ì„œ $\bar{s}$ëŠ” ì¶”ì •ëœ í‰ê·  ì„±ëŠ¥ì´ê³ , $\sigma_s^2$ëŠ” ì„±ëŠ¥ì˜ ë¶„ì‚°(ëª¨ë¸ ì•ˆì •ì„±ì˜ ì§€í‘œ)ì…ë‹ˆë‹¤.

---

## 2. K-í´ë“œ êµì°¨ ê²€ì¦ (K-Fold Cross-Validation)

### 2.1. ë™ì‘ ì›ë¦¬

K-í´ë“œ êµì°¨ ê²€ì¦ì€ ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” êµì°¨ ê²€ì¦ ê¸°ë²•ì…ë‹ˆë‹¤.

**ì•Œê³ ë¦¬ì¦˜:**
1. **ë°ì´í„° ë¶„í• **: ì „ì²´ í›ˆë ¨ ë°ì´í„°ë¥¼ Kê°œì˜ **ë™ì¼í•œ í¬ê¸°**ë¥¼ ê°€ì§„ ë¶€ë¶„ì§‘í•©(í´ë“œ, Fold)ìœ¼ë¡œ ë¶„í• 
2. **ë°˜ë³µ í•™ìŠµ**: ê° ë°˜ë³µì—ì„œ í•˜ë‚˜ì˜ í´ë“œë¥¼ **ê²€ì¦ ì„¸íŠ¸**ë¡œ, ë‚˜ë¨¸ì§€ K-1ê°œ í´ë“œë¥¼ **í›ˆë ¨ ì„¸íŠ¸**ë¡œ ì‚¬ìš©
3. **ì„±ëŠ¥ ì§‘ê³„**: Kë²ˆì˜ í‰ê°€ ê²°ê³¼ë¥¼ ì§‘ê³„í•˜ì—¬ ìµœì¢… ì„±ëŠ¥ ì‚°ì¶œ

**ì‹œê°ì  í‘œí˜„ (5-í´ë“œ ì˜ˆì‹œ):**
```
Iteration 1: [Test ] [Train] [Train] [Train] [Train]
Iteration 2: [Train] [Test ] [Train] [Train] [Train]
Iteration 3: [Train] [Train] [Test ] [Train] [Train]
Iteration 4: [Train] [Train] [Train] [Test ] [Train]
Iteration 5: [Train] [Train] [Train] [Train] [Test ]
```

### 2.2. ì‹¤ì œ êµ¬í˜„

```python
import numpy as np
from sklearn.model_selection import KFold, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# ìƒ˜í”Œ ë°ì´í„° ìƒì„±
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, 
                         n_redundant=5, random_state=42)

# ëª¨ë¸ ì •ì˜
model = RandomForestClassifier(n_estimators=100, random_state=42)

# K-Fold êµì°¨ ê²€ì¦ (ê¸°ë³¸)
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# ìˆ˜ë™ìœ¼ë¡œ êµì°¨ ê²€ì¦ ìˆ˜í–‰
cv_scores = []
fold_number = 1

for train_index, val_index in kf.split(X):
    # ë°ì´í„° ë¶„í• 
    X_train, X_val = X[train_index], X[val_index]
    y_train, y_val = y[train_index], y[val_index]
    
    # ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    score = accuracy_score(y_val, y_pred)
    cv_scores.append(score)
    
    print(f"Fold {fold_number}: {score:.4f}")
    fold_number += 1

# ê²°ê³¼ ì§‘ê³„
mean_score = np.mean(cv_scores)
std_score = np.std(cv_scores)

print(f"\n=== K-Fold Cross-Validation ê²°ê³¼ ===")
print(f"ê°œë³„ í´ë“œ ì ìˆ˜: {cv_scores}")
print(f"í‰ê·  ì •í™•ë„: {mean_score:.4f} (Â±{std_score:.4f})")
print(f"ì‹ ë¢°êµ¬ê°„ (95%): [{mean_score - 1.96*std_score:.4f}, {mean_score + 1.96*std_score:.4f}]")

# scikit-learnì˜ ë‚´ì¥ í•¨ìˆ˜ ì‚¬ìš© (ë” ê°„í¸)
scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
print(f"\nscikit-learn CV ê²°ê³¼: {scores}")
print(f"í‰ê· : {scores.mean():.4f} (Â±{scores.std():.4f})")
```

### 2.3. K ê°’ ì„ íƒì˜ ê³ ë ¤ì‚¬í•­

**ì¼ë°˜ì ì¸ K ê°’ë“¤ê³¼ íŠ¹ì§•:**

| K ê°’ | ì¥ì  | ë‹¨ì  | ì‚¬ìš© ì‹œê¸° |
|------|------|------|-----------|
| **K=5** | ê³„ì‚° íš¨ìœ¨ì , ì ì ˆí•œ í¸í–¥-ë¶„ì‚° ê· í˜• | ìƒëŒ€ì ìœ¼ë¡œ ë†’ì€ ë¶„ì‚° | ì¼ë°˜ì ì¸ ê²½ìš°, ë¹ ë¥¸ í‰ê°€ í•„ìš” |
| **K=10** | ë‚®ì€ í¸í–¥, ì•ˆì •ì ì¸ ê²°ê³¼ | ë” ë§ì€ ê³„ì‚° ë¹„ìš© | ì •í™•í•œ ì„±ëŠ¥ ì¶”ì • í•„ìš” |
| **K=n (LOOCV)** | ìµœëŒ€ ë°ì´í„° í™œìš©, ìµœì†Œ í¸í–¥ | ë†’ì€ ê³„ì‚° ë¹„ìš©, ë†’ì€ ë¶„ì‚° | ì‘ì€ ë°ì´í„°ì…‹ |

**ìµœì  K ê°’ ê²°ì • ì‹¤í—˜:**
```python
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt

# ë‹¤ì–‘í•œ K ê°’ì— ëŒ€í•œ ì„±ëŠ¥ ë¹„êµ
k_values = [3, 5, 7, 10, 15, 20]
mean_scores = []
std_scores = []

for k in k_values:
    scores = cross_val_score(model, X, y, cv=k, scoring='accuracy')
    mean_scores.append(scores.mean())
    std_scores.append(scores.std())
    print(f"K={k}: {scores.mean():.4f} (Â±{scores.std():.4f})")

# ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(10, 6))
plt.errorbar(k_values, mean_scores, yerr=std_scores, marker='o', capsize=5)
plt.xlabel('K (í´ë“œ ìˆ˜)')
plt.ylabel('êµì°¨ ê²€ì¦ ì •í™•ë„')
plt.title('K ê°’ì— ë”°ë¥¸ êµì°¨ ê²€ì¦ ì„±ëŠ¥')
plt.grid(True, alpha=0.3)
plt.show()
```

---

## 3. ê³„ì¸µì  K-í´ë“œ êµì°¨ ê²€ì¦ (Stratified K-Fold Cross-Validation)

### 3.1. í•„ìš”ì„±ê³¼ ì›ë¦¬

**í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ:**
ë¶„ë¥˜ ë¬¸ì œì—ì„œ í´ë˜ìŠ¤ ë¶„í¬ê°€ ë¶ˆê· í˜•í•  ë•Œ, ì¼ë°˜ì ì¸ K-í´ë“œëŠ” ì‹¬ê°í•œ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ë¬¸ì œ ì‹œë‚˜ë¦¬ì˜¤:**
```python
# ë¶ˆê· í˜• ë°ì´í„° ì˜ˆì‹œ (í´ë˜ìŠ¤ 0: 90%, í´ë˜ìŠ¤ 1: 10%)
from sklearn.datasets import make_classification

X_imb, y_imb = make_classification(n_samples=1000, n_features=20, 
                                  n_clusters_per_class=1, n_redundant=0,
                                  weights=[0.9, 0.1], random_state=42)

print(f"í´ë˜ìŠ¤ ë¶„í¬: {np.bincount(y_imb)}")
print(f"í´ë˜ìŠ¤ ë¹„ìœ¨: {np.bincount(y_imb) / len(y_imb)}")

# ì¼ë°˜ K-Foldì˜ ë¬¸ì œì  ì‹œì—°
from sklearn.model_selection import KFold
import pandas as pd

kf = KFold(n_splits=5, shuffle=True, random_state=42)
fold_distributions = []

for i, (train_idx, val_idx) in enumerate(kf.split(X_imb)):
    val_classes = np.bincount(y_imb[val_idx])
    val_ratio = val_classes / len(val_idx)
    fold_distributions.append({
        'fold': i+1,
        'class_0_ratio': val_ratio[0],
        'class_1_ratio': val_ratio[1] if len(val_ratio) > 1 else 0.0
    })

df_dist = pd.DataFrame(fold_distributions)
print("\n=== ì¼ë°˜ K-Foldì˜ í´ë“œë³„ í´ë˜ìŠ¤ ë¶„í¬ ===")
print(df_dist)
```

### 3.2. Stratified K-Fold êµ¬í˜„

```python
from sklearn.model_selection import StratifiedKFold

# ê³„ì¸µì  K-í´ë“œ êµì°¨ ê²€ì¦
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# í´ë˜ìŠ¤ ë¶„í¬ ìœ ì§€ í™•ì¸
stratified_distributions = []

for i, (train_idx, val_idx) in enumerate(skf.split(X_imb, y_imb)):
    val_classes = np.bincount(y_imb[val_idx])
    val_ratio = val_classes / len(val_idx)
    stratified_distributions.append({
        'fold': i+1,
        'class_0_ratio': val_ratio[0],
        'class_1_ratio': val_ratio[1]
    })

df_stratified = pd.DataFrame(stratified_distributions)
print("\n=== Stratified K-Foldì˜ í´ë“œë³„ í´ë˜ìŠ¤ ë¶„í¬ ===")
print(df_stratified)

# ì„±ëŠ¥ ë¹„êµ
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(random_state=42)

# ì¼ë°˜ K-Fold
regular_scores = cross_val_score(model, X_imb, y_imb, cv=5, scoring='f1')

# Stratified K-Fold
stratified_scores = cross_val_score(model, X_imb, y_imb, 
                                   cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
                                   scoring='f1')

print(f"\n=== ì„±ëŠ¥ ë¹„êµ ===")
print(f"ì¼ë°˜ K-Fold F1: {regular_scores.mean():.4f} (Â±{regular_scores.std():.4f})")
print(f"Stratified K-Fold F1: {stratified_scores.mean():.4f} (Â±{stratified_scores.std():.4f})")
```

---

## 4. ê¸°íƒ€ êµì°¨ ê²€ì¦ ê¸°ë²•ë“¤

### 4.1. Leave-One-Out êµì°¨ ê²€ì¦ (LOOCV)

**íŠ¹ì§•:** K = n (ì „ì²´ ë°ì´í„° ê°œìˆ˜)ì¸ íŠ¹ìˆ˜í•œ ê²½ìš°

```python
from sklearn.model_selection import LeaveOneOut
from sklearn.datasets import load_iris

# ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ LOOCV ì‹œì—°
iris = load_iris()
X_small, y_small = iris.data[:50], iris.target[:50]  # 50ê°œ ìƒ˜í”Œë§Œ ì‚¬ìš©

loo = LeaveOneOut()
loo_scores = cross_val_score(model, X_small, y_small, cv=loo, scoring='accuracy')

print(f"LOOCV ê²°ê³¼ ({len(loo_scores)}ê°œ í´ë“œ):")
print(f"í‰ê·  ì •í™•ë„: {loo_scores.mean():.4f}")
print(f"í‘œì¤€í¸ì°¨: {loo_scores.std():.4f}")

# ê³„ì‚° ì‹œê°„ ë¹„êµ
import time

def time_cv_method(cv_method, X, y, name):
    start_time = time.time()
    scores = cross_val_score(model, X, y, cv=cv_method, scoring='accuracy')
    end_time = time.time()
    
    print(f"{name}: {end_time - start_time:.4f}ì´ˆ")
    return scores

print("\n=== ê³„ì‚° ì‹œê°„ ë¹„êµ ===")
cv_5fold = time_cv_method(5, X_small, y_small, "5-Fold CV")
cv_loo = time_cv_method(loo, X_small, y_small, "Leave-One-Out CV")
```

### 4.2. ì‹œê³„ì—´ êµì°¨ ê²€ì¦ (Time Series Cross-Validation)

**ì‹œê³„ì—´ ë°ì´í„°ì˜ íŠ¹ìˆ˜ì„±:** ì‹œê°„ ìˆœì„œê°€ ì¤‘ìš”í•˜ë¯€ë¡œ ë¬´ì‘ìœ„ ë¶„í•  ë¶ˆê°€

```python
from sklearn.model_selection import TimeSeriesSplit
import pandas as pd

# ì‹œê³„ì—´ ë°ì´í„° ìƒì„± (ì£¼ì‹ ê°€ê²© ì˜ˆì‹œ)
dates = pd.date_range('2020-01-01', periods=200, freq='D')
np.random.seed(42)
price_changes = np.random.randn(200).cumsum()  # ëœë¤ ì›Œí¬
prices = 100 + price_changes

# íŠ¹ì„± ìƒì„± (ì´ë™í‰ê· , ë³€ë™ì„± ë“±)
def create_features(prices, window=5):
    features = []
    targets = []
    
    for i in range(window, len(prices)-1):
        # íŠ¹ì„±: ê³¼ê±° windowì¼ì˜ ê°€ê²©ê³¼ ì´ë™í‰ê· 
        past_prices = prices[i-window:i]
        moving_avg = np.mean(past_prices)
        volatility = np.std(past_prices)
        
        features.append([moving_avg, volatility, past_prices[-1]])  # ì´ë™í‰ê· , ë³€ë™ì„±, ë§ˆì§€ë§‰ ê°€ê²©
        targets.append(1 if prices[i+1] > prices[i] else 0)  # ë‹¤ìŒë‚  ê°€ê²©ì´ ì˜¤ë¥´ë©´ 1
    
    return np.array(features), np.array(targets)

X_ts, y_ts = create_features(prices)

# ì‹œê³„ì—´ êµì°¨ ê²€ì¦
tscv = TimeSeriesSplit(n_splits=5)

print("=== Time Series Cross-Validation ===")
ts_scores = []

for i, (train_index, test_index) in enumerate(tscv.split(X_ts)):
    X_train, X_test = X_ts[train_index], X_ts[test_index]
    y_train, y_test = y_ts[train_index], y_ts[test_index]
    
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    ts_scores.append(score)
    
    print(f"Fold {i+1}: í›ˆë ¨ ê¸°ê°„ [{train_index[0]}-{train_index[-1]}], "
          f"í…ŒìŠ¤íŠ¸ ê¸°ê°„ [{test_index[0]}-{test_index[-1]}], "
          f"ì •í™•ë„: {score:.4f}")

print(f"\nì‹œê³„ì—´ CV í‰ê·  ì„±ëŠ¥: {np.mean(ts_scores):.4f} (Â±{np.std(ts_scores):.4f})")
```

---

## 5. êµì°¨ ê²€ì¦ì˜ í™œìš©

### 5.1. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€

```python
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.model_selection import cross_validate

# ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'SVM': SVC(random_state=42)
}

# ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œë¡œ êµì°¨ ê²€ì¦
scoring = ['accuracy', 'precision', 'recall', 'f1']

results = {}
for name, model in models.items():
    cv_results = cross_validate(model, X, y, cv=5, scoring=scoring)
    
    results[name] = {
        'accuracy': cv_results['test_accuracy'].mean(),
        'precision': cv_results['test_precision'].mean(),
        'recall': cv_results['test_recall'].mean(),
        'f1': cv_results['test_f1'].mean(),
        'std_accuracy': cv_results['test_accuracy'].std()
    }

# ê²°ê³¼ ì¶œë ¥
print("=== ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (5-Fold CV) ===")
for name, metrics in results.items():
    print(f"\n{name}:")
    for metric, value in metrics.items():
        if 'std' not in metric:
            print(f"  {metric}: {value:.4f}")
    print(f"  accuracy_std: Â±{metrics['std_accuracy']:.4f}")
```

### 5.2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ê³¼ ì¤‘ì²© êµì°¨ ê²€ì¦

```python
from sklearn.model_selection import GridSearchCV

# ì¤‘ì²© êµì°¨ ê²€ì¦ (Nested Cross-Validation)
def nested_cross_validation(model, param_grid, X, y, outer_cv=5, inner_cv=3):
    """
    ì¤‘ì²© êµì°¨ ê²€ì¦ ìˆ˜í–‰
    - ì™¸ë¶€ ë£¨í”„: ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ í‰ê°€
    - ë‚´ë¶€ ë£¨í”„: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
    """
    outer_scores = []
    best_params_list = []
    
    # ì™¸ë¶€ êµì°¨ ê²€ì¦
    outer_kfold = StratifiedKFold(n_splits=outer_cv, shuffle=True, random_state=42)
    
    for fold, (train_idx, test_idx) in enumerate(outer_kfold.split(X, y)):
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]
        
        # ë‚´ë¶€ êµì°¨ ê²€ì¦ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
        inner_cv_obj = StratifiedKFold(n_splits=inner_cv, shuffle=True, random_state=42)
        grid_search = GridSearchCV(model, param_grid, cv=inner_cv_obj, 
                                 scoring='f1', n_jobs=-1)
        grid_search.fit(X_train, y_train)
        
        # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ í…ŒìŠ¤íŠ¸
        best_model = grid_search.best_estimator_
        score = best_model.score(X_test, y_test)
        
        outer_scores.append(score)
        best_params_list.append(grid_search.best_params_)
        
        print(f"Outer Fold {fold + 1}: {score:.4f}, Best params: {grid_search.best_params_}")
    
    return outer_scores, best_params_list

# ì‹¤ì œ ì¤‘ì²© CV ìˆ˜í–‰
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [5, 10, None],
    'min_samples_split': [2, 5]
}

rf_model = RandomForestClassifier(random_state=42)
nested_scores, best_params = nested_cross_validation(rf_model, param_grid, X, y)

print(f"\n=== ì¤‘ì²© êµì°¨ ê²€ì¦ ê²°ê³¼ ===")
print(f"í‰ê·  ì„±ëŠ¥: {np.mean(nested_scores):.4f} (Â±{np.std(nested_scores):.4f})")
print(f"ëª¨ë“  í´ë“œ ì„±ëŠ¥: {nested_scores}")
```

### 5.3. í•™ìŠµ ê³¡ì„  ë¶„ì„

```python
from sklearn.model_selection import learning_curve

# í•™ìŠµ ê³¡ì„  ìƒì„±
train_sizes, train_scores, val_scores = learning_curve(
    RandomForestClassifier(n_estimators=100, random_state=42),
    X, y, cv=5,
    train_sizes=np.linspace(0.1, 1.0, 10),
    scoring='accuracy'
)

# ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(12, 8))

# í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)

# í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸°
plt.subplot(2, 2, 1)
plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
plt.plot(train_sizes, val_mean, 'o-', color='red', label='Cross-Validation Score')
plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy Score')
plt.title('Learning Curve')
plt.legend()
plt.grid(True, alpha=0.3)

# êµì°¨ ê²€ì¦ ì ìˆ˜ì˜ ë¶„í¬
plt.subplot(2, 2, 2)
final_val_scores = val_scores[-1]  # ë§ˆì§€ë§‰(ìµœëŒ€ ë°ì´í„° í¬ê¸°)ì—ì„œì˜ CV ì ìˆ˜ë“¤
plt.hist(final_val_scores, bins=5, alpha=0.7, color='red')
plt.axvline(np.mean(final_val_scores), color='black', linestyle='--', label=f'Mean: {np.mean(final_val_scores):.3f}')
plt.xlabel('Cross-Validation Score')
plt.ylabel('Frequency')
plt.title('CV Score Distribution')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"ìµœì¢… êµì°¨ ê²€ì¦ ì ìˆ˜: {np.mean(final_val_scores):.4f} (Â±{np.std(final_val_scores):.4f})")
```

---

## ì˜ˆì œ ë° í’€ì´ (Examples and Solutions)

### ì˜ˆì œ 1: ì˜ë£Œ ì§„ë‹¨ ëª¨ë¸ì˜ êµì°¨ ê²€ì¦

**ë¬¸ì œ:** ì˜ë£Œ ì§„ë‹¨ ë°ì´í„°ì—ì„œ ì§ˆë³‘ ì˜ˆì¸¡ ëª¨ë¸ì„ ê°œë°œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ëŠ” ë§¤ìš° ë¶ˆê· í˜•í•˜ë©°(ì§ˆë³‘ í™˜ì 5%, ì •ìƒ í™˜ì 95%), ëª¨ë¸ì˜ ì‹ ë¢°ì„±ì´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ì ì ˆí•œ êµì°¨ ê²€ì¦ ì „ëµì„ ìˆ˜ë¦½í•˜ê³  êµ¬í˜„í•˜ì„¸ìš”.

**ë°ì´í„° íŠ¹ì„±:**
- 1000ëª…ì˜ í™˜ì ë°ì´í„°
- ì§ˆë³‘ í™˜ì: 50ëª… (5%)
- ì •ìƒ í™˜ì: 950ëª… (95%)
- íŠ¹ì„±: ë‚˜ì´, í˜ˆì••, ì½œë ˆìŠ¤í…Œë¡¤, í˜ˆë‹¹ ë“± 10ê°œ

**í’€ì´:**

**1ë‹¨ê³„: ë¶ˆê· í˜• ë°ì´í„° ìƒì„±**
```python
import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from sklearn.model_selection import StratifiedKFold, cross_validate
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight

# ë¶ˆê· í˜• ì˜ë£Œ ë°ì´í„° ìƒì„±
np.random.seed(42)
X_medical, y_medical = make_classification(
    n_samples=1000,
    n_features=10,
    n_informative=8,
    n_redundant=2,
    n_clusters_per_class=1,
    weights=[0.95, 0.05],  # 95% ì •ìƒ, 5% ì§ˆë³‘
    random_state=42
)

# íŠ¹ì„±ì— ì˜ë¯¸ìˆëŠ” ì´ë¦„ ë¶€ì—¬
feature_names = ['age', 'blood_pressure_sys', 'blood_pressure_dia', 'cholesterol',
                'blood_sugar', 'bmi', 'heart_rate', 'exercise_hours',
                'smoking_years', 'family_history']

df_medical = pd.DataFrame(X_medical, columns=feature_names)
df_medical['disease'] = y_medical

print("=== ì˜ë£Œ ë°ì´í„° ê°œìš” ===")
print(f"ì „ì²´ ìƒ˜í”Œ ìˆ˜: {len(df_medical)}")
print(f"í´ë˜ìŠ¤ ë¶„í¬: {np.bincount(y_medical)}")
print(f"ì§ˆë³‘ ë¹„ìœ¨: {y_medical.sum() / len(y_medical) * 100:.1f}%")
```

**2ë‹¨ê³„: ì ì ˆí•œ êµì°¨ ê²€ì¦ ì „ëµ ì„¤ì •**
```python
from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score, roc_auc_score

# ë¶ˆê· í˜• ë°ì´í„°ì— ì í•©í•œ í‰ê°€ ì§€í‘œ ì„¤ì •
scoring = {
    'accuracy': 'accuracy',
    'precision': make_scorer(precision_score, pos_label=1),
    'recall': make_scorer(recall_score, pos_label=1),
    'f1': make_scorer(f1_score, pos_label=1),
    'roc_auc': make_scorer(roc_auc_score, needs_proba=True)
}

# Stratified K-Fold ì„¤ì • (í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€)
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¶ˆê· í˜• ëŒ€ì‘)
class_weights = compute_class_weight('balanced', classes=np.unique(y_medical), y=y_medical)
weight_dict = {0: class_weights[0], 1: class_weights[1]}

print(f"í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {weight_dict}")
```

**3ë‹¨ê³„: ëª¨ë¸ë³„ êµì°¨ ê²€ì¦ ìˆ˜í–‰**
```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC

# ë¶ˆê· í˜• ë°ì´í„°ì— ì í•©í•œ ëª¨ë¸ë“¤
models = {
    'Logistic Regression (Balanced)': LogisticRegression(
        class_weight='balanced', random_state=42, max_iter=1000
    ),
    'Random Forest (Balanced)': RandomForestClassifier(
        class_weight='balanced', n_estimators=100, random_state=42
    ),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'SVM (Balanced)': SVC(
        class_weight='balanced', probability=True, random_state=42
    )
}

# ê° ëª¨ë¸ì— ëŒ€í•œ êµì°¨ ê²€ì¦ ìˆ˜í–‰
results_medical = {}

for name, model in models.items():
    print(f"\n=== {name} êµì°¨ ê²€ì¦ ì¤‘... ===")
    
    cv_results = cross_validate(
        model, X_medical, y_medical,
        cv=skf,
        scoring=scoring,
        return_train_score=False,
        n_jobs=-1
    )
    
    results_medical[name] = {
        metric: {
            'mean': cv_results[f'test_{metric}'].mean(),
            'std': cv_results[f'test_{metric}'].std(),
            'scores': cv_results[f'test_{metric}']
        }
        for metric in scoring.keys()
    }
    
    # ê²°ê³¼ ì¶œë ¥
    for metric in scoring.keys():
        mean_score = results_medical[name][metric]['mean']
        std_score = results_medical[name][metric]['std']
        print(f"  {metric}: {mean_score:.4f} (Â±{std_score:.4f})")
```

**4ë‹¨ê³„: ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”**
```python
import matplotlib.pyplot as plt

# ê²°ê³¼ ë¹„êµ ì‹œê°í™”
metrics_to_plot = ['f1', 'precision', 'recall', 'roc_auc']
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
axes = axes.flatten()

for i, metric in enumerate(metrics_to_plot):
    ax = axes[i]
    
    model_names = list(results_medical.keys())
    means = [results_medical[name][metric]['mean'] for name in model_names]
    stds = [results_medical[name][metric]['std'] for name in model_names]
    
    bars = ax.bar(range(len(model_names)), means, yerr=stds, capsize=5, alpha=0.7)
    ax.set_xlabel('Models')
    ax.set_ylabel(f'{metric.upper()} Score')
    ax.set_title(f'{metric.upper()} Comparison (10-Fold CV)')
    ax.set_xticks(range(len(model_names)))
    ax.set_xticklabels([name.split('(')[0].strip() for name in model_names], rotation=45)
    ax.grid(True, alpha=0.3)
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ í•˜ì´ë¼ì´íŠ¸
    best_idx = np.argmax(means)
    bars[best_idx].set_color('red')
    bars[best_idx].set_alpha(0.9)

plt.tight_layout()
plt.show()

# ìµœì  ëª¨ë¸ ì„ ì •
best_model_name = max(results_medical.keys(), 
                     key=lambda x: results_medical[x]['f1']['mean'])
print(f"\nğŸ† ìµœì  ëª¨ë¸: {best_model_name}")
print(f"F1 Score: {results_medical[best_model_name]['f1']['mean']:.4f} (Â±{results_medical[best_model_name]['f1']['std']:.4f})")
```

**5ë‹¨ê³„: ëª¨ë¸ ì•ˆì •ì„± ë¶„ì„**
```python
# ìµœì  ëª¨ë¸ì˜ ê° í´ë“œë³„ ì„±ëŠ¥ ë¶„ì„
best_model = models[best_model_name]
fold_results = []

for fold, (train_idx, val_idx) in enumerate(skf.split(X_medical, y_medical)):
    X_train, X_val = X_medical[train_idx], X_medical[val_idx]
    y_train, y_val = y_medical[train_idx], y_medical[val_idx]
    
    # ëª¨ë¸ í›ˆë ¨
    best_model.fit(X_train, y_train)
    
    # ì˜ˆì¸¡ ë° í‰ê°€
    y_pred = best_model.predict(X_val)
    y_pred_proba = best_model.predict_proba(X_val)[:, 1]
    
    fold_results.append({
        'fold': fold + 1,
        'f1': f1_score(y_val, y_pred),
        'precision': precision_score(y_val, y_pred),
        'recall': recall_score(y_val, y_pred),
        'roc_auc': roc_auc_score(y_val, y_pred_proba),
        'true_positives': confusion_matrix(y_val, y_pred)[1, 1],
        'false_positives': confusion_matrix(y_val, y_pred)[0, 1],
        'false_negatives': confusion_matrix(y_val, y_pred)[1, 0]
    })

# í´ë“œë³„ ê²°ê³¼ DataFrame
df_folds = pd.DataFrame(fold_results)
print(f"\n=== {best_model_name} í´ë“œë³„ ìƒì„¸ ê²°ê³¼ ===")
print(df_folds.round(4))

# ì„±ëŠ¥ ë¶„ì‚° ë¶„ì„
print(f"\n=== ì„±ëŠ¥ ì•ˆì •ì„± ë¶„ì„ ===")
for metric in ['f1', 'precision', 'recall', 'roc_auc']:
    values = df_folds[metric]
    cv_coefficient = values.std() / values.mean()  # ë³€ë™ê³„ìˆ˜
    print(f"{metric.upper()}: í‰ê·  {values.mean():.4f}, í‘œì¤€í¸ì°¨ {values.std():.4f}, CV {cv_coefficient:.4f}")

# ì„ê³„ì  ë¶„ì„ (ì˜ë£Œ ì§„ë‹¨ì—ì„œ ì¤‘ìš”)
print(f"\n=== ì„ê³„ ì„±ëŠ¥ ë¶„ì„ ===")
min_recall = 0.8  # ì˜ë£Œ ì§„ë‹¨ì—ì„œëŠ” ì¬í˜„ìœ¨ì´ ë§¤ìš° ì¤‘ìš”
reliable_folds = df_folds[df_folds['recall'] >= min_recall]
print(f"ì¬í˜„ìœ¨ {min_recall} ì´ìƒ ë‹¬ì„± í´ë“œ: {len(reliable_folds)}/10")
print(f"ìµœì†Œ ì¬í˜„ìœ¨: {df_folds['recall'].min():.4f}")
print(f"ì¬í˜„ìœ¨ í‰ê· : {df_folds['recall'].mean():.4f}")

if df_folds['recall'].min() < min_recall:
    print("âš ï¸  ì¼ë¶€ í´ë“œì—ì„œ ì¬í˜„ìœ¨ì´ ê¸°ì¤€ ë¯¸ë‹¬. ëª¨ë¸ ê°œì„  í•„ìš”.")
else:
    print("âœ… ëª¨ë“  í´ë“œì—ì„œ ì•ˆì •ì ì¸ ì¬í˜„ìœ¨ ë‹¬ì„±.")
```

**í•´ì„¤:**
ì´ ì˜ˆì œëŠ” ì˜ë£Œ ì§„ë‹¨ê³¼ ê°™ì€ **ë†’ì€ ì‹ ë¢°ì„±ì´ ìš”êµ¬ë˜ëŠ” ë¶„ì•¼**ì—ì„œì˜ êµì°¨ ê²€ì¦ í™œìš©ì„ ë³´ì—¬ì¤ë‹ˆë‹¤:

1. **Stratified CV ì‚¬ìš©**: ê·¹ë„ë¡œ ë¶ˆê· í˜•í•œ ë°ì´í„°ì—ì„œ í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€
2. **ì ì ˆí•œ í‰ê°€ ì§€í‘œ**: ì •í™•ë„ë³´ë‹¤ F1, ì¬í˜„ìœ¨, ROC-AUC ì¤‘ì‹¬ í‰ê°€
3. **í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜**: ë¶ˆê· í˜• ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ëª¨ë¸ ì¡°ì •
4. **ì•ˆì •ì„± ë¶„ì„**: ëª¨ë“  í´ë“œì—ì„œ ì¼ê´€ëœ ì„±ëŠ¥ í™•ì¸
5. **ì„ê³„ì  ê¸°ë°˜ í‰ê°€**: ì˜ë£Œ ë¶„ì•¼ì˜ íŠ¹ìˆ˜ ìš”êµ¬ì‚¬í•­(ë†’ì€ ì¬í˜„ìœ¨) ë°˜ì˜

### ì˜ˆì œ 2: A/B í…ŒìŠ¤íŠ¸ì™€ êµì°¨ ê²€ì¦

**ë¬¸ì œ:** ì˜¨ë¼ì¸ ì‡¼í•‘ëª°ì—ì„œ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ë‘ ê°€ì§€ ë²„ì „(Aì™€ B)ì„ ë¹„êµí•˜ë ¤ê³  í•©ë‹ˆë‹¤. êµì°¨ ê²€ì¦ì„ ì‚¬ìš©í•˜ì—¬ ì–´ë–¤ ë²„ì „ì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ”ì§€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ê°€ ìˆëŠ”ì§€ ê²€ì¦í•˜ì„¸ìš”.

**í’€ì´:**

```python
from scipy import stats
from sklearn.metrics import accuracy_score
import numpy as np

# ë‘ ê°€ì§€ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ ì‹œë®¬ë ˆì´ì…˜
def recommendation_algorithm_A(X):
    """ê¸°ì¡´ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ A"""
    # ê°„ë‹¨í•œ ë¡œì§€ìŠ¤í‹± íšŒê·€ ê¸°ë°˜
    from sklearn.linear_model import LogisticRegression
    model = LogisticRegression(random_state=42)
    return model

def recommendation_algorithm_B(X):
    """ìƒˆë¡œìš´ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ B"""
    # ëœë¤ í¬ë ˆìŠ¤íŠ¸ ê¸°ë°˜
    from sklearn.ensemble import RandomForestClassifier
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    return model

# ì¶”ì²œ ì„±ëŠ¥ ë¹„êµë¥¼ ìœ„í•œ êµì°¨ ê²€ì¦
def compare_algorithms_with_cv(X, y, algorithm_A, algorithm_B, cv_splits=10):
    """êµì°¨ ê²€ì¦ì„ ì‚¬ìš©í•œ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ"""
    
    skf = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42)
    
    scores_A = []
    scores_B = []
    
    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]
        
        # ì•Œê³ ë¦¬ì¦˜ A í‰ê°€
        model_A = algorithm_A(X_train)
        model_A.fit(X_train, y_train)
        pred_A = model_A.predict(X_test)
        score_A = accuracy_score(y_test, pred_A)
        scores_A.append(score_A)
        
        # ì•Œê³ ë¦¬ì¦˜ B í‰ê°€
        model_B = algorithm_B(X_train)
        model_B.fit(X_train, y_train)
        pred_B = model_B.predict(X_test)
        score_B = accuracy_score(y_test, pred_B)
        scores_B.append(score_B)
        
        print(f"Fold {fold+1}: Algorithm A = {score_A:.4f}, Algorithm B = {score_B:.4f}")
    
    return np.array(scores_A), np.array(scores_B)

# ì‹¤ì œ ë¹„êµ ìˆ˜í–‰
scores_A, scores_B = compare_algorithms_with_cv(X, y, recommendation_algorithm_A, recommendation_algorithm_B)

# í†µê³„ì  ê²€ì •
print(f"\n=== ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ ===")
print(f"Algorithm A: {scores_A.mean():.4f} (Â±{scores_A.std():.4f})")
print(f"Algorithm B: {scores_B.mean():.4f} (Â±{scores_B.std():.4f})")

# ëŒ€ì‘í‘œë³¸ t-ê²€ì • (paired t-test)
t_stat, p_value = stats.ttest_rel(scores_B, scores_A)
print(f"\n=== í†µê³„ì  ê²€ì • ê²°ê³¼ ===")
print(f"t-statistic: {t_stat:.4f}")
print(f"p-value: {p_value:.4f}")

alpha = 0.05
if p_value < alpha:
    winner = "Algorithm B" if scores_B.mean() > scores_A.mean() else "Algorithm A"
    print(f"ê²°ë¡ : {winner}ê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ê²Œ ë” ìš°ìˆ˜í•¨ (Î± = {alpha})")
else:
    print(f"ê²°ë¡ : ë‘ ì•Œê³ ë¦¬ì¦˜ ê°„ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´ ì—†ìŒ (Î± = {alpha})")

# íš¨ê³¼ í¬ê¸° ê³„ì‚° (Cohen's d)
pooled_std = np.sqrt(((len(scores_A) - 1) * scores_A.var() + 
                     (len(scores_B) - 1) * scores_B.var()) / 
                    (len(scores_A) + len(scores_B) - 2))
cohens_d = (scores_B.mean() - scores_A.mean()) / pooled_std
print(f"Cohen's d (íš¨ê³¼ í¬ê¸°): {cohens_d:.4f}")

if abs(cohens_d) < 0.2:
    effect_size = "ì‘ì€ íš¨ê³¼"
elif abs(cohens_d) < 0.8:
    effect_size = "ì¤‘ê°„ íš¨ê³¼"
else:
    effect_size = "í° íš¨ê³¼"
    
print(f"íš¨ê³¼ í¬ê¸° í•´ì„: {effect_size}")
```

**í•´ì„¤:**
ì´ ì˜ˆì œëŠ” **í†µê³„ì  ê²€ì •ê³¼ ê²°í•©ëœ êµì°¨ ê²€ì¦**ì„ ë³´ì—¬ì¤ë‹ˆë‹¤:

1. **ëŒ€ì‘í‘œë³¸ ì„¤ê³„**: ë™ì¼í•œ ë°ì´í„° ë¶„í• ì—ì„œ ë‘ ì•Œê³ ë¦¬ì¦˜ì„ ë¹„êµí•˜ì—¬ ê³µì •ì„± í™•ë³´
2. **í†µê³„ì  ìœ ì˜ì„± ê²€ì •**: t-ê²€ì •ì„ í†µí•œ ì°¨ì´ì˜ í†µê³„ì  ìœ ì˜ì„± í™•ì¸
3. **íš¨ê³¼ í¬ê¸° ì¸¡ì •**: ì‹¤ìš©ì  ì˜ë¯¸ì˜ í¬ê¸° í‰ê°€
4. **ë°˜ë³µ ê²€ì¦**: ì—¬ëŸ¬ í´ë“œë¥¼ í†µí•œ ì•ˆì •ì ì¸ ê²°ê³¼ í™•ë³´

---

## í•µì‹¬ ìš”ì•½ (Key Takeaways)

### êµì°¨ ê²€ì¦ì˜ í•µì‹¬ ê°€ì¹˜
1. **ì‹ ë¢°ì„± í–¥ìƒ**: ë‹¨ì¼ ë¶„í• ì˜ ìš°ì—°ì„±ì„ ë°°ì œí•˜ê³  ì•ˆì •ì ì¸ ì„±ëŠ¥ ì¶”ì •
2. **ë°ì´í„° íš¨ìœ¨ì„±**: ëª¨ë“  ë°ì´í„°ê°€ í›ˆë ¨ê³¼ ê²€ì¦ì— í™œìš©ë˜ì–´ ì •ë³´ ì†ì‹¤ ìµœì†Œí™”
3. **ì¼ë°˜í™” ì„±ëŠ¥**: ë‹¤ì–‘í•œ ë°ì´í„° ë¶„í• ì—ì„œ ì¼ê´€ëœ ì„±ëŠ¥ì„ í†µí•œ ëª¨ë¸ ì‹ ë¢°ì„± í™•ë³´
4. **ê³¼ì í•© ë°©ì§€**: íŠ¹ì • ë¶„í• ì— ê³¼ë„í•˜ê²Œ ìµœì í™”ë˜ëŠ” ê²ƒì„ ë°©ì§€

### ìƒí™©ë³„ ìµœì  ì „ëµ
- **ê· í˜• ë°ì´í„°**: K-Fold CV (K=5 ë˜ëŠ” 10)
- **ë¶ˆê· í˜• ë°ì´í„°**: Stratified K-Fold CV
- **ì‹œê³„ì—´ ë°ì´í„°**: Time Series Split
- **ì†Œê·œëª¨ ë°ì´í„°**: LOOCV
- **ê³„ì‚° ìì› ì œí•œ**: K=3 ë˜ëŠ” 5

### ì£¼ì˜ì‚¬í•­ê³¼ í•œê³„
- **ê³„ì‚° ë¹„ìš©**: Kë°°ì˜ ì¶”ê°€ ê³„ì‚° ì‹œê°„ í•„ìš”
- **ë°ì´í„° ë…ë¦½ì„±**: ì‹œê³„ì—´ì´ë‚˜ ê·¸ë£¹í™”ëœ ë°ì´í„°ì—ì„œëŠ” íŠ¹ë³„í•œ ê³ ë ¤ í•„ìš”
- **í‰ê°€ ì§€í‘œ ì„ íƒ**: ë¬¸ì œ íŠ¹ì„±ì— ë§ëŠ” ì ì ˆí•œ ì§€í‘œ ì‚¬ìš©
- **ì¤‘ì²© CV**: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ê³¼ ëª¨ë¸ í‰ê°€ì˜ ë¶„ë¦¬

êµì°¨ ê²€ì¦ì€ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ **ëª¨ë¸ì˜ ì§„ì •í•œ ì„±ëŠ¥ì„ í‰ê°€**í•˜ê³  **ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼**ë¥¼ ì–»ê¸° ìœ„í•œ í•„ìˆ˜ì ì¸ ë„êµ¬ì…ë‹ˆë‹¤. ì ì ˆí•œ êµì°¨ ê²€ì¦ ì „ëµì˜ ì„ íƒê³¼ ì˜¬ë°”ë¥¸ í•´ì„ì„ í†µí•´ ë” ë‚˜ì€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
