# 분류 모델 평가 지표

분류 모델의 성능은 단순히 '얼마나 맞췄는가'를 넘어, '어떤 종류의 실수를 하는가'를 종합적으로 평가해야 합니다. 특히 데이터가 불균형할 경우, 정확도(Accuracy)만으로는 모델을 올바르게 평가할 수 없습니다.

---

### 1. 혼동 행렬 (Confusion Matrix)

**혼동 행렬**은 분류 모델의 예측 결과를 실제 정답과 비교하여, 어떤 종류의 예측 오류가 발생하는지를 보여주는 행렬 표입니다. 이진 분류(Positive/Negative) 문제에서 혼동 행렬은 다음 네 가지 값으로 구성됩니다.

| | **예측: Positive** | **예측: Negative** |
|---|---|---|
| **실제: Positive** | **TP** (True Positive) | **FN** (False Negative) |
| **실제: Negative** | **FP** (False Positive) | **TN** (True Negative) |

- **TP (진짜 양성):** 실제 Positive인 것을 Positive로 올바르게 예측.
- **TN (진짜 음성):** 실제 Negative인 것을 Negative로 올바르게 예측.
- **FP (가짜 양성, 1종 오류):** 실제 Negative인 것을 Positive로 잘못 예측. (예: 정상 메일을 스팸으로 분류)
- **FN (가짜 음성, 2종 오류):** 실제 Positive인 것을 Negative로 잘못 예측. (예: 암 환자를 정상으로 진단)

---

### 2. 정확도 (Accuracy)

- **정의:** 전체 예측 중 올바르게 예측한 비율. 가장 직관적인 지표입니다.
- **공식:** `(TP + TN) / (TP + TN + FP + FN)`
- **한계:** 데이터의 클래스 분포가 **불균형(imbalanced)**할 때 모델 성능을 왜곡할 수 있습니다. 예를 들어, 99%가 정상이고 1%가 불량인 데이터에서, 모델이 모든 것을 '정상'으로만 예측해도 정확도는 99%가 되어 모델 성능을 오해하게 만듭니다.

---

### 3. 정밀도 (Precision)

- **정의:** **Positive로 예측한 것들 중에서**, 실제로 Positive인 것의 비율.
- **공식:** `TP / (TP + FP)`
- **의미:** 모델의 "Positive 예측"이 얼마나 정밀하고 정확한지를 나타냅니다. 정밀도가 높다는 것은, 모델이 Positive라고 예측했을 때 틀릴 확률이 낮다는 의미입니다.
- **중요한 경우:** 스팸 메일 필터링. **정상 메일(Negative)을 스팸(Positive)으로 잘못 분류(FP)하면 안 되는** 경우에 중요합니다. (정밀도 = 1 - (FP 비율))

---

### 4. 재현율 (Recall) / 민감도 (Sensitivity)

- **정의:** **실제 Positive인 것들 중에서**, 모델이 Positive로 올바르게 예측한 것의 비율.
- **공식:** `TP / (TP + FN)`
- **의미:** 모델이 실제 Positive인 케이스를 얼마나 잘 '재현'해내는지를 나타냅니다. 재현율이 높다는 것은, 실제 Positive인 데이터를 놓치지 않고 잘 찾아낸다는 의미입니다.
- **중요한 경우:** 암 진단. **실제 암 환자(Positive)를 정상(Negative)으로 잘못 진단(FN)하면 치명적인** 경우에 중요합니다.

**정밀도와 재현율의 트레이드오프:** 일반적으로 정밀도를 높이면 재현율이 떨어지고, 재현율을 높이면 정밀도가 떨어지는 상충 관계(Trade-off)가 존재합니다.

---

### 5. F1 점수 (F1-Score)

- **정의:** 정밀도와 재현율의 **조화 평균(harmonic mean)**. 두 지표의 균형을 나타냅니다.
- **공식:** `2 * (Precision * Recall) / (Precision + Recall)`
- **특징:**
  - 정밀도와 재현율이 모두 높을 때 F1 점수도 높아집니다.
  - 데이터가 불균형할 때, 모델의 성능을 정확하게 평가하는 데 유용한 지표입니다.

---

### 6. ROC 곡선과 AUC

- **ROC 곡선 (Receiver Operating Characteristic Curve):**
  - 분류 모델의 결정 임계값(decision threshold)을 0부터 1까지 변화시키면서, 각 임계값에서의 **FPR(False Positive Rate)**과 **TPR(True Positive Rate)**의 관계를 그린 곡선입니다.
  - **FPR (가짜 양성 비율):** `FP / (FP + TN)`. 실제 Negative 중 Positive로 잘못 예측한 비율.
  - **TPR (진짜 양성 비율):** `TP / (TP + FN)`. **재현율(Recall)**과 동일합니다.
  - 곡선이 왼쪽 위 모서리(좌상단)에 가까울수록, 즉 FPR이 낮으면서 TPR이 높을수록 모델의 성능이 좋다고 평가합니다.

- **AUC (Area Under the Curve):**
  - ROC 곡선 아래의 면적을 의미합니다. 0과 1 사이의 값을 가집니다.
  - **AUC가 1에 가까울수록**, 모델이 Positive와 Negative 클래스를 얼마나 잘 구별하는지를 나타내는 지표로, 성능이 좋음을 의미합니다.
  - AUC가 0.5이면 모델이 무작위로 추측하는 것과 같은 성능을 의미합니다.
  - AUC는 임계값의 변화에도 모델의 전반적인 성능을 일관되게 평가할 수 있다는 장점이 있습니다.
