# 가중치 초기화 (Weight Initialization)

## 1. 가중치 초기화의 중요성

신경망의 학습 과정에서 가중치(Weight)를 처음에 어떤 값으로 설정하느냐는 전체 학습 과정에 매우 큰 영향을 미칩니다. 좋은 가중치 초기화는 다음과 같은 이점을 가집니다.

- **학습 속도 향상:** 최적의 해를 더 빨리 찾을 수 있도록 도와줍니다.
- **지역 최적해(Local Minimum) 탈출 가능성 증가:** 좋지 않은 초기값은 모델을 나쁜 지역 최적해에 갇히게 할 수 있습니다.
- **기울기 소실/폭주(Vanishing/Exploding Gradient) 방지:** 층이 깊어질수록 그래디언트가 너무 작아지거나 너무 커지는 현상을 완화합니다.

만약 모든 가중치를 0이나 동일한 값으로 초기화하면, 모든 뉴런이 동일한 출력을 내고 동일한 그래디언트를 받게 됩니다. 이는 대칭성(symmetry)을 깨뜨리지 못해 여러 개의 뉴런을 사용하는 의미가 없게 만듭니다. 따라서 가중치는 보통 **무작위(random)** 값을 가지면서도 특정 분포를 따르도록 초기화합니다.

---

## 2. 주요 가중치 초기화 방법

### 2.1. Xavier / Glorot 초기화 (Xavier / Glorot Initialization)

- **제안 배경:** 활성화 함수로 **Sigmoid**나 **Tanh**를 사용할 때 주로 효과적입니다. 이들 함수는 입력값이 0 근처일 때 선형적인 특성을 가지므로, 활성값(activation)의 분산을 입력과 출력에서 최대한 유지하여 정보가 잘 전달되도록 하는 것이 목표입니다.
- **원리:** 이전 층의 노드 수($n_{in}$)와 다음 층의 노드 수($n_{out}$)를 모두 고려하여 초기화합니다.

- **균등 분포 (Uniform Distribution) 사용 시:**
  $$ W \sim U\left[-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}}\right] $$

- **정규 분포 (Normal Distribution) 사용 시:**
  $$ W \sim N\left(0, \sqrt{\frac{2}{n_{in} + n_{out}}}\right) $$

### 2.2. He 초기화 (He Initialization)

- **제안 배경:** 활성화 함수로 **ReLU** 계열(ReLU, Leaky ReLU 등)을 사용할 때 주로 효과적입니다. ReLU는 입력이 양수일 때는 그대로 전달하지만, 음수일 때는 0으로 만들어버려 활성값의 절반 정도가 사라집니다. He 초기화는 이 점을 보완합니다.
- **원리:** ReLU를 통과한 후에도 활성값의 분산을 유지하기 위해, 입력 층의 노드 수($n_{in}$)만을 고려하여 분산을 조절합니다. Xavier 초기화보다 더 넓은 분포를 가집니다.

- **균등 분포 (Uniform Distribution) 사용 시:**
  $$ W \sim U\left[-\sqrt{\frac{6}{n_{in}}}, \sqrt{\frac{6}{n_{in}}}\right] $$

- **정규 분포 (Normal Distribution) 사용 시:**
  $$ W \sim N\left(0, \sqrt{\frac{2}{n_{in}}}\right) $$

---

## 3. 어떤 초기화 방법을 선택해야 하는가?

| 활성화 함수 | 추천 초기화 방법 | 이유 |
|---|---|---|
| **Sigmoid, Tanh** | **Xavier/Glorot 초기화** | 활성화 함수의 출력 중심이 0이거나 0.5이고, 입력이 0 주변에 분포할 때 가장 효과적이기 때문입니다. |
| **ReLU, Leaky ReLU, PReLU, ELU** | **He 초기화** | ReLU 계열 함수가 입력의 절반을 0으로 만드는 특성을 고려하여 분산을 보정해주기 때문입니다. |

올바른 가중치 초기화는 안정적인 딥러닝 모델 학습의 첫걸음이며, 사용하는 활성화 함수에 맞는 초기화 방법을 선택하는 것이 매우 중요합니다. 대부분의 최신 딥러닝 프레임워크는 이러한 초기화 기법을 기본 옵션으로 제공합니다.
