# 5.3.2. 가치 기반 학습 (Value-Based Learning)

**가치 기반 학습(Value-Based Learning)**은 최적 행동 가치 함수(Optimal Q-function)를 학습하고, 이를 통해 간접적으로 최적 정책을 따르는 방식입니다. 즉, 각 상태에서 어떤 행동의 가치가 가장 높은지를 학습하여, 가장 가치가 높은 행동을 선택하는 정책(예: ε-greedy policy)을 사용합니다.

- **Q-러닝 (Q-Learning):** 대표적인 오프-폴리시(Off-policy) 시간차 학습(Temporal-Difference Learning) 알고리즘입니다. '오프-폴리시'란, 행동을 선택하는 정책과 학습(가치 함수 업데이트)에 사용되는 정책이 다를 수 있음을 의미합니다. Q-러닝은 실제 에이전트가 어떤 행동을 하든, 항상 최적이라고 가정되는 행동(최대 Q값)을 이용해 Q함수를 업데이트합니다.

- **SARSA (State-Action-Reward-State-Action):** 대표적인 온-폴리시(On-policy) 시간차 학습 알고리즘입니다. 이름처럼 (S, A, R, S', A')의 샘플을 이용해 학습하며, '온-폴리시'는 행동을 선택하는 정책과 학습에 사용되는 정책이 동일함을 의미합니다. 즉, 실제로 에이전트가 다음 상태(S')에서 선택한 행동(A')을 기준으로 현재 Q함수를 업데이트합니다.

- **심층 Q-네트워크 (Deep Q-Network, DQN):** Q-러닝에 심층 신경망(Deep Neural Network)을 결합한 알고리즘입니다. 상태 공간이 매우 크거나 연속적인 문제(예: 아타리 게임)에서 Q-테이블을 사용할 수 없을 때, 신경망을 이용해 Q-함수를 근사합니다.
  - **경험 재현 (Experience Replay):** 에이전트의 경험(s, a, r, s')을 리플레이 버퍼에 저장해두고, 학습 시 무작위로 샘플을 추출하여 사용합니다. 이를 통해 데이터 샘플 간의 상관관계를 줄여 학습 안정성을 높입니다.
  - **타겟 네트워크 (Target Network):** Q함수 업데이트 시, 목표(Target) Q값을 계산하는 네트워크를 별도로 두어 주기적으로 업데이트하는 기법입니다. 이를 통해 업데이트 목표값이 계속 변하는 문제를 완화하여 학습을 안정시킵니다.
