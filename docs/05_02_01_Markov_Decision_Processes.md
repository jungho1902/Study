# 5.2. 마르코프 결정 과정 (Markov Decision Processes, MDP)

**마르코프 결정 과정(MDP)**은 강화학습 문제를 수학적으로 형식화하기 위한 프레임워크입니다. 순차적인 의사결정 문제를 모델링하는 데 사용됩니다.

- **마르코프 속성 (Markov Property):** 미래의 상태는 오직 현재 상태에만 의존하며, 과거의 모든 이력과는 무관하다는 가정입니다. 즉, "현재 상태가 과거의 모든 정보를 압축하여 담고 있다"는 의미입니다. `P[St+1 | St] = P[St+1 | S1, ..., St]`

- **MDP의 구성요소:** MDP는 (S, A, P, R, γ)의 튜플로 정의됩니다.
  - **S (States):** 가능한 모든 상태의 집합.
  - **A (Actions):** 에이전트가 취할 수 있는 모든 행동의 집합.
  - **P (State Transition Probability):** 상태 s에서 행동 a를 취했을 때, 다음 상태 s'으로 전이될 확률. `P(s' | s, a)`
  - **R (Reward Function):** 상태 s에서 행동 a를 취해 다음 상태 s'에 도달했을 때 받는 즉각적인 보상. `R(s, a, s')`
  - **γ (Discount Factor):** 감가율. 0과 1 사이의 값으로, 미래 보상을 현재 가치로 환산할 때 얼마나 할인할지를 결정합니다. 1에 가까울수록 미래 보상을 중시하고, 0에 가까울수록 현재 보상을 중시합니다.
