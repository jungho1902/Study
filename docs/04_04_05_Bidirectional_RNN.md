# 양방향 RNN (Bidirectional RNN)

## 1. 양방향 RNN의 필요성

기본적인 순환 신경망(RNN, LSTM, GRU)은 **단방향(unidirectional)**으로 동작합니다. 즉, 과거의 정보(왼쪽에서 오른쪽, 또는 시간 순서대로)만을 이용하여 현재를 예측합니다. 하지만 많은 순차 데이터 문제에서는 **미래의 정보**가 현재를 이해하는 데 중요한 단서가 될 수 있습니다.

**예시: 문장 번역**
> "나는 어제 사과를 먹었다. 그 **[   ]**는 정말 맛있었다."

빈칸에 들어갈 단어를 예측하기 위해, 우리는 빈칸 이전의 "사과"라는 정보뿐만 아니라, 빈칸 이후의 "맛있었다"라는 정보도 함께 고려해야 합니다. 이처럼 특정 시점의 출력을 예측하기 위해 과거와 미래, 즉 **양방향의 문맥(context)**을 모두 활용해야 하는 경우가 많습니다.

**양방향 RNN(Bidirectional RNN, BRNN)**은 이러한 문제를 해결하기 위해 제안된 모델입니다.

## 2. 양방향 RNN의 구조

양방향 RNN은 이름 그대로, 두 개의 독립적인 RNN을 사용하여 데이터를 양방향으로 처리합니다.

1.  **순방향 RNN (Forward RNN):**
    - 입력 시퀀스를 시간 순서대로 (예: $t=1$부터 $t=T$까지) 처리합니다.
    - 각 시점에서 과거의 정보를 요약한 순방향 은닉 상태($\overrightarrow{h_t}$)를 계산합니다.

2.  **역방향 RNN (Backward RNN):**
    - 입력 시퀀스를 시간의 역순으로 (예: $t=T$부터 $t=1$까지) 처리합니다.
    - 각 시점에서 미래의 정보를 요약한 역방향 은닉 상태($\overleftarrow{h_t}$)를 계산합니다.

3.  **출력 계산:**
    - 각 시점에서 계산된 **순방향 은닉 상태**와 **역방향 은닉 상태**를 하나로 합칩니다(보통 연결(concatenate) 방식을 사용).
    - 이 합쳐진 은닉 상태($h_t = [\overrightarrow{h_t}, \overleftarrow{h_t}]$)가 해당 시점의 모든 문맥 정보(과거 + 미래)를 담게 되며, 이를 사용하여 최종 출력을 계산합니다.

**시각적 구조:**
```
      x_1 ----> x_2 ----> x_3  (입력)
       |         |         |
+-----------------------------------+
|      v         v         v        |
|  [Forward RNN Cell] -> ...        | (순방향 RNN)
|      |         |         |        |
|  h_f1 -----> h_f2 -----> h_f3     |
+-----------------------------------+
       ^         ^         ^
       |         |         |
+-----------------------------------+
|      |         |         |        |
|  ... <---- [Backward RNN Cell]    | (역방향 RNN)
|      v         v         v        |
|  h_b1 <----- h_b2 <----- h_b3     |
+-----------------------------------+
       |         |         |
   [h_f1,h_b1] [h_f2,h_b2] [h_f3,h_b3] (연결된 은닉 상태)
       |         |         |
       v         v         v
      y_1       y_2       y_3   (최종 출력)
```

## 3. 양방향 RNN의 장점과 단점

### 장점
- **풍부한 문맥 정보 활용:** 특정 시점을 예측할 때 과거와 미래의 정보를 모두 활용하므로, 단방향 RNN보다 더 높은 성능을 보이는 경우가 많습니다.
- **다양한 분야 적용:** 자연어 처리의 품사 판별(POS tagging), 개체명 인식(NER), 기계 번역 등에서 문맥을 파악하는 데 매우 효과적입니다.

### 단점
- **실시간 예측의 어려움:** 특정 시점의 출력을 계산하기 위해 **전체 시퀀스**가 입력될 때까지 기다려야 합니다. 역방향 RNN이 시퀀스의 끝에서부터 계산을 시작하기 때문입니다. 따라서, 실시간 음성 인식이나 주가 예측처럼 미래 정보를 알 수 없는 문제에는 적용하기 어렵습니다.
- **계산량 증가:** 두 개의 독립적인 RNN을 사용하므로, 단방향 RNN에 비해 파라미터 수와 계산량이 약 2배로 증가합니다.

양방향 RNN은 LSTM이나 GRU와 같은 다양한 RNN 셀과 결합하여 **양방향 LSTM(Bi-LSTM)**, **양방향 GRU(Bi-GRU)** 형태로 널리 사용되며, 자연어 처리 분야의 많은 모델에서 기본적인 구성 요소로 활용됩니다.
