# 파트 10: AI 기반 로봇 응용 (AI-Powered Robotic Applications)

## 10.1. 자율 이동 로봇 (Autonomous Mobile Robots, AMR) & 자율주행

### 10.1.1. 인식 (Perception)

**들어가며**

인식(Perception)은 로봇이 주변 환경을 '보고' '이해'하여 의미 있는 정보로 바꾸는 과정입니다. 이는 위치 추정(Localization), 경로 계획(Planning), 제어(Control)로 이어지는 **자율주행 스택(Autonomous Stack)의 가장 첫 단계**로, 시스템 전체 성능의 기반이 됩니다. 로봇은 인간의 감각기관처럼 센서를 사용하여 외부 세계로부터 데이터를 수집하고, 이 데이터를 해석하여 '장애물의 위치', '차선의 형태', '신호등의 색상'과 같은 정보를 추출합니다.

이 장에서는 로봇의 '눈' 역할을 하는 핵심 센서인 **카메라(Camera)**와 **라이다(LiDAR)**의 데이터를 처리하는 방법과, 여러 센서의 정보를 종합하여 신뢰도를 높이는 **센서 퓨전(Sensor Fusion)** 기술에 대해 알아보겠습니다.

---

**1. 카메라 (Camera) 데이터 처리**

카메라는 인간의 눈과 가장 유사한 센서로, 풍부한 색상과 질감 정보를 얻을 수 있어 주변 환경을 직관적으로 이해하는 데 매우 유용합니다. 하지만 조명 변화나 날씨에 민감하고, 거리를 직접 측정하기 어렵다는 단점이 있습니다.

- **주요 처리 기술 (파트 4.3 CV 연계):**
  - **객체 탐지 (Object Detection):** 이미지 내에서 자동차, 보행자, 표지판 등 특정 객체의 위치와 종류를 식별합니다. (예: YOLO, Faster R-CNN)
  - **의미론적 분할 (Semantic Segmentation):** 이미지를 픽셀 단위로 '이해'하여, 각 픽셀이 어떤 의미적 클래스(차도, 인도, 건물, 하늘 등)에 속하는지 구분합니다. 주행 가능 영역을 파악하는 데 핵심적입니다. (예: FCN, U-Net)
  - **차선 검출 (Lane Detection):** 주행 가능한 도로의 차선을 인식하여 주행 경로를 유지하도록 돕습니다.
  - **깊이 추정 (Depth Estimation):** 단안(monocular) 또는 양안(stereo) 카메라 이미지를 사용하여 2D 이미지로부터 3D 깊이 정보를 추정합니다.

---

**2. 라이다 (LiDAR) 데이터 처리**

라이다(LiDAR, Light Detection and Ranging)는 레이저 펄스를 발사하고, 그 빛이 물체에 반사되어 돌아오는 시간을 측정하여 거리를 계산하는 센서입니다. 이를 통해 주변 환경을 3D 포인트 클라우드(Point Cloud) 형태로 정밀하게 표현할 수 있습니다.

- **장점:**
  - 거리 측정의 정확도가 매우 높음
  - 조명 변화나 밤/낮에 관계없이 안정적인 성능
- **단점:**
  - 색상이나 질감 정보를 얻을 수 없음
  - 악천후(비, 눈, 안개)에 성능이 저하될 수 있음
  - 카메라에 비해 고가

- **주요 처리 기술:**
  - **포인트 클라우드 필터링 (Point Cloud Filtering):** 노이즈를 제거하거나, 분석에 불필요한 영역(예: 너무 멀거나 가까운 지점)을 제거합니다.
  - **포인트 클라우드 군집화 (Clustering):** 가까이 모여 있는 포인트들을 그룹화하여 개별 객체로 분리합니다. (예: DBSCAN, K-Means)
  - **포인트 클라우드 분할 (Segmentation):** 지면, 벽, 식생 등 의미 있는 단위로 포인트 클라우드를 분할합니다.
  - **3D 객체 탐지 (3D Object Detection):** 포인트 클라우드 내에서 차량, 보행자 등 객체의 3D 경계 상자(Bounding Box)와 방향을 추정합니다.

---

**3. 센서 퓨전 (Sensor Fusion)**

센서 퓨전은 여러 종류의 센서에서 얻은 데이터를 결합하여, 단일 센서만으로는 얻을 수 없는 더 정확하고(Accurate), 완전하며(Complete), 신뢰성 있는(Reliable) 환경 정보를 생성하는 기술입니다. 예를 들어, 카메라는 객체의 종류(Class)를 잘 인식하고 라이다는 정확한 거리(Distance)를 잘 측정하므로, 이 둘을 퓨전하면 '전방 20m에 있는 자동차'와 같이 훨씬 완전한 정보를 얻을 수 있습니다.

- **퓨전 레벨:**
  - **초기 퓨전 (Early Fusion / Low-level Fusion):** 원본 데이터(raw data) 레벨에서 결합. 정보 손실이 적지만, 데이터 동기화가 복잡하고 한 센서의 노이즈가 다른 데이터에 영향을 줄 수 있습니다.
  - **후기 퓨전 (Late Fusion / High-level Fusion):** 각 센서가 독립적으로 객체를 탐지한 후, 그 결과물(object list)을 결합. 구현이 용이하지만, 한 센서에서 탐지 실패 시 해당 정보가 누락될 수 있습니다.
  - **중간 퓨전 (Intermediate Fusion):** 각 센서에서 추출한 특징(feature) 레벨에서 결합하여 장단점을 절충합니다.

- **주요 상태 추정(State Estimation) 알고리즘:**

  - **칼만 필터 (Kalman Filter):**
    - **개념:** 불확실한(noisy) 측정값을 바탕으로 시스템의 **상태(State, 예: 위치, 속도)를 최적으로 추정**하는 재귀적인 필터입니다.
    - **가정:** 시스템이 **선형(linear)**이고, 노이즈가 **가우시안 분포(Gaussian distribution)**를 따른다고 가정합니다.
    - **프로세스:**
      1. **예측 (Prediction):** 이전 상태를 기반으로 현재 상태를 예측합니다.
      2. **업데이트 (Update):** 실제 센서 측정값을 사용하여 예측된 상태를 보정합니다.
    - **예시:** 이동하는 물체의 위치와 속도를 연속적으로 추정하는 데 사용됩니다.

  - **확장 칼만 필터 (Extended Kalman Filter, EKF):**
    - **개념:** 칼만 필터가 선형 시스템에만 적용 가능한 한계를 극복하기 위해, 비선형 시스템을 **선형으로 근사(linearization)**하여 칼만 필터를 적용하는 기법입니다.
    - **동작 원리:** 비선형 함수를 현재 추정치 지점에서 **테일러 급수(Taylor series)**로 전개하고, 1차 항까지만 사용하여 선형으로 근사합니다. (자코비안 행렬 사용)
    - **한계:** 강한 비선형 시스템에서는 근사 오차가 커져 필터가 발산할 수 있습니다.

  - **무향 칼만 필터 (Unscented Kalman Filter, UKF):** EKF보다 더 정교하게 비선형 시스템을 다루는 기법으로, 비선형 함수 자체를 근사하는 대신 확률 분포를 대표하는 샘플 포인트(시그마 포인트)를 사용하여 더 정확한 추정을 수행합니다.

**정리**

로봇의 인식 기술은 자율 시스템의 성능과 안전을 좌우하는 핵심 요소입니다. 카메라는 풍부한 시각 정보를, 라이다는 정밀한 3D 공간 정보를 제공하며, 이들을 효과적으로 융합하는 센서 퓨전 기술(특히 칼만 필터 계열)을 통해 로봇은 비로소 주변 환경을 강건하게 이해하고 신뢰성 있는 판단을 내릴 수 있게 됩니다.
