# 역전파 알고리즘 (Backpropagation)

## 1. 개요

**역전파(Backpropagation)**는 '오차 역전파'의 줄임말로, 인공 신경망을 학습시키는 데 사용되는 핵심 알고리즘입니다. 이 알고리즘은 모델의 예측 값과 실제 값 사이의 오차를 계산한 다음, 그 오차를 신경망의 출력층에서부터 입력층 방향으로 거꾸로 전파하면서 각 층의 가중치(Weight)와 편향(Bias)을 조정합니다.

역전파의 목표는 **손실 함수(Loss Function)의 값을 최소화**하는 것입니다. 이를 위해 **경사 하강법(Gradient Descent)**을 사용하여 손실 함수에 대한 각 가중치의 그래디언트(기울기)를 계산하고, 이 그래디언트를 이용해 가중치를 업데이트합니다.

## 2. 핵심 원리: 연쇄 법칙 (Chain Rule)

역전파는 미분의 **연쇄 법칙(Chain Rule)**을 기반으로 동작합니다. 신경망은 여러 함수가 중첩된 복잡한 합성 함수와 같습니다. 연쇄 법칙은 이러한 합성 함수의 미분을 효율적으로 계산할 수 있게 해줍니다.

예를 들어, $y = f(u)$이고 $u = g(x)$일 때, $y$를 $x$에 대해 미분하면 다음과 같습니다.
$$ \frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx} $$

신경망에서 손실 함수 $L$에 대한 특정 가중치 $w$의 그래디언트($\frac{\partial L}{\partial w}$)를 계산하려면, 출력층에서부터 해당 가중치가 있는 층까지 연쇄 법칙을 순차적으로 적용해야 합니다.

## 3. 역전파의 과정

역전파 과정은 크게 두 단계로 나눌 수 있습니다.

### 3.1. 순전파 (Forward Propagation)
1. 입력 데이터가 입력층에서 출발하여 신경망을 통과합니다.
2. 각 층에서 입력은 가중치와 곱해지고 편향과 더해진 후, 활성화 함수를 거쳐 다음 층으로 전달됩니다.
3. 이 과정이 출력층까지 계속되어 최종 예측 값이 계산됩니다.
4. 계산된 예측 값과 실제 정답 값을 사용하여 손실 함수를 통해 오차(Loss)를 계산합니다.

### 3.2. 역전파 (Backward Propagation)
1. **출력층의 그래디언트 계산:** 먼저, 손실 함수에 대한 출력층의 가중치와 편향의 그래디언트를 계산합니다.
2. **그래디언트 역전파:** 계산된 그래디언트를 이전 층(은닉층)으로 거꾸로 전파합니다. 연쇄 법칙에 따라, 출력층에서 전달된 그래디언트와 현재 층의 로컬 그래디언트(현재 층의 출력에 대한 입력의 미분)를 곱하여 새로운 그래디언트를 계산합니다.
3. **가중치 업데이트:** 각 층에서 계산된 그래디언트를 사용하여 경사 하강법을 통해 가중치와 편향을 업데이트합니다. 업데이트 규칙은 다음과 같습니다.
   $$ w_{\text{new}} = w_{\text{old}} - \eta \frac{\partial L}{\partial w_{\text{old}}} $$
   - $\eta$ (에타): 학습률(Learning Rate)로, 가중치를 얼마나 크게 업데이트할지 결정하는 하이퍼파라미터입니다.

이 순전파와 역전파 과정은 손실 함수의 값이 충분히 작아지거나 정해진 학습 횟수(epoch)에 도달할 때까지 반복됩니다.

## 4. 시각적 이해

```
          [입력]
             |
             v (순전파)
      +--------------+
      |  가중치 w1   |
      +--------------+
             |
             v
        [은닉층 1]
             |
             v (순전파)
      +--------------+
      |  가중치 w2   |
      +--------------+
             |
             v
          [출력] ----> [실제값] -> [손실 계산]
             ^                        |
             | (역전파)               |
      +--------------+              |
      | ∂L/∂w2 계산  | <--------------+ (오차 전파)
      |  w2 업데이트 |
      +--------------+
             ^
             | (역전파)
      +--------------+
      | ∂L/∂w1 계산  |
      |  w1 업데이트 |
      +--------------+
             ^
             |
           [입력]
```

이처럼 역전파는 신경망의 각 파라미터가 최종 오차에 얼마나 기여했는지를 효율적으로 계산하여 모델을 학습시키는 강력하고 핵심적인 알고리즘입니다.
