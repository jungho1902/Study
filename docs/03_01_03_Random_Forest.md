# 3.1.3. 랜덤 포레스트 (Random Forest)

**랜덤 포레스트(Random Forest)**는 **앙상블 학습(Ensemble Learning)**의 대표적인 알고리즘 중 하나로, 여러 개의 **결정 트리(Decision Tree)**를 결합하여 예측 성능을 높이는 모델입니다. 이름 그대로, 여러 개의 결정 트리가 모여 하나의 '숲(Forest)'을 이루는 형태입니다.

랜덤 포레스트는 **배깅(Bagging)**을 기반으로 하지만, 여기에 한 가지 아이디어를 더 추가하여 개별 트리의 독립성을 높이고 모델 전체의 성능을 극대화합니다.

## 1. 랜덤 포레스트의 동작 원리

랜덤 포레스트는 다음과 같은 과정으로 학습하고 예측합니다.

1.  **데이터 샘플링 (부트스트랩)**:
    - 원본 훈련 데이터셋에서 **부트스트랩(Bootstrap)** 방식을 사용하여 여러 개의 서브 데이터셋을 만듭니다. (배깅과 동일)
    - 즉, 중복을 허용하여 무작위로 데이터를 샘플링합니다.

2.  **결정 트리 학습 (특성 무작위 선택 추가)**:
    - 각 서브 데이터셋에 대해 하나의 결정 트리를 학습시킵니다.
    - **(랜덤 포레스트의 핵심)** 각 트리의 노드를 분기할 때, 전체 특성(feature) 중에서 **무작위로 일부 특성만 선택**하고, 선택된 특성들 중에서만 최적의 분기 기준을 찾습니다.
    - 예를 들어, 10개의 특성이 있다면, 각 노드에서 무작위로 3개(사용자가 지정하는 하이퍼파라미터)의 특성만 고르고, 그 3개 안에서만 최적의 분할을 찾는 식입니다.

3.  **예측 (투표 또는 평균)**:
    - 학습된 모든 결정 트리의 예측 결과를 취합하여 최종 예측을 결정합니다.
    - **분류(Classification)**: 각 트리가 예측한 클래스 레이블을 모아 **다수결 투표(Majority Vote)**로 최종 클래스를 결정합니다.
    - **회귀(Regression)**: 각 트리가 예측한 값들의 **평균(Average)**을 최종 예측값으로 사용합니다.

## 2. 배깅과의 차이점 및 효과

랜덤 포레스트는 배깅에 **특성(feature)의 무작위성**을 추가한 것입니다.

- **배깅**: 데이터의 무작위성만 존재. 각 트리는 노드를 분할할 때마다 모든 특성을 고려하여 최적의 분할을 찾습니다. 이 때문에, 특정 강력한 특성이 있다면 많은 트리들이 그 특성을 상위 노드에서 사용하게 되어 트리들이 서로 비슷해지는 경향이 있습니다.
- **랜덤 포레스트**: 데이터의 무작위성 + **특성의 무작위성**. 각 노드에서 고려할 수 있는 특성의 수를 제한함으로써, 개별 트리들이 서로 다른 특성을 기반으로 성장하도록 유도합니다.

이러한 특성의 무작위성은 개별 트리들 간의 **상관관계(correlation)를 감소**시키는 효과를 가져옵니다. 서로 다른 패턴을 학습한 다양한 트리들이 모이게 되므로, 앙상블 모델 전체의 **분산(Variance)이 더 효과적으로 감소**하고, 이는 곧 모델의 일반화 성능 향상과 과대적합 방지로 이어집니다.

## 3. 특성 중요도 (Feature Importance)

랜덤 포레스트는 모델을 학습한 후 각 특성이 예측에 얼마나 중요한지를 평가하는 **특성 중요도** 정보를 제공할 수 있습니다.

계산 방식은 주로 다음과 같습니다. 특정 특성의 값을 무작위로 섞었을 때(permutation), 모델의 예측 정확도가 얼마나 감소하는지를 측정합니다. 만약 정확도가 크게 감소했다면, 해당 특성은 모델의 예측에 매우 중요한 역할을 하는 것으로 판단할 수 있습니다. 이 정보는 어떤 특성이 타겟 변수에 큰 영향을 미치는지 이해하는 데 도움을 줍니다.

## 4. 랜덤 포레스트의 장단점

### 장점
- **높은 정확도**: 현존하는 많은 머신러닝 알고리즘 중에서 매우 높은 성능을 자랑합니다.
- **과대적합에 강함**: 여러 트리를 결합하는 방식 덕분에 단일 결정 트리에 비해 과대적합에 매우 강합니다.
- **대용량 데이터 처리 용이**: 데이터의 크기가 커도 잘 동작하며, 수천 개의 입력 변수(특성)를 다룰 수 있습니다.
- **사용의 편리성**: 데이터 스케일링과 같은 전처리 과정이 크게 중요하지 않아 사용하기 편리합니다.
- **특성 중요도 제공**: 모델이 어떤 특성을 중요하게 생각하는지 알 수 있습니다.

### 단점
- **해석의 어려움**: 단일 결정 트리와 달리, 수많은 트리가 결합되어 있어 모델의 내부 동작을 직관적으로 이해하기 어렵습니다. (블랙박스 모델에 가까움)
- **속도와 메모리**: 훈련 데이터셋이 매우 크고 트리의 수가 많아지면 학습 속도가 느려지고 상당한 메모리를 차지할 수 있습니다.
- **편향 문제**: (드물지만) 특정 편향을 가진 데이터로 학습할 경우, 그 편향이 모델에 반영될 수 있습니다.

---

## 예제 및 풀이 (Examples and Solutions)

### 예제 1: 랜덤 포레스트 학습 과정

**문제:** 100명의 환자 데이터(특성 10개)를 바탕으로 암 발병 여부를 예측하는 랜덤 포레스트 모델(트리 100개)을 만든다고 가정합시다. 이 모델의 첫 번째 트리(Tree 1)와 두 번째 트리(Tree 2)가 어떻게 다르게 학습되는지 개념적으로 설명하시오.

**풀이:**
랜덤 포레스트는 **데이터의 무작위성**과 **특성의 무작위성**이라는 두 가지 핵심 요소를 통해 다양한 트리들을 만듭니다.

1.  **Tree 1의 학습 과정:**
    - **데이터 샘플링:** 100명의 원본 환자 데이터에서 중복을 허용하여 100개의 데이터를 무작위로 뽑습니다 (부트스트랩). 이 과정에서 어떤 환자는 여러 번 뽑히고, 어떤 환자는 한 번도 뽑히지 않을 수 있습니다.
    - **첫 번째 분기:**
      - 10개의 전체 특성(예: 나이, 성별, 혈압 등) 중에서 **무작위로 3개**의 특성(예: 혈압, BMI, 흡연 여부)만 선택합니다.
      - 선택된 3개의 특성 중에서만 데이터를 가장 잘 나누는 최적의 분기 기준(예: '혈압 > 140')을 찾습니다.
    - **이후 분기:**
      - 나누어진 각 노드에서도, 다시 전체 10개 특성 중 무작위로 3개를 뽑고, 그 안에서 최적의 분기를 찾는 과정을 반복합니다.
    - 이렇게 하나의 결정 트리(Tree 1)가 완성됩니다.

2.  **Tree 2의 학습 과정:**
    - **데이터 샘플링:** 다시 100명의 원본 데이터에서 부트스트랩 방식으로 100개의 데이터를 뽑습니다. Tree 1을 만들 때와는 다른 구성의 데이터셋이 만들어집니다.
    - **첫 번째 분기:**
      - 다시 10개의 전체 특성 중에서 **무작위로 3개**를 선택합니다. 이번에는 (예: 나이, 혈당, 가족력)이 선택될 수 있습니다.
      - 이 3개의 특성 내에서 최적의 분기 기준(예: '나이 > 50')을 찾습니다.
    - **이후 분기:**
      - Tree 1과 마찬가지로, 각 노드에서 계속해서 특성을 무작위로 일부만 선택하여 분기를 반복합니다.
    - 이렇게 Tree 1과는 다른 데이터와 다른 특성 조합으로 학습된 두 번째 결정 트리(Tree 2)가 완성됩니다.

**결론:**
이러한 과정을 100번 반복하여 각각 다른 데이터와 다른 특성 조합으로 학습된 100개의 '개성 있는' 트리를 만듭니다. 새로운 환자 데이터가 들어오면, 이 100개 트리의 예측을 다수결로 종합하여 최종 결정을 내립니다. 이 과정을 통해 단일 트리의 단점인 과대적합을 방지하고 매우 안정적이고 정확한 모델을 만들 수 있습니다.
