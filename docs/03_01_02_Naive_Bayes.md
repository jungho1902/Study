# 3.1.2. 나이브 베이즈 분류기 (Naive Bayes Classifier)

**나이브 베이즈 분류기(Naive Bayes Classifier)**는 **베이즈 정리(Bayes' Theorem)**에 기반한 간단하면서도 강력한 확률적 분류 알고리즘입니다. 주로 텍스트 분류(예: 스팸 메일 필터링, 감성 분석)와 같이 고차원 데이터를 다루는 문제에서 뛰어난 성능을 보입니다.

## 1. 베이즈 정리 (Bayes' Theorem)

베이즈 정리는 두 확률 변수의 조건부 확률과 사전 확률 사이의 관계를 나타내는 정리입니다. 이를 분류 문제에 적용하면 다음과 같이 표현할 수 있습니다.

> **P(y | X) = [ P(X | y) * P(y) ] / P(X)**

각 항은 다음과 같은 의미를 가집니다.
- **P(y | X)**: **사후 확률 (Posterior Probability)**. 주어진 데이터 `X`가 있을 때, 이 데이터가 클래스 `y`에 속할 확률. 우리가 최종적으로 구하고자 하는 값입니다.
- **P(X | y)**: **가능도 (Likelihood)**. 클래스 `y`가 주어졌을 때, 데이터 `X`가 나타날 확률.
- **P(y)**: **사전 확률 (Prior Probability)**. 전체 데이터에서 클래스 `y`가 나타날 확률.
- **P(X)**: **증거 (Evidence)**. 데이터 `X`가 나타날 확률.

나이브 베이즈 분류는 모든 클래스 `y`에 대해 사후 확률 `P(y|X)`를 각각 계산하고, 이 확률값이 가장 높은 클래스를 예측 결과로 선택합니다. 이 과정을 **최대 사후 확률 추정(Maximum A Posteriori, MAP)**이라고 합니다.

## 2. "나이브(Naive)"한 가정

베이즈 정리를 그대로 계산하려면 `P(X|y)`를 계산해야 합니다. 데이터 `X`가 여러 개의 특성(feature) `x₁, x₂, ..., xₙ`으로 구성되어 있을 때, `P(x₁, x₂, ..., xₙ | y)`를 계산하는 것은 특성 간의 상호작용을 모두 고려해야 하므로 매우 복잡합니다.

나이브 베이즈 분류기는 여기서 매우 단순하지만 효과적인, 즉 **"나이브(Naive)"한 가정**을 도입합니다. 바로 **모든 특성들은 클래스 `y`가 주어졌을 때 서로 조건부 독립(conditionally independent)이라는 가정**입니다.

> **P(X | y) = P(x₁|y) * P(x₂|y) * ... * P(xₙ|y)**

이 가정 덕분에 복잡했던 가능도 `P(X|y)` 계산이 각 특성에 대한 확률들의 곱으로 단순화됩니다. 현실에서는 특성들이 서로 영향을 주는 경우가 많아 이 가정이 완벽하게 성립하지는 않지만, 그럼에도 불구하고 나이브 베이즈는 많은 실제 문제에서 준수한 성능을 보입니다.

## 3. 나이브 베이즈 분류기의 종류

데이터 특성의 종류에 따라 주로 세 가지 유형의 나이브 베이즈 분류기가 사용됩니다.

- **가우시안 나이브 베이즈 (Gaussian Naive Bayes)**:
  - 키, 몸무게와 같은 **연속적인(continuous)** 특성을 다룰 때 사용됩니다.
  - 각 클래스별로 특성들이 **정규 분포(가우시안 분포)**를 따른다고 가정하고, 해당 분포의 평균과 분산을 통해 가능도를 계산합니다.

- **다항분포 나이브 베이즈 (Multinomial Naive Bayes)**:
  - 텍스트 분류에서 **단어의 등장 횟수(term frequency)**와 같이, 각 특성이 이산적인(discrete) 카운트 값을 가질 때 주로 사용됩니다.
  - 예를 들어, 스팸 메일 분류에서 '광고'라는 단어가 3번 등장한 것과 같은 정보를 모델링합니다.

- **베르누이 나이브 베이즈 (Bernoulli Naive Bayes)**:
  - 다항분포 나이브 베이즈와 유사하지만, 특성의 값을 카운트하는 대신 **존재 여부(0 또는 1)**만을 다룹니다.
  - 예를 들어, 특정 단어가 문서에 포함되었는지 아닌지만을 이진(binary) 특성으로 사용합니다.

## 4. 나이브 베이즈의 장단점

### 장점
- **빠른 속도**: 계산이 매우 간단하여 훈련과 예측 속도가 매우 빠릅니다.
- **고차원 데이터에 효과적**: "차원의 저주" 문제에 비교적 강하며, 텍스트 데이터와 같이 특성이 매우 많은 데이터셋에서도 잘 작동합니다.
- **적은 데이터로도 준수한 성능**: 비교적 적은 훈련 데이터만으로도 좋은 성능을 내는 경우가 많습니다.
- **다중 클래스 분류**: 이진 분류뿐만 아니라 다중 클래스 분류 문제에도 자연스럽게 적용할 수 있습니다.

### 단점
- **조건부 독립 가정의 한계**: 모든 특성이 독립적이라는 가정은 현실 세계에서는 거의 성립하지 않으므로, 특성 간의 상호작용이 중요한 문제에서는 성능이 저하될 수 있습니다.
- **제로 빈도 문제 (Zero-Frequency Problem)**: 훈련 데이터에 나타나지 않은 특성 값이 테스트 데이터에 나타날 경우, 해당 특성의 확률이 0이 되어 전체 사후 확률이 0이 되는 문제가 발생할 수 있습니다. (이는 **라플라스 스무딩(Laplace Smoothing)**과 같은 기법으로 완화할 수 있습니다.)
- **연속형 데이터 처리**: 가우시안 나이브 베이즈는 특성이 정규 분포를 따른다고 가정하므로, 이 가정이 틀릴 경우 성능이 좋지 않을 수 있습니다.

---

## 예제 및 풀이 (Examples and Solutions)

### 예제 1: 스팸 메일 필터링

**문제:** "무료 당첨"이라는 문구가 포함된 새로운 이메일이 도착했습니다. 이 이메일이 스팸일 확률을 나이브 베이즈 분류기를 이용하여 어떻게 판단하는지 개념적으로 설명하시오.

- **클래스(y):** {스팸, 정상}
- **특성(X):** 이메일에 포함된 단어들. 여기서는 "무료", "당첨" 두 단어만 고려합니다.

**풀이:**
우리의 목표는 `P(스팸 | "무료", "당첨")`과 `P(정상 | "무료", "당첨")`을 비교하여 더 큰 값을 가진 클래스로 이메일을 분류하는 것입니다.

1.  **사전 확률 `P(y)` 계산:**
    - 훈련 데이터셋 전체에서 '스팸' 메일과 '정상' 메일의 비율을 계산합니다.
    - 예: `P(스팸) = 0.3` (전체 메일 중 30%가 스팸), `P(정상) = 0.7`

2.  **가능도 `P(X|y)` 계산:**
    - **"나이브" 가정:** "무료"라는 단어의 등장과 "당첨"이라는 단어의 등장은 서로 독립적이라고 가정합니다.
    - `P("무료", "당첨" | 스팸) = P("무료" | 스팸) * P("당첨" | 스팸)`
    - `P("무료", "당첨" | 정상) = P("무료" | 정상) * P("당첨" | 정상)`
    - 각 확률은 훈련 데이터에서 계산합니다.
      - `P("무료" | 스팸)`: 스팸 메일 중에서 "무료"라는 단어가 포함될 확률. (예: 0.8)
      - `P("당첨" | 스팸)`: 스팸 메일 중에서 "당첨"이라는 단어가 포함될 확률. (예: 0.7)
      - `P("무료" | 정상)`: 정상 메일 중에서 "무료"라는 단어가 포함될 확률. (예: 0.1)
      - `P("당첨" | 정상)`: 정상 메일 중에서 "당첨"이라는 단어가 포함될 확률. (예: 0.01)

3.  **사후 확률 계산 (비교):**
    - 베이즈 정리에서 분모 `P(X)`는 두 경우 모두 동일하므로, 분자 `P(X|y) * P(y)`의 크기만 비교해도 충분합니다.
    - **'스팸'일 경우:**
      - `P("무료", "당첨" | 스팸) * P(스팸) = (0.8 * 0.7) * 0.3 = 0.56 * 0.3 = 0.168`
    - **'정상'일 경우:**
      - `P("무료", "당첨" | 정상) * P(정상) = (0.1 * 0.01) * 0.7 = 0.001 * 0.7 = 0.0007`

**답:**
'스팸'일 경우의 값(0.168)이 '정상'일 경우의 값(0.0007)보다 훨씬 크므로, 나이브 베이즈 분류기는 이 이메일을 **스팸**으로 예측합니다.
