# 3.1.2. 서포트 벡터 머신 (Support Vector Machines, SVM)

**서포트 벡터 머신(Support Vector Machines, SVM)**은 분류(Classification)와 회귀(Regression)에 모두 사용될 수 있는 강력하고 다재다능한 지도 학습 모델입니다. SVM은 두 클래스 간의 경계를 정의하는 최적의 **초평면(Hyperplane)**을 찾는 것을 목표로 합니다.

## 1. SVM의 핵심 아이디어: 마진 최대화 (Maximizing the Margin)

SVM의 가장 중요한 아이디어는 두 클래스를 구분하는 결정 경계(초평면) 중에서, 각 클래스의 가장 가까운 데이터 포인트와의 거리, 즉 **마진(Margin)**을 최대화하는 경계를 찾는 것입니다.

- **초평면 (Hyperplane)**: n차원 공간에서 데이터를 둘로 나누는 n-1차원의 부분 공간. (2차원에서는 직선, 3차원에서는 평면)
- **마진 (Margin)**: 초평면과 가장 가까운 양쪽 클래스의 데이터 포인트(들) 사이의 거리.
- **서포트 벡터 (Support Vectors)**: 마진의 경계에 위치한 데이터 포인트들. 이름 그대로 이 데이터 포인트들이 초평면을 "서포트(지지)"하고 있으며, 이들의 위치가 결정 경계를 결정하는 데 결정적인 역할을 합니다. 다른 데이터 포인트들은 결정 경계에 영향을 주지 않습니다.

마진을 최대화하는 이유는 모델의 **일반화(Generalization)** 성능을 높이기 위함입니다. 마진이 클수록 새로운 데이터에 대해 더 안정적으로 분류할 수 있는, 즉 더 잘 일반화된 모델이라고 할 수 있습니다.

## 2. 하드 마진과 소프트 마진 (Hard Margin vs. Soft Margin)

- **하드 마진 (Hard Margin)**:
  - 모든 훈련 데이터가 마진 바깥쪽에 완벽하게 분류되어야 한다고 가정합니다.
  - 데이터가 선형적으로 완벽하게 분리될 수 있을 때만 사용할 수 있습니다.
  - 이상치(Outlier)에 매우 민감하여, 이상치 하나 때문에 결정 경계가 크게 바뀔 수 있습니다.

- **소프트 마진 (Soft Margin)**:
  - 약간의 오분류나 마진 위반(margin violation)을 허용하여 더 유연한 모델을 만드는 방식입니다.
  - 대부분의 실제 데이터는 완벽하게 선형 분리가 불가능하므로, 소프트 마진 방식이 더 널리 사용됩니다.
  - 하이퍼파라미터 `C`를 통해 규제의 강도를 조절합니다.
    - `C` 값이 크면: 마진 위반을 거의 허용하지 않으려는 하드 마진에 가까워집니다. 모델이 훈련 데이터에 과대적합될 수 있습니다.
    - `C` 값이 작으면: 마진 위반을 많이 허용하여 마진을 최대한 넓히려 합니다. 모델이 과소적합될 수 있습니다.

## 3. 커널 트릭 (The Kernel Trick)

현실의 많은 데이터는 직선(초평면)으로 분리할 수 없는 **비선형(non-linear)** 구조를 가집니다. SVM은 이러한 비선형 문제를 해결하기 위해 **커널 트릭(Kernel Trick)**이라는 강력한 기술을 사용합니다.

**아이디어**: 현재 차원에서는 선형 분리가 불가능한 데이터를, 더 높은 차원의 공간으로 **매핑(mapping)**하여 선형 분리가 가능하도록 만드는 것입니다.

![Kernel Trick](https://raw.githubusercontent.com/logic-know/ml-course/master/images/svm-kernel-trick.png)
*(이미지 출처: logic-know.github.io)*

위 그림처럼 1차원에서는 분리할 수 없는 데이터를 2차원으로 매핑하여 직선으로 분리할 수 있습니다.

**커널 트릭**: 실제로 데이터를 고차원으로 변환하는 것은 막대한 계산 비용을 필요로 합니다. 커널 트릭은 데이터를 실제로 변환하지 않으면서, 고차원 공간에서 두 벡터의 내적(dot product)을 계산한 것과 동일한 결과를 얻게 해주는 **커널 함수(Kernel Function)**를 사용하는 기법입니다. 이 덕분에 계산적으로 효율적인 비선형 분류가 가능해집니다.

**주요 커널 함수:**
- **선형 커널 (Linear Kernel)**: 커널을 사용하지 않은 것과 동일. 데이터가 선형적으로 분리 가능할 때 사용합니다.
- **다항식 커널 (Polynomial Kernel)**: 데이터를 다항식 형태의 고차원으로 매핑합니다.
- **가우시안 RBF 커널 (Gaussian Radial Basis Function Kernel)**: 데이터를 무한 차원의 공간으로 매핑하는 효과를 내며, 가장 널리 사용되는 커널 중 하나입니다. 복잡한 결정 경계를 만들 수 있습니다.
- **시그모이드 커널 (Sigmoid Kernel)**: 로지스틱 회귀의 시그모이드 함수와 유사한 형태를 가집니다.

## 4. SVM의 장단점

### 장점
- **높은 정확도**: 특히 데이터가 명확하게 구분될 때 매우 좋은 성능을 보입니다.
- **고차원 데이터에 효과적**: 특성의 수가 샘플 수보다 많아도 잘 동작합니다.
- **메모리 효율성**: 결정 경계를 만드는 데 서포트 벡터만 사용하므로 메모리 사용량이 적습니다.
- **다재다능함**: 커널 트릭을 통해 선형/비선형 분류, 회귀 등 다양한 문제에 적용 가능합니다.

### 단점
- **느린 학습 속도**: 데이터셋이 매우 클 경우 학습 속도가 느려질 수 있습니다.
- **하이퍼파라미터 민감성**: 커널 종류와 규제 파라미터 `C` 등 하이퍼파라미터 설정에 따라 성능이 크게 달라집니다.
- **확률 추정 불가**: 예측 결과가 특정 클래스에 속할 확률 정보를 직접 제공하지 않습니다. (별도의 보정 과정이 필요)
- **해석의 어려움**: RBF 커널 등을 사용한 비선형 모델의 경우, 모델의 내부 동작을 해석하기 어렵습니다.

---

## 예제 및 풀이 (Examples and Solutions)

### 예제 1: 최대 마진 분류기

**문제:** 아래 그림은 두 클래스의 데이터를 분리하는 세 개의 다른 결정 경계(초평면) H₁, H₂, H₃를 보여줍니다. SVM의 관점에서 어떤 결정 경계가 가장 최적인지, 그리고 그 이유는 무엇인지 설명하시오.

![SVM Margin](https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/400px-SVM_margin.png)
*(이미지 출처: 위키미디어)*

**풀이:**

1.  **H₁ 분석:** 이 결정 경계는 데이터를 완벽하게 분류하지 못합니다. 일부 데이터 포인트가 잘못된 클래스 영역에 위치해 있습니다. 따라서 최적의 결정 경계가 아닙니다.

2.  **H₂ 분석:** 이 결정 경계는 모든 데이터를 올바르게 분류합니다. 하지만, 파란색 클래스의 데이터 포인트 하나와 매우 가깝게 위치해 있습니다. 즉, 마진(margin)이 매우 좁습니다. 마진이 좁은 모델은 새로운 데이터의 작은 변화에도 예측이 쉽게 흔들릴 수 있어 일반화 성능이 떨어질 수 있습니다.

3.  **H₃ 분석:** 이 결정 경계는 모든 데이터를 올바르게 분류하면서, 양쪽 클래스의 가장 가까운 데이터 포인트(서포트 벡터)로부터의 거리를 최대로 하고 있습니다. 즉, **마진을 최대화**하고 있습니다.

**답:**
**H₃**가 가장 최적의 결정 경계입니다.

**이유:**
SVM은 단순히 데이터를 분류하는 것을 넘어, **가장 안정적인(robust)** 결정 경계를 찾는 것을 목표로 합니다. H₃는 가장 넓은 마진을 확보함으로써, 아직 보지 못한 새로운 데이터가 경계 근처에 나타나더라도 가장 정확하게 분류할 가능성이 높습니다. 이는 모델의 **일반화 성능**이 가장 높을 것임을 시사합니다.
