# 활성화 함수 (Activation Functions)

## 1. 활성화 함수의 역할과 필요성

### 역할
활성화 함수는 인공 신경망에서 뉴런의 최종 출력 값을 결정하는 함수입니다. 이전 층으로부터 전달받은 가중합(Weighted Sum)을 입력으로 받아, 이를 특정 범위의 값으로 변환하여 다음 층으로 전달합니다.

### 필요성: 비선형성(Non-linearity) 도입
만약 활성화 함수가 없다면, 신경망의 각 층은 단순히 선형 변환(입력에 가중치를 곱하고 더하는 것)을 반복하는 것과 같습니다. 여러 개의 선형 변환을 연속적으로 적용하더라도 그 결과는 여전히 하나의 선형 변환으로 표현될 수 있습니다.

> 예: $f(x) = ax+b$, $g(x) = cx+d$ 일 때, $g(f(x)) = c(ax+b)+d = (ac)x + (cb+d)$ 이므로 여전히 선형 함수입니다.

따라서 신경망이 복잡한 패턴을 학습하기 위해서는 **비선형 활성화 함수**를 사용하여 각 층에 비선형성을 추가해야 합니다. 이를 통해 신경망은 깊어질수록 더 복잡하고 다양한 함수를 근사할 수 있게 됩니다.

---

## 2. 주요 활성화 함수

### 2.1. 시그모이드 함수 (Sigmoid Function)

$$ \sigma(x) = \frac{1}{1 + e^{-x}} $$

- **특징:**
  - 출력을 **0과 1 사이**의 값으로 압축합니다.
  - 확률적인 해석이 가능하여, 이진 분류(Binary Classification) 모델의 출력층에서 주로 사용됩니다.
- **단점:**
  - **기울기 소실 (Vanishing Gradient) 문제:** 입력값의 절댓값이 커지면(즉, 0에서 멀어지면) 미분값이 0에 가까워집니다. 이는 역전파 과정에서 그래디언트가 소실되어 가중치 업데이트가 제대로 이루어지지 않는 원인이 됩니다.
  - **Zero-centered가 아님:** 출력값의 중심이 0.5이므로, 다음 층의 입력이 항상 양수가 됩니다. 이는 학습 효율을 저하시킬 수 있습니다.
  - 연산 비용이 상대적으로 높습니다 (지수 함수 포함).

### 2.2. 하이퍼볼릭 탄젠트 함수 (Hyperbolic Tangent, Tanh)

$$ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} $$

- **특징:**
  - 출력을 **-1과 1 사이**의 값으로 압축합니다.
  - 출력의 중심이 0 (Zero-centered)이므로, 시그모이드 함수보다 학습 효율이 좋은 경향이 있습니다.
- **단점:**
  - 시그모이드 함수와 마찬가지로, 입력값의 절댓값이 커지면 **기울기 소실 문제**가 발생합니다.

### 2.3. ReLU (Rectified Linear Unit) 함수

$$ \text{ReLU}(x) = \max(0, x) $$

- **특징:**
  - 입력이 0보다 크면 입력을 그대로 출력하고, 0보다 작으면 0을 출력합니다.
  - 연산이 매우 간단하여 계산 속도가 빠릅니다.
  - 시그모이드, Tanh 함수에 비해 기울기 소실 문제가 적어, 딥러닝 모델에서 가장 널리 사용되는 활성화 함수 중 하나입니다.
- **단점:**
  - **죽은 ReLU (Dying ReLU) 문제:** 입력이 음수일 때 항상 0을 출력하므로, 해당 뉴런의 그래디언트도 0이 됩니다. 만약 학습 과정에서 특정 뉴런으로 들어오는 입력이 계속 음수이면, 해당 뉴런은 더 이상 가중치를 업데이트하지 못하고 '죽게' 될 수 있습니다.
  - Zero-centered가 아닙니다.

### 2.4. Leaky ReLU, PReLU, ELU

`죽은 ReLU` 문제를 해결하기 위한 변형 함수들입니다.

- **Leaky ReLU:**
  $$ f(x) = \max(0.01x, x) $$
  - 입력이 음수일 때 0이 아닌 아주 작은 기울기(보통 0.01)를 가집니다. 이를 통해 뉴런이 죽는 것을 방지합니다.

- **PReLU (Parametric ReLU):**
  $$ f(x) = \max(\alpha x, x) $$
  - Leaky ReLU에서 음수 부분의 기울기 $\alpha$를 하이퍼파라미터가 아닌, **학습 가능한 파라미터**로 설정하여 데이터로부터 최적의 기울기를 학습합니다.

- **ELU (Exponential Linear Unit):**
  $$ f(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha(e^x - 1) & \text{if } x \le 0 \end{cases} $$
  - ReLU의 장점을 가지면서, 출력의 중심을 0에 가깝게 만들어줍니다.
  - 음수 입력에 대해 지수 함수 계산으로 연산 비용이 증가하는 단점이 있습니다.

### 2.5. 소프트맥스 함수 (Softmax Function)

$$ \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{K} e^{x_j}} \quad \text{for } i=1, ..., K $$

- **특징:**
  - 다중 클래스 분류(Multi-class Classification) 문제의 **출력층**에서 주로 사용됩니다.
  - K개의 클래스에 대한 모델의 예측 점수를 입력으로 받아, 각 클래스에 속할 **확률**로 변환합니다.
  - 모든 출력의 합은 항상 1이 됩니다.
  - 입력 벡터의 각 원소를 0과 1 사이의 값으로 정규화합니다.
