# MLOps ê°œìš” (Introduction to MLOps)

í˜„ëŒ€ì˜ ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ëŠ” ë” ì´ìƒ ë°ì´í„° ê³¼í•™ìê°€ ê°œì¸ ë…¸íŠ¸ë¶ì—ì„œ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ëë‚˜ëŠ” ê²ƒìœ¼ë¡œ ì™„ë£Œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ë¥¼ ì°½ì¶œí•˜ê¸° ìœ„í•´ì„œëŠ” ëª¨ë¸ì„ **í”„ë¡œë•ì…˜ í™˜ê²½ì— ë°°í¬**í•˜ê³ , **ì§€ì†ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§**í•˜ë©°, **ìƒˆë¡œìš´ ë°ì´í„°ì— ë§ì¶° ì—…ë°ì´íŠ¸**í•˜ëŠ” ë³µì¡í•œ ê³¼ì •ì„ ê±°ì³ì•¼ í•©ë‹ˆë‹¤.

**MLOps(Machine Learning Operations)**ëŠ” ì´ëŸ¬í•œ ë„ì „ê³¼ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ **ì‚¬ëŒ, í”„ë¡œì„¸ìŠ¤, ê·¸ë¦¬ê³  ê¸°ìˆ ì˜ í†µí•©ëœ ì ‘ê·¼ë²•**ìœ¼ë¡œ, ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì „ì²´ ìƒëª…ì£¼ê¸°ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  ìë™í™”í•˜ëŠ” ë°©ë²•ë¡ ì…ë‹ˆë‹¤.

---

## 1. MLOpsì˜ ì •ì˜ì™€ í•„ìš”ì„±

### 1.1. MLOpsë€ ë¬´ì—‡ì¸ê°€?

**MLOps(Machine Learning Operations)**ëŠ” ë¨¸ì‹ ëŸ¬ë‹ê³¼ DevOpsì˜ ê²°í•©ìœ¼ë¡œ, ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

> **MLOpsëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ê°œë°œ, ë°°í¬, ìš´ì˜ì„ ìœ„í•œ ì¼ë ¨ì˜ ê´€í–‰, ë„êµ¬, ê·¸ë¦¬ê³  ë¬¸í™”ë¡œì„œ, ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œì˜ ì‹ ë¢°ì„±, í™•ì¥ì„±, ê·¸ë¦¬ê³  ì„±ëŠ¥ì„ ë³´ì¥í•˜ê¸° ìœ„í•œ ì²´ê³„ì ì¸ ì ‘ê·¼ë²•ì…ë‹ˆë‹¤.**

**í•µì‹¬ êµ¬ì„±ìš”ì†Œ:**
- **ì‚¬ëŒ (People)**: ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸, ML ì—”ì§€ë‹ˆì–´, ë°ì´í„° ì—”ì§€ë‹ˆì–´, DevOps ì—”ì§€ë‹ˆì–´
- **í”„ë¡œì„¸ìŠ¤ (Process)**: ëª¨ë¸ ê°œë°œ ì›Œí¬í”Œë¡œìš°, í’ˆì§ˆ ê´€ë¦¬, ë°°í¬ ì ˆì°¨
- **ê¸°ìˆ  (Technology)**: ìë™í™” ë„êµ¬, ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ, ì¸í”„ë¼

### 1.2. MLOpsì˜ í•„ìš”ì„±

#### 1.2.1. ì—°êµ¬ì™€ í”„ë¡œë•ì…˜ ê°„ì˜ ê°„ê·¹

**ì—°êµ¬ í™˜ê²½ì˜ íŠ¹ì§•:**
```python
# ì—°êµ¬/ì‹¤í—˜ í™˜ê²½ì—ì„œì˜ ì¼ë°˜ì ì¸ ì½”ë“œ
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier

# ë°ì´í„° ë¡œë”©
df = pd.read_csv('data.csv')

# ê°„ë‹¨í•œ ì „ì²˜ë¦¬
X = df.drop('target', axis=1).fillna(0)
y = df['target']

# ëª¨ë¸ í›ˆë ¨
model = RandomForestClassifier()
model.fit(X, y)

print(f"í›ˆë ¨ ì •í™•ë„: {model.score(X, y)}")
```

**í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ í•„ìš”í•œ ê³ ë ¤ì‚¬í•­:**
```python
# í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ í•„ìš”í•œ ì½”ë“œ êµ¬ì¡°
import logging
import joblib
from datetime import datetime
from typing import Dict, Any
import pandas as pd
from sklearn.base import BaseEstimator
from monitoring import ModelMonitor
from data_validation import validate_input_data

class ProductionMLModel:
    def __init__(self, model_path: str, monitor: ModelMonitor):
        self.model = joblib.load(model_path)
        self.monitor = monitor
        self.version = "v1.2.3"
        self.logger = logging.getLogger(__name__)
        
    def predict(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        try:
            # ì…ë ¥ ë°ì´í„° ê²€ì¦
            validated_data = validate_input_data(input_data)
            
            # íŠ¹ì„± ì¶”ì¶œ
            features = self._extract_features(validated_data)
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            prediction = self.model.predict(features)[0]
            confidence = self.model.predict_proba(features)[0].max()
            
            # ëª¨ë‹ˆí„°ë§ ë¡œê¹…
            self.monitor.log_prediction(features, prediction, confidence)
            
            # ê²°ê³¼ ë°˜í™˜
            result = {
                'prediction': int(prediction),
                'confidence': float(confidence),
                'model_version': self.version,
                'timestamp': datetime.now().isoformat()
            }
            
            self.logger.info(f"Prediction made: {result}")
            return result
            
        except Exception as e:
            self.logger.error(f"Prediction failed: {str(e)}")
            raise
    
    def _extract_features(self, data: Dict[str, Any]) -> pd.DataFrame:
        # í”„ë¡œë•ì…˜ì—ì„œëŠ” ì •êµí•œ íŠ¹ì„± ì¶”ì¶œ ë¡œì§ í•„ìš”
        pass
```

#### 1.2.2. ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­

**1) ì‹ ì†í•œ ëª¨ë¸ ì—…ë°ì´íŠ¸**
- ìƒˆë¡œìš´ ë°ì´í„°ì— ë”°ë¥¸ ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ ëŒ€ì‘
- ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ë³€ê²½ì— ë”°ë¥¸ ë¹ ë¥¸ ëª¨ë¸ ì¡°ì •
- A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ

**2) í™•ì¥ì„±ê³¼ ì•ˆì •ì„±**
- ëŒ€ìš©ëŸ‰ íŠ¸ë˜í”½ ì²˜ë¦¬ ëŠ¥ë ¥
- ì‹œìŠ¤í…œ ì¥ì•  ì‹œ ìë™ ë³µêµ¬
- ì˜ˆì¸¡ ì§€ì—° ì‹œê°„ ìµœì†Œí™”

**3) ê·œì œ ë° ì»´í”Œë¼ì´ì–¸ìŠ¤**
- ëª¨ë¸ ê²°ì • ê³¼ì •ì˜ íˆ¬ëª…ì„± (ì„¤ëª… ê°€ëŠ¥í•œ AI)
- ë°ì´í„° í”„ë¼ì´ë²„ì‹œ ë³´í˜¸ (GDPR, CCPA)
- ê°ì‚¬ ì¶”ì  ê°€ëŠ¥ì„±

#### 1.2.3. ê¸°ìˆ ì  ë„ì „ê³¼ì œ

**ëª¨ë¸ ë“œë¦¬í”„íŠ¸ ë¬¸ì œ:**
```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# ë°ì´í„° ë“œë¦¬í”„íŠ¸ ì‹œë®¬ë ˆì´ì…˜
def simulate_data_drift():
    # í›ˆë ¨ ì‹œê¸° ë°ì´í„° ë¶„í¬
    train_data = np.random.normal(0, 1, 1000)
    
    # 6ê°œì›” í›„ ë°ì´í„° ë¶„í¬ (ë“œë¦¬í”„íŠ¸ ë°œìƒ)
    production_data = np.random.normal(0.5, 1.2, 1000)  # í‰ê· ê³¼ ë¶„ì‚° ë³€í™”
    
    # KS ê²€ì •ìœ¼ë¡œ ë¶„í¬ ë³€í™” ê°ì§€
    ks_statistic, p_value = stats.ks_2samp(train_data, production_data)
    
    print(f"KS Statistic: {ks_statistic:.4f}")
    print(f"P-value: {p_value:.4f}")
    
    if p_value < 0.05:
        print("âš ï¸  ë°ì´í„° ë“œë¦¬í”„íŠ¸ ê°ì§€ë¨! ëª¨ë¸ ì¬í•™ìŠµ í•„ìš”")
    else:
        print("âœ… ë°ì´í„° ë¶„í¬ ì•ˆì •")
    
    # ì‹œê°í™”
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.hist(train_data, alpha=0.7, label='Training Data', bins=30)
    plt.hist(production_data, alpha=0.7, label='Production Data', bins=30)
    plt.legend()
    plt.title('Data Distribution Comparison')
    
    plt.subplot(1, 2, 2)
    plt.plot(np.cumsum(train_data[:100]), label='Training Cumulative')
    plt.plot(np.cumsum(production_data[:100]), label='Production Cumulative')
    plt.legend()
    plt.title('Cumulative Values Over Time')
    plt.show()

simulate_data_drift()
```

**ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§:**
```python
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score
from datetime import datetime, timedelta

class ModelPerformanceTracker:
    def __init__(self):
        self.predictions = []
        self.actuals = []
        self.timestamps = []
        
    def log_prediction(self, prediction, actual=None, timestamp=None):
        if timestamp is None:
            timestamp = datetime.now()
            
        self.predictions.append(prediction)
        self.actuals.append(actual)
        self.timestamps.append(timestamp)
        
    def calculate_performance_metrics(self, window_days=7):
        # ìµœê·¼ Nì¼ê°„ì˜ ì„±ëŠ¥ ê³„ì‚°
        cutoff_date = datetime.now() - timedelta(days=window_days)
        recent_indices = [i for i, ts in enumerate(self.timestamps) if ts > cutoff_date]
        
        if not recent_indices:
            return None
            
        recent_predictions = [self.predictions[i] for i in recent_indices]
        recent_actuals = [self.actuals[i] for i in recent_indices if self.actuals[i] is not None]
        
        if len(recent_actuals) < len(recent_predictions) * 0.5:  # 50% ì´ìƒ ì‹¤ì œê°’ í•„ìš”
            print("âš ï¸  ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•œ ì¶©ë¶„í•œ ì‹¤ì œê°’ ì—†ìŒ")
            return None
            
        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        accuracy = accuracy_score(recent_actuals, recent_predictions[:len(recent_actuals)])
        f1 = f1_score(recent_actuals, recent_predictions[:len(recent_actuals)], average='weighted')
        
        return {
            'accuracy': accuracy,
            'f1_score': f1,
            'sample_size': len(recent_actuals),
            'period': f'Last {window_days} days'
        }
    
    def detect_performance_degradation(self, baseline_accuracy=0.85, threshold=0.05):
        current_metrics = self.calculate_performance_metrics()
        
        if current_metrics is None:
            return False
            
        performance_drop = baseline_accuracy - current_metrics['accuracy']
        
        if performance_drop > threshold:
            print(f"ğŸš¨ ì„±ëŠ¥ ì €í•˜ ê°ì§€: {performance_drop:.3f} í¬ì¸íŠ¸ í•˜ë½")
            print(f"í˜„ì¬ ì •í™•ë„: {current_metrics['accuracy']:.3f}")
            print(f"ê¸°ì¤€ ì •í™•ë„: {baseline_accuracy:.3f}")
            return True
        else:
            print(f"âœ… ì„±ëŠ¥ ì•ˆì •: í˜„ì¬ ì •í™•ë„ {current_metrics['accuracy']:.3f}")
            return False

# ì‚¬ìš© ì˜ˆì‹œ
tracker = ModelPerformanceTracker()

# ì˜ˆì¸¡ ë¡œê¹… ì‹œë®¬ë ˆì´ì…˜
for i in range(100):
    prediction = np.random.choice([0, 1])
    # ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” ìƒí™© ì‹œë®¬ë ˆì´ì…˜
    accuracy_decay = max(0.5, 0.9 - i * 0.005)
    actual = prediction if np.random.random() < accuracy_decay else 1 - prediction
    
    tracker.log_prediction(prediction, actual)

# ì„±ëŠ¥ ì €í•˜ ê°ì§€
tracker.detect_performance_degradation()
```

---

## 2. DevOpsì™€ MLOpsì˜ ì°¨ì´ì 

### 2.1. í•µì‹¬ ì°¨ì´ì  ë¹„êµ

| ì¸¡ë©´ | DevOps | MLOps |
|------|--------|-------|
| **ì£¼ìš” ì•„í‹°íŒ©íŠ¸** | ì½”ë“œ, ì„¤ì • íŒŒì¼ | ì½”ë“œ + ë°ì´í„° + ëª¨ë¸ + í•˜ì´í¼íŒŒë¼ë¯¸í„° |
| **ë³€ê²½ íŠ¸ë¦¬ê±°** | ì½”ë“œ ë³€ê²½ | ì½”ë“œ ë³€ê²½ + ë°ì´í„° ë³€ê²½ + ì„±ëŠ¥ ì €í•˜ |
| **í…ŒìŠ¤íŠ¸ ì¢…ë¥˜** | ë‹¨ìœ„/í†µí•©/UI í…ŒìŠ¤íŠ¸ | ë°ì´í„° í…ŒìŠ¤íŠ¸ + ëª¨ë¸ í…ŒìŠ¤íŠ¸ + ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ |
| **ë°°í¬ ë³µì¡ì„±** | ì¤‘ê°„ | ë†’ìŒ (ë°ì´í„° ì˜ì¡´ì„±, ëª¨ë¸ ê°€ì¤‘ì¹˜) |
| **ëª¨ë‹ˆí„°ë§ ë²”ìœ„** | ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ | ì‹œìŠ¤í…œ + ë°ì´í„° + ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ |
| **ë¡¤ë°± ë³µì¡ì„±** | ê°„ë‹¨ (ì½”ë“œ ë²„ì „ë§Œ ê´€ë¦¬) | ë³µì¡ (ì½”ë“œ + ë°ì´í„° + ëª¨ë¸ ë²„ì „ ê´€ë¦¬) |
| **ì˜ˆì¸¡ ê°€ëŠ¥ì„±** | ë†’ìŒ (ê²°ì •ë¡ ì ) | ë‚®ìŒ (í™•ë¥ ì , ë°ì´í„° ì˜ì¡´ì ) |

### 2.2. MLOpsë§Œì˜ ê³ ìœ í•œ ìš”ì†Œë“¤

#### 2.2.1. ë°ì´í„° ë²„ì „ ê´€ë¦¬ (Data Versioning)

**DVC(Data Version Control) ì‚¬ìš© ì˜ˆì‹œ:**
```bash
# DVC ì´ˆê¸°í™”
dvc init

# ë°ì´í„° ì¶”ê°€ ë° ë²„ì „ ê´€ë¦¬
dvc add data/raw/dataset.csv
git add data/raw/dataset.csv.dvc .gitignore
git commit -m "Add raw dataset v1.0"

# ë°ì´í„° ë³€ê²½ í›„ ìƒˆ ë²„ì „ ìƒì„±
dvc add data/processed/features_v2.csv
git add data/processed/features_v2.csv.dvc
git commit -m "Add processed features v2.0"

# íŠ¹ì • ë°ì´í„° ë²„ì „ìœ¼ë¡œ ë˜ëŒë¦¬ê¸°
git checkout data-v1.0
dvc checkout
```

#### 2.2.2. ì‹¤í—˜ ì¶”ì  (Experiment Tracking)

**MLflowë¥¼ ì‚¬ìš©í•œ ì‹¤í—˜ ê´€ë¦¬:**
```python
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score
import numpy as np

# MLflow ì‹¤í—˜ ì‹œì‘
mlflow.set_experiment("house_price_prediction")

def train_and_log_model(n_estimators, max_depth, min_samples_split, X_train, y_train, X_test, y_test):
    with mlflow.start_run():
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê¹…
        mlflow.log_param("n_estimators", n_estimators)
        mlflow.log_param("max_depth", max_depth)
        mlflow.log_param("min_samples_split", min_samples_split)
        
        # ëª¨ë¸ í›ˆë ¨
        model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            random_state=42
        )
        model.fit(X_train, y_train)
        
        # ì˜ˆì¸¡ ë° í‰ê°€
        train_pred = model.predict(X_train)
        test_pred = model.predict(X_test)
        
        train_accuracy = accuracy_score(y_train, train_pred)
        test_accuracy = accuracy_score(y_test, test_pred)
        test_f1 = f1_score(y_test, test_pred, average='weighted')
        
        # ë©”íŠ¸ë¦­ ë¡œê¹…
        mlflow.log_metric("train_accuracy", train_accuracy)
        mlflow.log_metric("test_accuracy", test_accuracy)
        mlflow.log_metric("test_f1", test_f1)
        mlflow.log_metric("overfitting", train_accuracy - test_accuracy)
        
        # ëª¨ë¸ ì €ì¥
        mlflow.sklearn.log_model(model, "model")
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™” ë° ë¡œê¹…
        import matplotlib.pyplot as plt
        
        plt.figure(figsize=(10, 6))
        feature_importance = model.feature_importances_
        plt.barh(range(len(feature_importance)), feature_importance)
        plt.title('Feature Importance')
        plt.tight_layout()
        plt.savefig('feature_importance.png')
        mlflow.log_artifact('feature_importance.png')
        
        return model

# ë‹¤ì–‘í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì‹¤í—˜ ìˆ˜í–‰
experiments = [
    {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2},
    {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5},
    {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 10}
]

# ì‹¤ì œ ë°ì´í„°ë¡œ ì‹¤í—˜ (ì˜ˆì‹œìš© ë”ë¯¸ ë°ì´í„°)
X_train, X_test = np.random.randn(1000, 10), np.random.randn(200, 10)
y_train, y_test = np.random.randint(0, 2, 1000), np.random.randint(0, 2, 200)

for exp in experiments:
    print(f"ì‹¤í—˜ ì§„í–‰: {exp}")
    train_and_log_model(X_train=X_train, y_train=y_train, 
                       X_test=X_test, y_test=y_test, **exp)
```

#### 2.2.3. ëª¨ë¸ ì„œë¹™ ì•„í‚¤í…ì²˜

**REST API ê¸°ë°˜ ëª¨ë¸ ì„œë¹™:**
```python
from flask import Flask, request, jsonify
import joblib
import numpy as np
import pandas as pd
from datetime import datetime
import logging

app = Flask(__name__)

class ModelServer:
    def __init__(self, model_path):
        self.model = joblib.load(model_path)
        self.version = "1.0.0"
        self.load_time = datetime.now()
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.prediction_count = 0
        self.total_inference_time = 0.0
        
    def predict(self, features):
        start_time = datetime.now()
        
        try:
            # ì…ë ¥ ê²€ì¦
            if not isinstance(features, (list, np.ndarray)):
                raise ValueError("Features must be a list or numpy array")
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            features_array = np.array(features).reshape(1, -1)
            prediction = self.model.predict(features_array)[0]
            probability = self.model.predict_proba(features_array)[0]
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            inference_time = (datetime.now() - start_time).total_seconds()
            self.prediction_count += 1
            self.total_inference_time += inference_time
            
            result = {
                'prediction': int(prediction),
                'probability': probability.tolist(),
                'model_version': self.version,
                'inference_time_ms': inference_time * 1000,
                'timestamp': datetime.now().isoformat()
            }
            
            self.logger.info(f"Prediction completed: {result}")
            return result
            
        except Exception as e:
            self.logger.error(f"Prediction failed: {str(e)}")
            raise

    def get_health_status(self):
        avg_inference_time = (self.total_inference_time / self.prediction_count 
                             if self.prediction_count > 0 else 0)
        
        return {
            'status': 'healthy',
            'model_version': self.version,
            'uptime': str(datetime.now() - self.load_time),
            'prediction_count': self.prediction_count,
            'average_inference_time_ms': avg_inference_time * 1000
        }

# ëª¨ë¸ ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
model_server = ModelServer('model.pkl')

@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.get_json()
        features = data.get('features')
        
        if features is None:
            return jsonify({'error': 'Features not provided'}), 400
        
        result = model_server.predict(features)
        return jsonify(result)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/health', methods=['GET'])
def health():
    return jsonify(model_server.get_health_status())

@app.route('/metrics', methods=['GET'])
def metrics():
    # Prometheus í˜•ì‹ì˜ ë©”íŠ¸ë¦­ ì œê³µ
    metrics = f"""# HELP model_predictions_total Total number of predictions made
# TYPE model_predictions_total counter
model_predictions_total {model_server.prediction_count}

# HELP model_inference_duration_seconds Time spent on inference
# TYPE model_inference_duration_seconds summary
model_inference_duration_seconds_sum {model_server.total_inference_time}
model_inference_duration_seconds_count {model_server.prediction_count}
"""
    return metrics, 200, {'Content-Type': 'text/plain'}

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=False)
```

---

## 3. MLOps ì„±ìˆ™ë„ ëª¨ë¸

MLOps êµ¬í˜„ì€ ë‹¨ê³„ì ìœ¼ë¡œ ë°œì „í•˜ë©°, Googleì—ì„œ ì œì•ˆí•œ ì„±ìˆ™ë„ ëª¨ë¸ì„ ê¸°ì¤€ìœ¼ë¡œ ì¡°ì§ì˜ MLOps ìˆ˜ì¤€ì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 3.1. Level 0: Manual Process (ìˆ˜ë™ í”„ë¡œì„¸ìŠ¤)

**íŠ¹ì§•:**
- ë°ì´í„° ê³¼í•™ìê°€ ìˆ˜ë™ìœ¼ë¡œ ëª¨ë“  ì‘ì—… ìˆ˜í–‰
- Jupyter ë…¸íŠ¸ë¶ ì¤‘ì‹¬ì˜ ì‹¤í—˜ì  ì ‘ê·¼
- ëª¨ë¸ ë°°í¬ëŠ” ì¼íšŒì„±, ìˆ˜ë™ ì‘ì—…

**êµ¬í˜„ ì˜ˆì‹œ:**
```python
# Level 0 - ì „í˜•ì ì¸ ìˆ˜ë™ ì›Œí¬í”Œë¡œìš°
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import joblib

# 1. ë°ì´í„° ë¡œë”© (ìˆ˜ë™)
print("ë°ì´í„° ë¡œë”©...")
data = pd.read_csv('data.csv')

# 2. ì „ì²˜ë¦¬ (ìˆ˜ë™)
print("ì „ì²˜ë¦¬ ì¤‘...")
X = data.drop('target', axis=1)
y = data['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 3. ëª¨ë¸ í›ˆë ¨ (ìˆ˜ë™)
print("ëª¨ë¸ í›ˆë ¨ ì¤‘...")
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)

# 4. í‰ê°€ (ìˆ˜ë™)
score = model.score(X_test, y_test)
print(f"ì •í™•ë„: {score:.3f}")

# 5. ëª¨ë¸ ì €ì¥ (ìˆ˜ë™)
joblib.dump(model, 'model_v1.pkl')
print("ëª¨ë¸ ì €ì¥ ì™„ë£Œ")

# ë°°í¬ëŠ” ìˆ˜ë™ìœ¼ë¡œ íŒŒì¼ ë³µì‚¬...
```

### 3.2. Level 1: ML Pipeline Automation (ML íŒŒì´í”„ë¼ì¸ ìë™í™”)

**íŠ¹ì§•:**
- ëª¨ë¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ìë™í™”
- ì§€ì†ì  í›ˆë ¨ (Continuous Training, CT)
- ì‹¤í—˜ ì¶”ì  ë° ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë„ì…

**êµ¬í˜„ ì˜ˆì‹œ:**
```python
# Level 1 - ìë™í™”ëœ ML íŒŒì´í”„ë¼ì¸
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
import mlflow
import mlflow.sklearn

class MLTrainingPipeline:
    def __init__(self, experiment_name):
        mlflow.set_experiment(experiment_name)
        
    def create_pipeline(self):
        pipeline_options = PipelineOptions()
        
        with beam.Pipeline(options=pipeline_options) as pipeline:
            # ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
            (pipeline
             | 'ReadData' >> beam.io.ReadFromText('gs://bucket/data/*.csv')
             | 'ParseCSV' >> beam.Map(self.parse_csv)
             | 'Preprocess' >> beam.Map(self.preprocess)
             | 'TrainModel' >> beam.Map(self.train_model)
             | 'EvaluateModel' >> beam.Map(self.evaluate_model)
             | 'RegisterModel' >> beam.Map(self.register_model))
    
    def parse_csv(self, line):
        # CSV íŒŒì‹± ë¡œì§
        pass
    
    def preprocess(self, data):
        # ì „ì²˜ë¦¬ ë¡œì§
        pass
    
    def train_model(self, processed_data):
        with mlflow.start_run():
            # ëª¨ë¸ í›ˆë ¨ ë¡œì§
            model = RandomForestClassifier()
            # ... í›ˆë ¨ ì½”ë“œ ...
            mlflow.sklearn.log_model(model, "model")
            return model
    
    def evaluate_model(self, model):
        # í‰ê°€ ë¡œì§
        pass
    
    def register_model(self, model):
        # ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ë“±ë¡
        pass

# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
pipeline = MLTrainingPipeline("automated_training")
pipeline.create_pipeline()
```

### 3.3. Level 2: CI/CD Pipeline Automation (CI/CD íŒŒì´í”„ë¼ì¸ ìë™í™”)

**íŠ¹ì§•:**
- ì™„ì „í•œ CI/CD/CT êµ¬í˜„
- ìë™í™”ëœ í…ŒìŠ¤íŠ¸ ë° ê²€ì¦
- í”„ë¡œë•ì…˜ ë°°í¬ ìë™í™”

**GitHub Actionsë¥¼ í™œìš©í•œ CI/CD ì˜ˆì‹œ:**
```yaml
# .github/workflows/mlops-pipeline.yml
name: MLOps Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 2 * * 0'  # ë§¤ì£¼ ì¼ìš”ì¼ 2ì‹œì— ì¬í›ˆë ¨

jobs:
  data-validation:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: 3.8
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
    
    - name: Data Quality Tests
      run: |
        python tests/test_data_quality.py
        python tests/test_data_schema.py
    
    - name: Data Drift Detection
      run: |
        python src/data_drift_detection.py
  
  model-training:
    needs: data-validation
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    
    - name: Train Model
      run: |
        python src/train_model.py
    
    - name: Model Validation Tests
      run: |
        python tests/test_model_performance.py
        python tests/test_model_bias.py
    
    - name: Upload Model Artifacts
      uses: actions/upload-artifact@v2
      with:
        name: trained-model
        path: models/
  
  model-deployment:
    needs: model-training
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
    - name: Download Model
      uses: actions/download-artifact@v2
      with:
        name: trained-model
        path: models/
    
    - name: Deploy to Staging
      run: |
        docker build -t model-server:latest .
        docker tag model-server:latest registry.com/model-server:staging
        docker push registry.com/model-server:staging
    
    - name: Integration Tests
      run: |
        python tests/test_api_integration.py
    
    - name: Deploy to Production
      if: success()
      run: |
        kubectl apply -f k8s/deployment.yaml
        kubectl rollout status deployment/model-server
```

**ìë™í™”ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸:**
```python
# tests/test_model_performance.py
import unittest
import joblib
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, f1_score

class TestModelPerformance(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.model = joblib.load('models/latest_model.pkl')
        cls.test_data = pd.read_csv('data/test_set.csv')
        cls.X_test = cls.test_data.drop('target', axis=1)
        cls.y_test = cls.test_data['target']
    
    def test_minimum_accuracy(self):
        """ëª¨ë¸ì´ ìµœì†Œ ì •í™•ë„ë¥¼ ë§Œì¡±í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸"""
        predictions = self.model.predict(self.X_test)
        accuracy = accuracy_score(self.y_test, predictions)
        
        minimum_accuracy = 0.85
        self.assertGreaterEqual(
            accuracy, minimum_accuracy,
            f"ëª¨ë¸ ì •í™•ë„ {accuracy:.3f}ê°€ ìµœì†Œ ê¸°ì¤€ {minimum_accuracy}ë¥¼ ë§Œì¡±í•˜ì§€ ì•ŠìŒ"
        )
    
    def test_f1_score(self):
        """F1 ì ìˆ˜ê°€ ê¸°ì¤€ì„ ë§Œì¡±í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸"""
        predictions = self.model.predict(self.X_test)
        f1 = f1_score(self.y_test, predictions, average='weighted')
        
        minimum_f1 = 0.80
        self.assertGreaterEqual(
            f1, minimum_f1,
            f"F1 ì ìˆ˜ {f1:.3f}ê°€ ìµœì†Œ ê¸°ì¤€ {minimum_f1}ë¥¼ ë§Œì¡±í•˜ì§€ ì•ŠìŒ"
        )
    
    def test_prediction_time(self):
        """ì˜ˆì¸¡ ì‹œê°„ì´ í—ˆìš© ë²”ìœ„ ë‚´ì¸ì§€ í…ŒìŠ¤íŠ¸"""
        import time
        
        start_time = time.time()
        self.model.predict(self.X_test[:100])  # 100ê°œ ìƒ˜í”Œ ì˜ˆì¸¡
        prediction_time = time.time() - start_time
        
        max_time_per_sample = 0.01  # 10ms per sample
        avg_time = prediction_time / 100
        
        self.assertLessEqual(
            avg_time, max_time_per_sample,
            f"í‰ê·  ì˜ˆì¸¡ ì‹œê°„ {avg_time*1000:.2f}msê°€ ê¸°ì¤€ {max_time_per_sample*1000}msë¥¼ ì´ˆê³¼"
        )
    
    def test_model_bias(self):
        """ëª¨ë¸ í¸í–¥ì„± í…ŒìŠ¤íŠ¸"""
        # ì„±ë³„ ë“± ë¯¼ê°í•œ ì†ì„±ì— ëŒ€í•œ ê³µì •ì„± ê²€ì¦
        if 'gender' in self.X_test.columns:
            male_indices = self.X_test['gender'] == 'M'
            female_indices = self.X_test['gender'] == 'F'
            
            male_predictions = self.model.predict(self.X_test[male_indices])
            female_predictions = self.model.predict(self.X_test[female_indices])
            
            male_positive_rate = np.mean(male_predictions)
            female_positive_rate = np.mean(female_predictions)
            
            bias_threshold = 0.1  # 10% ì°¨ì´ê¹Œì§€ í—ˆìš©
            bias = abs(male_positive_rate - female_positive_rate)
            
            self.assertLessEqual(
                bias, bias_threshold,
                f"ì„±ë³„ ê°„ í¸í–¥ {bias:.3f}ì´ í—ˆìš© ê¸°ì¤€ {bias_threshold}ë¥¼ ì´ˆê³¼"
            )

if __name__ == '__main__':
    unittest.main()
```

---

## 4. MLOps ë„êµ¬ ë° ê¸°ìˆ  ìŠ¤íƒ

### 4.1. MLOps ë„êµ¬ ìƒíƒœê³„

**ì¹´í…Œê³ ë¦¬ë³„ ì£¼ìš” ë„êµ¬ë“¤:**

| ì¹´í…Œê³ ë¦¬ | ë„êµ¬ | ì„¤ëª… | ì‚¬ìš© ì˜ˆì‹œ |
|----------|------|------|----------|
| **ë°ì´í„° ë²„ì „ ê´€ë¦¬** | DVC, Pachyderm | ë°ì´í„°ì™€ ëª¨ë¸ì˜ ë²„ì „ ê´€ë¦¬ | ë°ì´í„°ì…‹ ë³€ê²½ ì´ë ¥ ì¶”ì  |
| **ì‹¤í—˜ ì¶”ì ** | MLflow, Weights & Biases, Neptune | ì‹¤í—˜ ë¡œê¹… ë° ë¹„êµ | í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ ê´€ë¦¬ |
| **ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬** | MLflow Model Registry, Seldon Core | ëª¨ë¸ ì €ì¥ ë° ë²„ì „ ê´€ë¦¬ | í”„ë¡œë•ì…˜ ëª¨ë¸ ê´€ë¦¬ |
| **íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜** | Kubeflow, Apache Airflow, Prefect | ì›Œí¬í”Œë¡œìš° ìë™í™” | ë°ì´í„° ì²˜ë¦¬ â†’ í›ˆë ¨ â†’ ë°°í¬ |
| **ëª¨ë¸ ì„œë¹™** | TensorFlow Serving, Seldon, BentoML | ëª¨ë¸ API ì„œë¹„ìŠ¤ | REST APIë¡œ ì˜ˆì¸¡ ì œê³µ |
| **ëª¨ë‹ˆí„°ë§** | Prometheus, Grafana, Evidently | ì‹œìŠ¤í…œ ë° ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ | ì„±ëŠ¥ ì €í•˜ ì•Œë¦¼ |
| **ì¸í”„ë¼** | Kubernetes, Docker, AWS SageMaker | ì»¨í…Œì´ë„ˆí™” ë° í´ë¼ìš°ë“œ ë°°í¬ | í™•ì¥ ê°€ëŠ¥í•œ ëª¨ë¸ ì„œë¹™ |

### 4.2. í†µí•©ëœ MLOps í”Œë«í¼ êµ¬ì¶•

**Dockerë¥¼ í™œìš©í•œ ì»¨í…Œì´ë„ˆí™”:**
```dockerfile
# Dockerfile
FROM python:3.8-slim

# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
WORKDIR /app

# ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY src/ ./src/
COPY models/ ./models/
COPY config/ ./config/

# í—¬ìŠ¤ì²´í¬ ìŠ¤í¬ë¦½íŠ¸
COPY healthcheck.py .

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 5000

# í—¬ìŠ¤ì²´í¬ ì„¤ì •
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
  CMD python healthcheck.py

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
CMD ["python", "src/model_server.py"]
```

**Kubernetes ë°°í¬ ì„¤ì •:**
```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-model-server
  labels:
    app: ml-model-server
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ml-model-server
  template:
    metadata:
      labels:
        app: ml-model-server
    spec:
      containers:
      - name: model-server
        image: your-registry/ml-model-server:latest
        ports:
        - containerPort: 5000
        env:
        - name: MODEL_PATH
          value: "/app/models/latest_model.pkl"
        - name: MONITORING_ENDPOINT
          value: "http://prometheus:9090"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        readinessProbe:
          httpGet:
            path: /health
            port: 5000
          initialDelaySeconds: 10
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 5000
          initialDelaySeconds: 30
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: ml-model-service
spec:
  selector:
    app: ml-model-server
  ports:
    - protocol: TCP
      port: 80
      targetPort: 5000
  type: LoadBalancer
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ml-model-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ml-model-server
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

---

## 5. MLOps íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì‹¤ìŠµ

### 5.1. ì¢…ë‹¨ê°„ MLOps íŒŒì´í”„ë¼ì¸ ì˜ˆì œ

**í”„ë¡œì íŠ¸ êµ¬ì¡°:**
```
mlops-project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ schemas/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ingestion.py
â”‚   â”‚   â””â”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â””â”€â”€ predict.py
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ drift_detection.py
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ server.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_data_quality.py
â”‚   â”œâ”€â”€ test_model_performance.py
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml
â”‚   â””â”€â”€ model_config.yaml
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploratory_analysis.ipynb
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â””â”€â”€ service.yaml
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ mlops-pipeline.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

**ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬:**
```python
# src/data/ingestion.py
import pandas as pd
import numpy as np
from datetime import datetime
import yaml
import logging

class DataIngestion:
    def __init__(self, config_path):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.logger = logging.getLogger(__name__)
        
    def collect_data(self, source_type='database'):
        """ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ìˆ˜ì§‘"""
        if source_type == 'database':
            return self._collect_from_database()
        elif source_type == 'api':
            return self._collect_from_api()
        elif source_type == 'file':
            return self._collect_from_file()
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤ íƒ€ì…: {source_type}")
    
    def _collect_from_database(self):
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° ë°ì´í„° ìˆ˜ì§‘
        import sqlalchemy
        
        engine = sqlalchemy.create_engine(self.config['database']['connection_string'])
        query = self.config['database']['query']
        
        df = pd.read_sql(query, engine)
        self.logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ {len(df)}ê°œ í–‰ ìˆ˜ì§‘")
        
        return df
    
    def _collect_from_api(self):
        # APIì—ì„œ ë°ì´í„° ìˆ˜ì§‘
        import requests
        
        api_url = self.config['api']['url']
        headers = self.config['api']['headers']
        
        response = requests.get(api_url, headers=headers)
        data = response.json()
        
        df = pd.DataFrame(data)
        self.logger.info(f"APIì—ì„œ {len(df)}ê°œ í–‰ ìˆ˜ì§‘")
        
        return df
    
    def _collect_from_file(self):
        # íŒŒì¼ì—ì„œ ë°ì´í„° ìˆ˜ì§‘
        file_path = self.config['file']['path']
        df = pd.read_csv(file_path)
        self.logger.info(f"íŒŒì¼ì—ì„œ {len(df)}ê°œ í–‰ ìˆ˜ì§‘")
        
        return df
    
    def validate_data_schema(self, df):
        """ë°ì´í„° ìŠ¤í‚¤ë§ˆ ê²€ì¦"""
        expected_columns = self.config['schema']['columns']
        missing_columns = set(expected_columns) - set(df.columns)
        
        if missing_columns:
            raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
        
        # ë°ì´í„° íƒ€ì… ê²€ì¦
        for col, expected_type in self.config['schema']['types'].items():
            if col in df.columns:
                if not df[col].dtype.name.startswith(expected_type):
                    self.logger.warning(f"ì»¬ëŸ¼ {col}ì˜ íƒ€ì…ì´ ì˜ˆìƒê³¼ ë‹¤ë¦„: {df[col].dtype} vs {expected_type}")
        
        return True
    
    def check_data_quality(self, df):
        """ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬"""
        quality_report = {}
        
        # ê²°ì¸¡ì¹˜ ë¹„ìœ¨
        missing_ratio = df.isnull().sum() / len(df)
        quality_report['missing_ratio'] = missing_ratio.to_dict()
        
        # ì¤‘ë³µ í–‰ ë¹„ìœ¨
        duplicate_ratio = df.duplicated().sum() / len(df)
        quality_report['duplicate_ratio'] = duplicate_ratio
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì˜ ì´ìƒì¹˜ ê²€ì‚¬
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        outliers = {}
        
        for col in numeric_columns:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            
            outlier_count = ((df[col] < (Q1 - 1.5 * IQR)) | 
                           (df[col] > (Q3 + 1.5 * IQR))).sum()
            outliers[col] = outlier_count / len(df)
        
        quality_report['outlier_ratio'] = outliers
        
        self.logger.info(f"ë°ì´í„° í’ˆì§ˆ ë³´ê³ ì„œ: {quality_report}")
        return quality_report
```

**ëª¨ë¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸:**
```python
# src/models/train.py
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import joblib
import pandas as pd
import numpy as np
from datetime import datetime

class ModelTrainer:
    def __init__(self, config):
        self.config = config
        mlflow.set_experiment(config['experiment_name'])
        
    def train_model(self, X, y, model_params=None):
        """ëª¨ë¸ í›ˆë ¨ ë° MLflow ë¡œê¹…"""
        with mlflow.start_run():
            # ë°ì´í„° ë¶„í• 
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •
            if model_params is None:
                model_params = self.config['model']['default_params']
            
            # ëª¨ë¸ ì´ˆê¸°í™”
            model = RandomForestClassifier(**model_params, random_state=42)
            
            # êµì°¨ ê²€ì¦
            cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_weighted')
            
            # ëª¨ë¸ í›ˆë ¨
            model.fit(X_train, y_train)
            
            # ì˜ˆì¸¡ ë° í‰ê°€
            y_pred = model.predict(X_test)
            
            # MLflowì— íŒŒë¼ë¯¸í„° ë° ë©”íŠ¸ë¦­ ë¡œê¹…
            mlflow.log_params(model_params)
            mlflow.log_metric("cv_mean_f1", cv_scores.mean())
            mlflow.log_metric("cv_std_f1", cv_scores.std())
            mlflow.log_metric("test_f1", self._calculate_f1(y_test, y_pred))
            mlflow.log_metric("test_accuracy", self._calculate_accuracy(y_test, y_pred))
            
            # í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
            self._log_confusion_matrix(y_test, y_pred)
            
            # íŠ¹ì„± ì¤‘ìš”ë„ ë¡œê¹…
            self._log_feature_importance(model, X.columns)
            
            # ëª¨ë¸ ì €ì¥
            mlflow.sklearn.log_model(
                model, 
                "model",
                registered_model_name=self.config['model']['registered_name']
            )
            
            # ë¡œì»¬ì—ë„ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_path = f"models/model_{timestamp}.pkl"
            joblib.dump(model, model_path)
            
            return model, {
                'cv_mean_f1': cv_scores.mean(),
                'cv_std_f1': cv_scores.std(),
                'test_f1': self._calculate_f1(y_test, y_pred),
                'test_accuracy': self._calculate_accuracy(y_test, y_pred)
            }
    
    def _calculate_f1(self, y_true, y_pred):
        from sklearn.metrics import f1_score
        return f1_score(y_true, y_pred, average='weighted')
    
    def _calculate_accuracy(self, y_true, y_pred):
        from sklearn.metrics import accuracy_score
        return accuracy_score(y_true, y_pred)
    
    def _log_confusion_matrix(self, y_true, y_pred):
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        cm = confusion_matrix(y_true, y_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title('Confusion Matrix')
        plt.ylabel('Actual')
        plt.xlabel('Predicted')
        plt.tight_layout()
        plt.savefig('confusion_matrix.png')
        mlflow.log_artifact('confusion_matrix.png')
        plt.close()
    
    def _log_feature_importance(self, model, feature_names):
        import matplotlib.pyplot as plt
        
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        plt.figure(figsize=(10, 8))
        plt.barh(importance_df['feature'][:20], importance_df['importance'][:20])
        plt.title('Top 20 Feature Importances')
        plt.xlabel('Importance')
        plt.tight_layout()
        plt.savefig('feature_importance.png')
        mlflow.log_artifact('feature_importance.png')
        plt.close()
        
        # CSVë¡œë„ ì €ì¥
        importance_df.to_csv('feature_importance.csv', index=False)
        mlflow.log_artifact('feature_importance.csv')
```

**ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ:**
```python
# src/monitoring/drift_detection.py
import pandas as pd
import numpy as np
from scipy import stats
from datetime import datetime, timedelta
import logging

class DataDriftDetector:
    def __init__(self, reference_data):
        self.reference_data = reference_data
        self.logger = logging.getLogger(__name__)
        
    def detect_drift(self, current_data, significance_level=0.05):
        """ë°ì´í„° ë“œë¦¬í”„íŠ¸ ê°ì§€"""
        drift_results = {}
        
        numeric_columns = self.reference_data.select_dtypes(include=[np.number]).columns
        categorical_columns = self.reference_data.select_dtypes(include=['object']).columns
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ë“œë¦¬í”„íŠ¸ ê²€ì‚¬ (KS Test)
        for col in numeric_columns:
            if col in current_data.columns:
                ks_stat, p_value = stats.ks_2samp(
                    self.reference_data[col].dropna(),
                    current_data[col].dropna()
                )
                
                drift_results[col] = {
                    'test': 'ks_test',
                    'statistic': ks_stat,
                    'p_value': p_value,
                    'drift_detected': p_value < significance_level
                }
        
        # ë²”ì£¼í˜• ì»¬ëŸ¼ ë“œë¦¬í”„íŠ¸ ê²€ì‚¬ (Chi-square Test)
        for col in categorical_columns:
            if col in current_data.columns:
                ref_counts = self.reference_data[col].value_counts()
                cur_counts = current_data[col].value_counts()
                
                # ê³µí†µ ì¹´í…Œê³ ë¦¬ë§Œ ë¹„êµ
                common_categories = set(ref_counts.index) & set(cur_counts.index)
                
                if len(common_categories) > 1:
                    ref_freq = ref_counts[common_categories]
                    cur_freq = cur_counts[common_categories]
                    
                    # ë¹ˆë„ë¥¼ ë¹„ìœ¨ë¡œ ë³€í™˜
                    ref_prop = ref_freq / ref_freq.sum()
                    cur_prop = cur_freq / cur_freq.sum()
                    
                    chi2_stat, p_value = stats.chisquare(cur_prop, ref_prop)
                    
                    drift_results[col] = {
                        'test': 'chi2_test',
                        'statistic': chi2_stat,
                        'p_value': p_value,
                        'drift_detected': p_value < significance_level
                    }
        
        # ì „ì²´ ë“œë¦¬í”„íŠ¸ ìš”ì•½
        total_features = len(drift_results)
        drifted_features = sum([r['drift_detected'] for r in drift_results.values()])
        drift_ratio = drifted_features / total_features if total_features > 0 else 0
        
        summary = {
            'timestamp': datetime.now().isoformat(),
            'total_features': total_features,
            'drifted_features': drifted_features,
            'drift_ratio': drift_ratio,
            'results': drift_results
        }
        
        self.logger.info(f"ë“œë¦¬í”„íŠ¸ ê²€ì‚¬ ì™„ë£Œ: {drifted_features}/{total_features} íŠ¹ì„±ì—ì„œ ë“œë¦¬í”„íŠ¸ ê°ì§€")
        
        return summary
    
    def generate_drift_report(self, drift_results):
        """ë“œë¦¬í”„íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        import matplotlib.pyplot as plt
        
        drifted_features = [
            feature for feature, result in drift_results['results'].items()
            if result['drift_detected']
        ]
        
        if drifted_features:
            fig, axes = plt.subplots(len(drifted_features), 2, 
                                   figsize=(15, 5 * len(drifted_features)))
            
            if len(drifted_features) == 1:
                axes = axes.reshape(1, -1)
            
            for i, feature in enumerate(drifted_features):
                # ë¶„í¬ ë¹„êµ
                axes[i, 0].hist(self.reference_data[feature].dropna(), 
                               alpha=0.7, label='Reference', bins=30)
                axes[i, 0].hist(self.current_data[feature].dropna(), 
                               alpha=0.7, label='Current', bins=30)
                axes[i, 0].set_title(f'{feature} - Distribution Comparison')
                axes[i, 0].legend()
                
                # P-value ì‹œê°í™”
                p_value = drift_results['results'][feature]['p_value']
                axes[i, 1].bar(['P-value'], [p_value], color='red' if p_value < 0.05 else 'green')
                axes[i, 1].axhline(y=0.05, color='r', linestyle='--', label='Significance Level')
                axes[i, 1].set_title(f'{feature} - P-value: {p_value:.4f}')
                axes[i, 1].legend()
            
            plt.tight_layout()
            plt.savefig('drift_report.png')
            plt.close()
            
            return 'drift_report.png'
        
        return None
```

**API ì„œë²„:**
```python
# src/api/server.py
from flask import Flask, request, jsonify
import joblib
import pandas as pd
import numpy as np
from datetime import datetime
import logging
import yaml
from src.monitoring.drift_detection import DataDriftDetector

app = Flask(__name__)

class MLOpsAPIServer:
    def __init__(self, config_path):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.model = joblib.load(self.config['model']['path'])
        self.model_version = self.config['model']['version']
        self.reference_data = pd.read_csv(self.config['reference_data_path'])
        
        self.drift_detector = DataDriftDetector(self.reference_data)
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.prediction_count = 0
        self.total_inference_time = 0.0
        self.predictions_log = []
        
    def predict(self, features):
        start_time = datetime.now()
        
        try:
            # ì…ë ¥ ê²€ì¦ ë° ì „ì²˜ë¦¬
            features_df = pd.DataFrame([features])
            
            # ë“œë¦¬í”„íŠ¸ ê°ì§€ (ì„ íƒì )
            if len(self.predictions_log) % 100 == 0:  # 100ë²ˆë§ˆë‹¤ ë“œë¦¬í”„íŠ¸ ì²´í¬
                recent_data = pd.DataFrame(self.predictions_log[-100:])
                if not recent_data.empty:
                    drift_results = self.drift_detector.detect_drift(recent_data)
                    if drift_results['drift_ratio'] > 0.3:  # 30% ì´ìƒ íŠ¹ì„±ì—ì„œ ë“œë¦¬í”„íŠ¸
                        self.logger.warning("ìƒë‹¹í•œ ë°ì´í„° ë“œë¦¬í”„íŠ¸ê°€ ê°ì§€ë¨!")
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            prediction = self.model.predict(features_df)[0]
            probabilities = self.model.predict_proba(features_df)[0]
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            inference_time = (datetime.now() - start_time).total_seconds()
            self.prediction_count += 1
            self.total_inference_time += inference_time
            
            # ì˜ˆì¸¡ ë¡œê¹…
            prediction_log = features.copy()
            prediction_log['prediction'] = prediction
            prediction_log['timestamp'] = start_time
            self.predictions_log.append(prediction_log)
            
            # ìµœê·¼ 1000ê°œë§Œ ìœ ì§€
            if len(self.predictions_log) > 1000:
                self.predictions_log = self.predictions_log[-1000:]
            
            result = {
                'prediction': int(prediction),
                'probabilities': probabilities.tolist(),
                'model_version': self.model_version,
                'inference_time_ms': inference_time * 1000,
                'timestamp': start_time.isoformat()
            }
            
            self.logger.info(f"ì˜ˆì¸¡ ì™„ë£Œ: {result}")
            return result
            
        except Exception as e:
            self.logger.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def get_health_status(self):
        avg_inference_time = (self.total_inference_time / self.prediction_count 
                             if self.prediction_count > 0 else 0)
        
        return {
            'status': 'healthy',
            'model_version': self.model_version,
            'prediction_count': self.prediction_count,
            'average_inference_time_ms': avg_inference_time * 1000,
            'last_prediction': self.predictions_log[-1]['timestamp'].isoformat() if self.predictions_log else None
        }
    
    def get_model_metrics(self):
        if not self.predictions_log:
            return {'message': 'ì˜ˆì¸¡ ë°ì´í„° ì—†ìŒ'}
        
        recent_predictions = self.predictions_log[-100:]  # ìµœê·¼ 100ê°œ
        prediction_distribution = pd.Series([p['prediction'] for p in recent_predictions]).value_counts()
        
        return {
            'recent_prediction_count': len(recent_predictions),
            'prediction_distribution': prediction_distribution.to_dict(),
            'average_inference_time_ms': self.total_inference_time * 1000 / self.prediction_count
        }

# API ì„œë²„ ì¸ìŠ¤í„´ìŠ¤
ml_server = MLOpsAPIServer('config/config.yaml')

@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.get_json()
        features = data.get('features')
        
        if features is None:
            return jsonify({'error': 'Features not provided'}), 400
        
        result = ml_server.predict(features)
        return jsonify(result)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/health', methods=['GET'])
def health():
    return jsonify(ml_server.get_health_status())

@app.route('/metrics', methods=['GET'])
def metrics():
    return jsonify(ml_server.get_model_metrics())

@app.route('/drift-check', methods=['POST'])
def drift_check():
    try:
        data = request.get_json()
        current_data = pd.DataFrame(data.get('data'))
        
        drift_results = ml_server.drift_detector.detect_drift(current_data)
        return jsonify(drift_results)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=False)
```

---

## ì˜ˆì œ ë° í’€ì´ (Examples and Solutions)

### ì˜ˆì œ 1: ì‹¤ì‹œê°„ ì¶”ì²œ ì‹œìŠ¤í…œì˜ MLOps êµ¬ì¶•

**ë¬¸ì œ:** ì˜¨ë¼ì¸ ì‡¼í•‘ëª°ì˜ ì‹¤ì‹œê°„ ìƒí’ˆ ì¶”ì²œ ì‹œìŠ¤í…œì„ ìœ„í•œ MLOps íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ì„¸ìš”. ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤:

**ìš”êµ¬ì‚¬í•­:**
1. ì‹¤ì‹œê°„ ì‚¬ìš©ì í–‰ë™ ë°ì´í„° ì²˜ë¦¬ (í´ë¦­, êµ¬ë§¤, í‰ì )
2. ì¼ì¼ ëª¨ë¸ ì¬í•™ìŠµ ë° ì„±ëŠ¥ ê²€ì¦
3. A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
4. ì¶”ì²œ ì„±ëŠ¥ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
5. ë ˆì´í„´ì‹œ < 100ms ë³´ì¥

**í’€ì´:**

**1ë‹¨ê³„: ì•„í‚¤í…ì²˜ ì„¤ê³„**
```python
# ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ê³„
class RecommendationMLOpsArchitecture:
    """
    ì‹¤ì‹œê°„ ì¶”ì²œ ì‹œìŠ¤í…œ MLOps ì•„í‚¤í…ì²˜
    
    Components:
    1. Data Streaming: Kafka + Spark Streaming
    2. Feature Store: Redis + Cassandra
    3. Model Training: Apache Airflow + MLflow
    4. Model Serving: TensorFlow Serving + Load Balancer
    5. Monitoring: Prometheus + Grafana + Custom Metrics
    6. A/B Testing: Custom Framework
    """
    
    def __init__(self):
        self.components = {
            'data_streaming': 'Kafka + Spark',
            'feature_store': 'Redis + Cassandra',
            'model_training': 'Airflow + MLflow',
            'model_serving': 'TF Serving',
            'monitoring': 'Prometheus + Grafana',
            'ab_testing': 'Custom Framework'
        }
```

**2ë‹¨ê³„: ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬**
```python
# ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
from kafka import KafkaConsumer, KafkaProducer
import json
import redis
from datetime import datetime
import pandas as pd

class RealtimeDataProcessor:
    def __init__(self):
        self.consumer = KafkaConsumer(
            'user-events',
            bootstrap_servers=['localhost:9092'],
            value_deserializer=lambda x: json.loads(x.decode('utf-8'))
        )
        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)
        self.producer = KafkaProducer(
            bootstrap_servers=['localhost:9092'],
            value_serializer=lambda x: json.dumps(x).encode('utf-8')
        )
    
    def process_user_events(self):
        """ì‚¬ìš©ì ì´ë²¤íŠ¸ ì‹¤ì‹œê°„ ì²˜ë¦¬"""
        for message in self.consumer:
            event = message.value
            
            # ì´ë²¤íŠ¸ íƒ€ì…ë³„ ì²˜ë¦¬
            if event['event_type'] == 'click':
                self.process_click_event(event)
            elif event['event_type'] == 'purchase':
                self.process_purchase_event(event)
            elif event['event_type'] == 'rating':
                self.process_rating_event(event)
            
            # ì‹¤ì‹œê°„ íŠ¹ì„± ì—…ë°ì´íŠ¸
            self.update_user_features(event['user_id'], event)
            
            # ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ë°ì´í„° ì „ì†¡
            self.send_monitoring_data(event)
    
    def process_click_event(self, event):
        """í´ë¦­ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        user_id = event['user_id']
        item_id = event['item_id']
        timestamp = event['timestamp']
        
        # ì‚¬ìš©ì í´ë¦­ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        click_key = f"user:{user_id}:clicks"
        self.redis_client.lpush(click_key, json.dumps({
            'item_id': item_id,
            'timestamp': timestamp
        }))
        self.redis_client.ltrim(click_key, 0, 99)  # ìµœê·¼ 100ê°œë§Œ ìœ ì§€
        
        # ì•„ì´í…œ ì¸ê¸°ë„ ì—…ë°ì´íŠ¸
        popularity_key = f"item:{item_id}:popularity"
        self.redis_client.incr(popularity_key)
        
        # CTR ê³„ì‚°ì„ ìœ„í•œ ë…¸ì¶œ ìˆ˜ ì¦ê°€
        impression_key = f"item:{item_id}:impressions"
        self.redis_client.incr(impression_key)
    
    def process_purchase_event(self, event):
        """êµ¬ë§¤ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        user_id = event['user_id']
        item_id = event['item_id']
        
        # êµ¬ë§¤ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        purchase_key = f"user:{user_id}:purchases"
        self.redis_client.lpush(purchase_key, json.dumps(event))
        
        # ì•„ì´í…œë³„ êµ¬ë§¤ ìˆ˜ ì—…ë°ì´íŠ¸
        purchase_count_key = f"item:{item_id}:purchases"
        self.redis_client.incr(purchase_count_key)
        
        # ì „í™˜ìœ¨ ê³„ì‚°ì„ ìœ„í•œ ë°ì´í„° ì—…ë°ì´íŠ¸
        conversion_key = f"item:{item_id}:conversions"
        self.redis_client.incr(conversion_key)
    
    def update_user_features(self, user_id, event):
        """ì‚¬ìš©ì íŠ¹ì„± ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸"""
        user_features_key = f"user:{user_id}:features"
        
        # í˜„ì¬ íŠ¹ì„± ì¡°íšŒ
        current_features = self.redis_client.hgetall(user_features_key)
        current_features = {k.decode(): float(v.decode()) 
                          for k, v in current_features.items()}
        
        # íŠ¹ì„± ì—…ë°ì´íŠ¸ ë¡œì§
        if event['event_type'] == 'click':
            current_features['total_clicks'] = current_features.get('total_clicks', 0) + 1
            current_features['last_click_timestamp'] = event['timestamp']
        elif event['event_type'] == 'purchase':
            current_features['total_purchases'] = current_features.get('total_purchases', 0) + 1
            current_features['total_spent'] = current_features.get('total_spent', 0) + event['amount']
        
        # Redisì— ì—…ë°ì´íŠ¸ëœ íŠ¹ì„± ì €ì¥
        self.redis_client.hmset(user_features_key, current_features)
        self.redis_client.expire(user_features_key, 86400 * 30)  # 30ì¼ TTL
    
    def send_monitoring_data(self, event):
        """ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì— ë°ì´í„° ì „ì†¡"""
        monitoring_event = {
            'timestamp': datetime.now().isoformat(),
            'event_type': event['event_type'],
            'user_id': event['user_id'],
            'item_id': event['item_id'],
            'metadata': event.get('metadata', {})
        }
        
        self.producer.send('monitoring-events', monitoring_event)

# ì‹¤ì‹œê°„ ë°ì´í„° í”„ë¡œì„¸ì„œ ì‹¤í–‰
processor = RealtimeDataProcessor()
processor.process_user_events()
```

**3ë‹¨ê³„: ëª¨ë¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ (Apache Airflow)**
```python
# airflow_dag.py - ì¼ì¼ ëª¨ë¸ ì¬í•™ìŠµ DAG
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import pandas as pd
import mlflow
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

default_args = {
    'owner': 'ml-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'recommendation_model_training',
    default_args=default_args,
    description='ì¼ì¼ ì¶”ì²œ ëª¨ë¸ ì¬í•™ìŠµ',
    schedule_interval='0 2 * * *',  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ
    catchup=False
)

def extract_training_data(**context):
    """í›ˆë ¨ ë°ì´í„° ì¶”ì¶œ"""
    import cassandra
    from cassandra.cluster import Cluster
    
    # Cassandraì—ì„œ ì‚¬ìš©ì í–‰ë™ ë°ì´í„° ì¶”ì¶œ
    cluster = Cluster(['127.0.0.1'])
    session = cluster.connect('recommendation')
    
    # ì§€ë‚œ 30ì¼ê°„ì˜ ë°ì´í„° ì¶”ì¶œ
    query = """
    SELECT user_id, item_id, event_type, timestamp, rating
    FROM user_events
    WHERE timestamp >= ?
    """
    
    thirty_days_ago = datetime.now() - timedelta(days=30)
    rows = session.execute(query, [thirty_days_ago])
    
    df = pd.DataFrame(rows)
    
    # ë°ì´í„° ì €ì¥
    df.to_parquet('/tmp/training_data.parquet')
    return '/tmp/training_data.parquet'

def train_collaborative_filtering_model(**context):
    """í˜‘ì—… í•„í„°ë§ ëª¨ë¸ í›ˆë ¨"""
    with mlflow.start_run():
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_parquet('/tmp/training_data.parquet')
        
        # ì‚¬ìš©ì-ì•„ì´í…œ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
        user_item_matrix = df.pivot_table(
            index='user_id', 
            columns='item_id', 
            values='rating',
            fill_value=0
        )
        
        # í˜‘ì—… í•„í„°ë§ ëª¨ë¸ (User-based)
        user_similarity = cosine_similarity(user_item_matrix)
        user_similarity_df = pd.DataFrame(
            user_similarity, 
            index=user_item_matrix.index, 
            columns=user_item_matrix.index
        )
        
        # ëª¨ë¸ í‰ê°€
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import mean_squared_error
        
        # í‰ê°€ ë°ì´í„° ë¶„í• 
        train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)
        
        # ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€
        predictions = []
        actuals = []
        
        for _, row in test_data.iterrows():
            user_id = row['user_id']
            item_id = row['item_id']
            actual_rating = row['rating']
            
            # ìœ ì‚¬í•œ ì‚¬ìš©ìë“¤ì˜ í‰ì ìœ¼ë¡œ ì˜ˆì¸¡
            if user_id in user_similarity_df.index:
                similar_users = user_similarity_df[user_id].sort_values(ascending=False)[1:11]  # Top 10
                
                predicted_rating = 0
                weight_sum = 0
                
                for similar_user, similarity in similar_users.items():
                    if item_id in user_item_matrix.columns and similarity > 0:
                        user_rating = user_item_matrix.loc[similar_user, item_id]
                        if user_rating > 0:
                            predicted_rating += similarity * user_rating
                            weight_sum += similarity
                
                if weight_sum > 0:
                    predicted_rating /= weight_sum
                    predictions.append(predicted_rating)
                    actuals.append(actual_rating)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        mse = mean_squared_error(actuals, predictions)
        rmse = np.sqrt(mse)
        
        # MLflowì— ë©”íŠ¸ë¦­ ë¡œê¹…
        mlflow.log_metric("rmse", rmse)
        mlflow.log_metric("mse", mse)
        mlflow.log_param("model_type", "collaborative_filtering")
        mlflow.log_param("similarity_metric", "cosine")
        
        # ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ì¥
        import joblib
        model_artifacts = {
            'user_item_matrix': user_item_matrix,
            'user_similarity': user_similarity_df
        }
        
        joblib.dump(model_artifacts, '/tmp/cf_model.pkl')
        mlflow.log_artifact('/tmp/cf_model.pkl')
        
        return rmse

def train_content_based_model(**context):
    """ì»¨í…ì¸  ê¸°ë°˜ ëª¨ë¸ í›ˆë ¨"""
    with mlflow.start_run():
        # ì•„ì´í…œ íŠ¹ì„± ë°ì´í„° ë¡œë“œ
        item_features = pd.read_parquet('/tmp/item_features.parquet')
        
        # TF-IDFë¥¼ ì‚¬ìš©í•œ ì•„ì´í…œ ìœ ì‚¬ë„ ê³„ì‚°
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.metrics.pairwise import linear_kernel
        
        # ì•„ì´í…œ ì„¤ëª…ìœ¼ë¡œë¶€í„° TF-IDF ë²¡í„° ìƒì„±
        tfidf = TfidfVectorizer(stop_words='english')
        tfidf_matrix = tfidf.fit_transform(item_features['description'])
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)
        
        # ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (ë‹¤ì–‘ì„± ë° ì»¤ë²„ë¦¬ì§€ ì¸¡ì •)
        diversity_score = np.mean(1 - cosine_sim[np.triu_indices_from(cosine_sim, k=1)])
        coverage = len(np.unique(np.argmax(cosine_sim, axis=1))) / len(item_features)
        
        # MLflowì— ë©”íŠ¸ë¦­ ë¡œê¹…
        mlflow.log_metric("diversity_score", diversity_score)
        mlflow.log_metric("coverage", coverage)
        mlflow.log_param("model_type", "content_based")
        mlflow.log_param("vectorizer", "tfidf")
        
        # ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ì¥
        import joblib
        model_artifacts = {
            'tfidf_vectorizer': tfidf,
            'cosine_similarity': cosine_sim,
            'item_features': item_features
        }
        
        joblib.dump(model_artifacts, '/tmp/content_model.pkl')
        mlflow.log_artifact('/tmp/content_model.pkl')
        
        return diversity_score

def model_validation(**context):
    """ëª¨ë¸ ê²€ì¦ ë° A/B í…ŒìŠ¤íŠ¸ ì„¤ì •"""
    # ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
    cf_rmse = context['task_instance'].xcom_pull(task_ids='train_cf_model')
    cb_diversity = context['task_instance'].xcom_pull(task_ids='train_content_model')
    
    # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
    if cf_rmse < 0.8 and cb_diversity > 0.3:  # ì„ê³„ê°’
        print("âœ… ëª¨ë¸ ì„±ëŠ¥ ê¸°ì¤€ í†µê³¼")
        
        # A/B í…ŒìŠ¤íŠ¸ ì„¤ì •
        ab_config = {
            'experiment_name': f'recommendation_ab_{datetime.now().strftime("%Y%m%d")}',
            'models': {
                'model_a': 'collaborative_filtering',
                'model_b': 'content_based'
            },
            'traffic_split': {'model_a': 0.5, 'model_b': 0.5},
            'success_metric': 'ctr',
            'minimum_sample_size': 10000
        }
        
        # A/B í…ŒìŠ¤íŠ¸ ì„¤ì • ì €ì¥
        with open('/tmp/ab_test_config.json', 'w') as f:
            json.dump(ab_config, f)
        
        return True
    else:
        print("âŒ ëª¨ë¸ ì„±ëŠ¥ ê¸°ì¤€ ë¯¸ë‹¬")
        return False

def deploy_models(**context):
    """ëª¨ë¸ ë°°í¬"""
    validation_passed = context['task_instance'].xcom_pull(task_ids='model_validation')
    
    if validation_passed:
        # TensorFlow Servingì— ëª¨ë¸ ë°°í¬
        import subprocess
        
        # ëª¨ë¸ì„ TensorFlow Serving í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ë° ë°°í¬
        commands = [
            "docker stop recommendation-server-a recommendation-server-b",
            "docker rm recommendation-server-a recommendation-server-b",
            "docker run -d --name recommendation-server-a -p 8501:8501 -v /models/cf:/models/cf -e MODEL_NAME=cf tensorflow/serving",
            "docker run -d --name recommendation-server-b -p 8502:8501 -v /models/content:/models/content -e MODEL_NAME=content tensorflow/serving"
        ]
        
        for cmd in commands:
            try:
                subprocess.run(cmd, shell=True, check=True)
            except subprocess.CalledProcessError as e:
                print(f"ë°°í¬ ëª…ë ¹ ì‹¤íŒ¨: {cmd}")
                raise
        
        print("âœ… ëª¨ë¸ ë°°í¬ ì™„ë£Œ")
    else:
        print("âŒ ê²€ì¦ ì‹¤íŒ¨ë¡œ ë°°í¬ ê±´ë„ˆë›°ê¸°")

# Airflow íƒœìŠ¤í¬ ì •ì˜
extract_data_task = PythonOperator(
    task_id='extract_training_data',
    python_callable=extract_training_data,
    dag=dag
)

train_cf_task = PythonOperator(
    task_id='train_cf_model',
    python_callable=train_collaborative_filtering_model,
    dag=dag
)

train_content_task = PythonOperator(
    task_id='train_content_model',
    python_callable=train_content_based_model,
    dag=dag
)

validation_task = PythonOperator(
    task_id='model_validation',
    python_callable=model_validation,
    dag=dag
)

deploy_task = PythonOperator(
    task_id='deploy_models',
    python_callable=deploy_models,
    dag=dag
)

# íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
extract_data_task >> [train_cf_task, train_content_task] >> validation_task >> deploy_task
```

**4ë‹¨ê³„: A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬**
```python
# ab_testing_framework.py
import random
import hashlib
import json
import redis
from datetime import datetime, timedelta
import pandas as pd

class ABTestingFramework:
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379, db=1)
        
    def assign_user_to_variant(self, user_id, experiment_name, variants):
        """ì‚¬ìš©ìë¥¼ A/B í…ŒìŠ¤íŠ¸ ë³€í˜•ì— í• ë‹¹"""
        # ì‚¬ìš©ì IDë¥¼ í•´ì‹±í•˜ì—¬ ì¼ê´€ëœ í• ë‹¹ ë³´ì¥
        user_hash = int(hashlib.md5(f"{user_id}_{experiment_name}".encode()).hexdigest(), 16)
        variant_index = user_hash % len(variants)
        
        assigned_variant = variants[variant_index]
        
        # Redisì— í• ë‹¹ ì •ë³´ ì €ì¥
        assignment_key = f"ab_test:{experiment_name}:{user_id}"
        self.redis_client.set(assignment_key, assigned_variant, ex=86400 * 30)  # 30ì¼
        
        return assigned_variant
    
    def get_user_variant(self, user_id, experiment_name):
        """ì‚¬ìš©ìì˜ í• ë‹¹ëœ ë³€í˜• ì¡°íšŒ"""
        assignment_key = f"ab_test:{experiment_name}:{user_id}"
        variant = self.redis_client.get(assignment_key)
        return variant.decode() if variant else None
    
    def log_conversion_event(self, user_id, experiment_name, variant, event_type, value=1):
        """ì „í™˜ ì´ë²¤íŠ¸ ë¡œê¹…"""
        event_data = {
            'user_id': user_id,
            'experiment_name': experiment_name,
            'variant': variant,
            'event_type': event_type,
            'value': value,
            'timestamp': datetime.now().isoformat()
        }
        
        # ì´ë²¤íŠ¸ ë¡œê·¸ ì €ì¥
        event_key = f"ab_events:{experiment_name}"
        self.redis_client.lpush(event_key, json.dumps(event_data))
        
        # ì§‘ê³„ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        metric_key = f"ab_metrics:{experiment_name}:{variant}:{event_type}"
        self.redis_client.incr(metric_key)
    
    def get_experiment_results(self, experiment_name):
        """ì‹¤í—˜ ê²°ê³¼ ì¡°íšŒ"""
        event_key = f"ab_events:{experiment_name}"
        events = self.redis_client.lrange(event_key, 0, -1)
        
        if not events:
            return {'message': 'ì‹¤í—˜ ë°ì´í„° ì—†ìŒ'}
        
        # ì´ë²¤íŠ¸ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        event_list = [json.loads(event.decode()) for event in events]
        df = pd.DataFrame(event_list)
        
        # ë³€í˜•ë³„ ì„±ëŠ¥ ë¶„ì„
        results = {}
        
        for variant in df['variant'].unique():
            variant_data = df[df['variant'] == variant]
            
            # ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°
            total_users = variant_data['user_id'].nunique()
            total_impressions = len(variant_data[variant_data['event_type'] == 'impression'])
            total_clicks = len(variant_data[variant_data['event_type'] == 'click'])
            total_purchases = len(variant_data[variant_data['event_type'] == 'purchase'])
            
            # CTR ë° ì „í™˜ìœ¨ ê³„ì‚°
            ctr = total_clicks / total_impressions if total_impressions > 0 else 0
            conversion_rate = total_purchases / total_users if total_users > 0 else 0
            
            results[variant] = {
                'users': total_users,
                'impressions': total_impressions,
                'clicks': total_clicks,
                'purchases': total_purchases,
                'ctr': ctr,
                'conversion_rate': conversion_rate
            }
        
        # í†µê³„ì  ìœ ì˜ì„± ê²€ì •
        if len(results) == 2:
            from scipy.stats import chi2_contingency
            
            variants = list(results.keys())
            var_a, var_b = variants[0], variants[1]
            
            # CTR ë¹„êµ (ì¹´ì´ì œê³± ê²€ì •)
            contingency_table = [
                [results[var_a]['clicks'], results[var_a]['impressions'] - results[var_a]['clicks']],
                [results[var_b]['clicks'], results[var_b]['impressions'] - results[var_b]['clicks']]
            ]
            
            chi2, p_value, _, _ = chi2_contingency(contingency_table)
            
            results['statistical_test'] = {
                'test': 'chi2_test',
                'chi2_statistic': chi2,
                'p_value': p_value,
                'significant': p_value < 0.05
            }
        
        return results
    
    def stop_experiment(self, experiment_name, winning_variant=None):
        """ì‹¤í—˜ ì¢…ë£Œ ë° ìŠ¹ì ê²°ì •"""
        results = self.get_experiment_results(experiment_name)
        
        if winning_variant is None:
            # ìë™ìœ¼ë¡œ ìŠ¹ì ê²°ì • (CTR ê¸°ì¤€)
            best_ctr = 0
            for variant, metrics in results.items():
                if isinstance(metrics, dict) and 'ctr' in metrics:
                    if metrics['ctr'] > best_ctr:
                        best_ctr = metrics['ctr']
                        winning_variant = variant
        
        # ì‹¤í—˜ ì¢…ë£Œ ì •ë³´ ì €ì¥
        experiment_result = {
            'experiment_name': experiment_name,
            'end_time': datetime.now().isoformat(),
            'winning_variant': winning_variant,
            'final_results': results
        }
        
        result_key = f"ab_results:{experiment_name}"
        self.redis_client.set(result_key, json.dumps(experiment_result))
        
        print(f"ì‹¤í—˜ {experiment_name} ì¢…ë£Œ. ìŠ¹ì: {winning_variant}")
        return experiment_result

# A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ ì‚¬ìš© ì˜ˆì‹œ
ab_tester = ABTestingFramework()

# ì¶”ì²œ APIì—ì„œ A/B í…ŒìŠ¤íŠ¸ ì ìš©
def get_recommendations_with_ab_test(user_id, num_recommendations=10):
    experiment_name = "recommendation_model_comparison"
    variants = ['collaborative_filtering', 'content_based']
    
    # ì‚¬ìš©ìë¥¼ ë³€í˜•ì— í• ë‹¹
    assigned_variant = ab_tester.assign_user_to_variant(user_id, experiment_name, variants)
    
    # ë…¸ì¶œ ì´ë²¤íŠ¸ ë¡œê¹…
    ab_tester.log_conversion_event(user_id, experiment_name, assigned_variant, 'impression')
    
    # í• ë‹¹ëœ ë³€í˜•ì— ë”°ë¥¸ ì¶”ì²œ ìƒì„±
    if assigned_variant == 'collaborative_filtering':
        recommendations = get_cf_recommendations(user_id, num_recommendations)
    else:
        recommendations = get_content_recommendations(user_id, num_recommendations)
    
    return {
        'recommendations': recommendations,
        'variant': assigned_variant,
        'experiment': experiment_name
    }

def track_recommendation_click(user_id, item_id, experiment_name, variant):
    """ì¶”ì²œ í´ë¦­ ì¶”ì """
    ab_tester.log_conversion_event(user_id, experiment_name, variant, 'click')

def track_recommendation_purchase(user_id, item_id, experiment_name, variant):
    """ì¶”ì²œ êµ¬ë§¤ ì¶”ì """
    ab_tester.log_conversion_event(user_id, experiment_name, variant, 'purchase')
```

**5ë‹¨ê³„: ëª¨ë‹ˆí„°ë§ ë° ì•ŒëŸ¿ ì‹œìŠ¤í…œ**
```python
# monitoring_system.py
import prometheus_client
from prometheus_client import Counter, Histogram, Gauge
import time
import pandas as pd
from datetime import datetime, timedelta

# Prometheus ë©”íŠ¸ë¦­ ì •ì˜
PREDICTION_COUNTER = Counter('ml_predictions_total', 'Total predictions made', ['model_version', 'variant'])
PREDICTION_LATENCY = Histogram('ml_prediction_duration_seconds', 'Prediction latency')
MODEL_ACCURACY = Gauge('ml_model_accuracy', 'Current model accuracy', ['model_version'])
CTR_GAUGE = Gauge('recommendation_ctr', 'Click-through rate', ['variant'])
DRIFT_SCORE = Gauge('data_drift_score', 'Data drift score', ['feature'])

class RecommendationMonitor:
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379, db=2)
        
    def track_prediction(self, model_version, variant, latency):
        """ì˜ˆì¸¡ ì¶”ì """
        PREDICTION_COUNTER.labels(model_version=model_version, variant=variant).inc()
        PREDICTION_LATENCY.observe(latency)
    
    def update_model_accuracy(self, model_version, accuracy):
        """ëª¨ë¸ ì •í™•ë„ ì—…ë°ì´íŠ¸"""
        MODEL_ACCURACY.labels(model_version=model_version).set(accuracy)
    
    def update_ctr(self, variant, ctr):
        """CTR ì—…ë°ì´íŠ¸"""
        CTR_GAUGE.labels(variant=variant).set(ctr)
    
    def update_drift_score(self, feature, drift_score):
        """ë“œë¦¬í”„íŠ¸ ì ìˆ˜ ì—…ë°ì´íŠ¸"""
        DRIFT_SCORE.labels(feature=feature).set(drift_score)
    
    def check_performance_degradation(self):
        """ì„±ëŠ¥ ì €í•˜ ì²´í¬"""
        # ìµœê·¼ 1ì‹œê°„ CTR ì¡°íšŒ
        current_time = datetime.now()
        one_hour_ago = current_time - timedelta(hours=1)
        
        # Redisì—ì„œ ìµœê·¼ CTR ë°ì´í„° ì¡°íšŒ
        ctr_key = "hourly_ctr"
        recent_ctr_data = self.redis_client.zrangebyscore(
            ctr_key, 
            one_hour_ago.timestamp(), 
            current_time.timestamp(),
            withscores=True
        )
        
        if recent_ctr_data:
            recent_ctrs = [float(score) for _, score in recent_ctr_data]
            avg_ctr = sum(recent_ctrs) / len(recent_ctrs)
            
            # ì„ê³„ê°’ ë¹„êµ (ì˜ˆ: ê¸°ì¤€ CTRì˜ 80% ì´í•˜)
            baseline_ctr = 0.05  # 5%
            threshold = baseline_ctr * 0.8
            
            if avg_ctr < threshold:
                self.send_alert(
                    "CTR ì„±ëŠ¥ ì €í•˜",
                    f"ìµœê·¼ 1ì‹œê°„ í‰ê·  CTR: {avg_ctr:.4f}, ì„ê³„ê°’: {threshold:.4f}"
                )
                return True
        
        return False
    
    def check_latency_issues(self):
        """ë ˆì´í„´ì‹œ ì´ìŠˆ ì²´í¬"""
        # Prometheusì—ì„œ ìµœê·¼ ë ˆì´í„´ì‹œ ë°ì´í„° ì¡°íšŒ
        # (ì‹¤ì œë¡œëŠ” Prometheus í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨)
        recent_latencies = self.get_recent_latencies()  # êµ¬í˜„ í•„ìš”
        
        if recent_latencies:
            p95_latency = np.percentile(recent_latencies, 95)
            
            # 95í¼ì„¼íƒ€ì¼ ë ˆì´í„´ì‹œê°€ 100ms ì´ˆê³¼
            if p95_latency > 0.1:
                self.send_alert(
                    "ë ˆì´í„´ì‹œ ì„ê³„ê°’ ì´ˆê³¼",
                    f"95í¼ì„¼íƒ€ì¼ ë ˆì´í„´ì‹œ: {p95_latency*1000:.2f}ms"
                )
                return True
        
        return False
    
    def send_alert(self, title, message):
        """ì•ŒëŸ¿ ì „ì†¡"""
        alert_data = {
            'title': title,
            'message': message,
            'timestamp': datetime.now().isoformat(),
            'severity': 'warning'
        }
        
        # Slack, Email, PagerDuty ë“±ìœ¼ë¡œ ì•ŒëŸ¿ ì „ì†¡
        print(f"ğŸš¨ ALERT: {title} - {message}")
        
        # ì•ŒëŸ¿ íˆìŠ¤í† ë¦¬ ì €ì¥
        alert_key = f"alerts:{datetime.now().strftime('%Y%m%d')}"
        self.redis_client.lpush(alert_key, json.dumps(alert_data))
    
    def run_monitoring_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì‹¤í–‰"""
        while True:
            try:
                # ì„±ëŠ¥ ì €í•˜ ì²´í¬
                if self.check_performance_degradation():
                    print("ì„±ëŠ¥ ì €í•˜ ê°ì§€ë¨")
                
                # ë ˆì´í„´ì‹œ ì´ìŠˆ ì²´í¬
                if self.check_latency_issues():
                    print("ë ˆì´í„´ì‹œ ì´ìŠˆ ê°ì§€ë¨")
                
                # 5ë¶„ë§ˆë‹¤ ì²´í¬
                time.sleep(300)
                
            except Exception as e:
                print(f"ëª¨ë‹ˆí„°ë§ ì—ëŸ¬: {e}")
                time.sleep(60)

# ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹¤í–‰
monitor = RecommendationMonitor()
# monitor.run_monitoring_loop()  # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰
```

**í•´ì„¤:**
ì´ ì˜ˆì œëŠ” ì‹¤ì‹œê°„ ì¶”ì²œ ì‹œìŠ¤í…œì„ ìœ„í•œ **ì¢…ë‹¨ê°„ MLOps íŒŒì´í”„ë¼ì¸**ì„ êµ¬í˜„í•©ë‹ˆë‹¤:

1. **ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬**: Kafkaì™€ Redisë¥¼ í™œìš©í•œ ì‹¤ì‹œê°„ ì‚¬ìš©ì í–‰ë™ ë°ì´í„° ì²˜ë¦¬
2. **ìë™í™”ëœ ëª¨ë¸ í›ˆë ¨**: Airflowë¥¼ í†µí•œ ì¼ì¼ ë°°ì¹˜ ì¬í•™ìŠµ
3. **A/B í…ŒìŠ¤íŠ¸**: ë‘ ëª¨ë¸ ê°„ì˜ ì„±ëŠ¥ì„ ì‹¤ì œ ì‚¬ìš©ì íŠ¸ë˜í”½ìœ¼ë¡œ ë¹„êµ
4. **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: Prometheusë¥¼ í†µí•œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì•ŒëŸ¿
5. **ì„±ëŠ¥ ë³´ì¥**: 100ms ë¯¸ë§Œ ë ˆì´í„´ì‹œ ìš”êµ¬ì‚¬í•­ ì¶©ì¡±

---

## í•µì‹¬ ìš”ì•½ (Key Takeaways)

### MLOpsì˜ í•µì‹¬ ê°€ì¹˜
1. **ìë™í™”**: ë°˜ë³µì ì¸ ML ì‘ì—…ì˜ ìë™í™”ë¥¼ í†µí•œ íš¨ìœ¨ì„± ì¦ëŒ€
2. **ì‹ ë¢°ì„±**: ì²´ê³„ì ì¸ í…ŒìŠ¤íŠ¸ì™€ ê²€ì¦ì„ í†µí•œ ëª¨ë¸ í’ˆì§ˆ ë³´ì¥  
3. **í™•ì¥ì„±**: ëŒ€ê·œëª¨ ì„œë¹„ìŠ¤ì—ì„œë„ ì•ˆì •ì ì¸ ëª¨ë¸ ì„œë¹™
4. **í˜‘ì—…**: ë‹¤ì–‘í•œ ì—­í•  ê°„ì˜ íš¨ê³¼ì ì¸ í˜‘ì—… ì²´ê³„ êµ¬ì¶•

### ì„±ê³µì ì¸ MLOps êµ¬ì¶• ìš”ì†Œ
1. **ë¬¸í™”**: DevOps ë¬¸í™”ì˜ MLíŒ€ í™•ì‚°
2. **í”„ë¡œì„¸ìŠ¤**: í‘œì¤€í™”ëœ ì›Œí¬í”Œë¡œìš°ì™€ í’ˆì§ˆ ê´€ë¦¬
3. **ë„êµ¬**: ì ì ˆí•œ MLOps ë„êµ¬ ìŠ¤íƒ ì„ íƒê³¼ í†µí•©
4. **ì¸¡ì •**: ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ì™€ ì—°ê²°ëœ ë©”íŠ¸ë¦­ ì •ì˜

### ë‹¨ê³„ë³„ MLOps ì„±ìˆ™ë„
- **Level 0**: ìˆ˜ë™ í”„ë¡œì„¸ìŠ¤ (ì‹¤í—˜ ë‹¨ê³„)
- **Level 1**: ML íŒŒì´í”„ë¼ì¸ ìë™í™” (CT ë„ì…)
- **Level 2**: CI/CD íŒŒì´í”„ë¼ì¸ ìë™í™” (ì™„ì „ ìë™í™”)

MLOpsëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ **ì—°êµ¬ì‹¤ì—ì„œ ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ë¡œ** ì—°ê²°í•˜ëŠ” í•µì‹¬ ë°©ë²•ë¡ ì…ë‹ˆë‹¤. ì„±ê³µì ì¸ MLOps êµ¬ì¶•ì„ í†µí•´ ì¡°ì§ì€ ë” ë¹ ë¥´ê³  ì•ˆì •ì ìœ¼ë¡œ AIì˜ ê°€ì¹˜ë¥¼ ì‹¤í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
