# 6.1. MLOps 개요 (Introduction to MLOps)

이 문서에서는 MLOps의 기본적인 개념과 필요성, 그리고 기존의 DevOps와의 차이점 및 전체적인 파이프라인에 대해 알아봅니다.

---

### 1. MLOps의 정의와 필요성

#### MLOps란?
MLOps(Machine Learning Operations)는 머신러닝(ML) 모델의 개발(Dev)과 운영(Ops)을 통합하여, 모델을 신속하고 안정적으로 프로덕션 환경에 배포하고 유지보수하기 위한 원칙과 관행의 집합입니다. 즉, 머신러닝 모델의 전체 생명주기(Life Cycle)를 자동화하고 표준화하는 것을 목표로 합니다.

#### MLOps의 필요성
- **실험과 현실의 간극:** 연구 단계(Jupyter Notebook 등)에서 성공적인 모델도 실제 서비스 환경에 배포하고 운영하는 것은 복잡한 과정입니다. MLOps는 이 간극을 메워줍니다.
- **신속한 배포와 반복:** 비즈니스 요구사항과 데이터는 계속해서 변합니다. MLOps를 통해 모델을 더 빠르게 재학습하고, 테스트하며, 배포하는 과정을 자동화할 수 있습니다.
- **안정성 및 확장성:** 자동화된 파이프라인은 수동 작업으로 인한 실수를 줄여주고, 대규모 트래픽에도 안정적으로 모델이 서빙될 수 있도록 지원합니다.
- **재현성 및 규제 준수:** 모델의 학습 과정, 사용된 데이터, 코드 버전을 모두 추적하고 관리하여 특정 시점의 모델 예측 결과를 재현할 수 있게 합니다. 이는 금융, 의료 등 규제가 중요한 산업에서 필수적입니다.
- **협업 강화:** 데이터 사이언티스트, 소프트웨어 엔지니어, 운영팀 간의 협업을 원활하게 만들어 생산성을 높입니다.

---

### 2. DevOps와 MLOps의 차이점

MLOps는 DevOps의 원칙을 기반으로 하지만, 머신러닝 시스템의 고유한 특성으로 인해 몇 가지 중요한 차이점을 가집니다.

| 구분 | DevOps (소프트웨어 개발) | MLOps (머신러닝 시스템) |
|---|---|---|
| **핵심 대상** | 애플리케이션 코드 | 코드 + **데이터** + **모델** |
| **개발 주기** | 코드 변경 시 새로운 빌드/배포 | 코드 변경뿐만 아니라, **새로운 데이터 유입** 시 모델 재학습/배포 |
| **팀 구성** | 소프트웨어 엔지니어, 운영 엔지니어 | 데이터 사이언티스트, ML 엔지니어, 데이터 엔지니어, 운영 엔지니어 |
| **테스트** | 단위 테스트, 통합 테스트 등 코드 중심 | 코드 테스트 + **데이터 유효성 검사**, **모델 품질 평가**, **모델 편향성 테스트** |
| **버전 관리** | Git 등을 이용한 **코드 버전 관리** | 코드 버전 관리 + **데이터셋 버전 관리 (DVC)** + **모델 버전 관리** |
| **모니터링** | CPU/메모리 사용량, 응답 시간 등 시스템 성능 | 시스템 성능 + **모델 예측 성능 저하**, **데이터 드리프트**, **컨셉 드리프트** |
| **실험적 성격** | 상대적으로 결정론적 | 모델 성능이 항상 보장되지 않는 **실험적이고 확률적인 성격** |

가장 큰 차이는 **MLOps가 데이터와 모델을 코드와 동등한 핵심 구성요소로 다룬다**는 점입니다. 데이터가 변하면 모델도 변해야 하므로, 지속적인 학습과 평가가 필수적입니다.

---

### 3. MLOps 파이프라인의 전체적인 그림

MLOps 파이프라인은 머신러닝 모델 개발부터 배포, 운영까지의 전 과정을 자동화한 워크플로우입니다. 일반적으로 다음과 같은 단계로 구성됩니다.

![MLOps Pipeline](https://i.imgur.com/8pT4aYy.png)
*(이미지 출처: Google Cloud)*

1.  **데이터 수집 및 처리 (Data Ingestion & Processing)**
    - 다양한 소스에서 데이터를 수집하고, 모델 학습에 적합한 형태로 가공(ETL/ELT)합니다.
    - 데이터 유효성을 검사하고 버전을 관리합니다.

2.  **모델 훈련 (Model Training)**
    - 준비된 데이터를 사용하여 모델을 학습시킵니다.
    - 다양한 알고리즘과 하이퍼파라미터를 실험하며 최적의 모델을 탐색합니다.
    - 이 과정에서 생성된 코드, 데이터, 파라미터, 모델 아티팩트를 모두 추적합니다.

3.  **모델 평가 (Model Evaluation)**
    - 훈련된 모델의 성능을 사전에 정의된 평가지표(예: 정확도, F1-score)로 평가합니다.
    - 비즈니스 요구사항을 충족하는지 검증하고, 이전 버전의 모델과 성능을 비교합니다.

4.  **모델 검증 및 등록 (Model Validation & Registry)**
    - 평가를 통과한 모델을 '스테이징(Staging)' 또는 '프로덕션(Production)' 용으로 승인합니다.
    - 승인된 모델은 **모델 레지스트리(Model Registry)**에 버전과 함께 저장되어 언제든지 불러올 수 있게 합니다.

5.  **모델 배포 (Model Deployment)**
    - 모델 레지스트리에 등록된 모델을 실제 서빙 환경에 배포합니다.
    - REST API 엔드포인트로 배포하거나, 배치 예측 시스템에 통합합니다.
    - Canary, A/B 테스팅 등 다양한 배포 전략을 사용합니다.

6.  **예측 서빙 (Prediction Serving)**
    - 배포된 모델을 통해 실제 사용자 요청에 대한 예측 결과를 제공합니다.

7.  **모델 모니터링 (Model Monitoring)**
    - 배포된 모델의 예측 성능과 안정성을 지속적으로 모니터링합니다.
    - **데이터 드리프트**(입력 데이터 분포 변화)나 **컨셉 드리프트**(데이터와 정답 간의 관계 변화)를 탐지합니다.

8.  **재학습 파이프라인 (Retraining Pipeline)**
    - 모니터링 결과, 모델 성능이 저하되거나 새로운 데이터가 충분히 쌓이면 자동으로 재학습 파이프라인을 실행하여 모델을 업데이트합니다. 이 전체 과정이 유기적으로 연결되어 **CI/CD/CT** 루프를 형성합니다.
