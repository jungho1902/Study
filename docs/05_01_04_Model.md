# 5.1.4. 모델 (Model)

**모델(Model)**은 강화학습에서 환경이 어떻게 동작하는지를 설명하는 수학적 표현입니다. 구체적으로, 에이전트가 현재 상태에서 특정 행동을 취했을 때 **다음 상태로의 전이**와 **받게 될 보상**을 예측할 수 있게 해주는 환경의 역학(dynamics)을 의미합니다.

모델을 알고 있다는 것은 환경의 "규칙"을 완전히 이해하고 있다는 뜻입니다. 예를 들어, 체스 게임에서는 각 말의 이동 규칙이 명확하고, 바둑에서는 돌을 놓았을 때의 결과가 확정적입니다. 반면, 주식 시장이나 실제 로봇 환경에서는 다음에 무엇이 일어날지 정확히 예측하기 어려워 모델을 모르는 상황에 해당합니다.

## 1. 모델의 구성 요소

### 1.1. 상태 전이 확률 (State Transition Probability)

**상태 전이 확률 P(s'|s, a)**는 현재 상태 s에서 행동 a를 수행했을 때, 다음 상태가 s'이 될 확률을 나타냅니다.

> **P(s'|s, a) = P[S_{t+1} = s' | S_t = s, A_t = a]**

**특징:**
- 모든 가능한 다음 상태에 대한 확률의 합은 1: ∑_{s'} P(s'|s, a) = 1
- 환경의 **확률적 특성**을 모델링 (동일한 상태-행동에서도 다른 결과 가능)
- **마르코프 성질**을 만족: 다음 상태는 현재 상태와 행동에만 의존

**예시:**
- 격자 세계에서 "위로 이동" 행동: 90% 확률로 위로, 5% 확률로 좌우로 미끄러짐
- 로봇 청소기: "직진" 명령 시 95% 확률로 직진, 5% 확률로 장애물에 막혀 제자리

### 1.2. 보상 함수 (Reward Function)

**보상 함수 R(s, a, s')**는 상태 s에서 행동 a를 수행하여 상태 s'으로 전이했을 때 받는 즉시 보상을 나타냅니다.

다양한 형태로 표현될 수 있습니다:
- **R(s, a, s')**: 상태 전이에 따른 보상 (가장 일반적)
- **R(s, a)**: 상태와 행동에만 의존하는 보상
- **R(s)**: 상태에만 의존하는 보상

**예시:**
- 게임: 목표 지점 도달 시 +100점, 함정에 빠질 시 -50점, 일반 이동 시 -1점
- 자율주행: 안전 운전 시 +1점, 속도 위반 시 -10점, 사고 시 -1000점

### 1.3. 결정론적 vs 확률적 환경

#### 1.3.1. 결정론적 환경 (Deterministic Environment)

- **특징:** 같은 상태에서 같은 행동을 하면 항상 같은 결과
- **모델링:** P(s'|s, a) ∈ {0, 1} (확률이 0 또는 1)
- **예시:** 체스, 틱택토, 퍼즐 게임

#### 1.3.2. 확률적 환경 (Stochastic Environment)

- **특징:** 같은 상태-행동에서도 다양한 결과 가능
- **모델링:** 0 ≤ P(s'|s, a) ≤ 1 (일반적인 확률)
- **예시:** 주사위 게임, 현실 세계 로봇 환경, 금융 시장

## 2. 모델 기반 vs 모델 프리 학습

### 2.1. 모델 기반 학습 (Model-Based Reinforcement Learning)

**모델 기반 학습**은 환경의 모델을 알고 있거나, 학습을 통해 모델을 추정하여 활용하는 접근법입니다.

#### 2.1.1. 특징
- **환경 모델 활용**: P(s'|s, a)와 R(s, a, s')를 알거나 학습
- **계획 기반**: 모델을 사용해 미래를 예측하고 최적 정책 계산
- **데이터 효율성**: 적은 경험으로도 효과적 학습 가능
- **계산 복잡성**: 모델 학습과 계획 단계에서 높은 계산 비용

#### 2.1.2. 주요 알고리즘
- **동적 계획법 (Dynamic Programming)**
  - 정책 반복 (Policy Iteration)
  - 가치 반복 (Value Iteration)
- **몬테카를로 트리 탐색 (Monte Carlo Tree Search)**
- **모델 예측 제어 (Model Predictive Control)**

#### 2.1.3. 적용 예시
- **AlphaGo/AlphaZero**: 바둑/체스 규칙을 알고 있는 상황
- **로봇 제어**: 물리 시뮬레이터를 통한 환경 모델링
- **자율주행**: 교통 규칙과 물리 법칙 기반 예측

### 2.2. 모델 프리 학습 (Model-Free Reinforcement Learning)

**모델 프리 학습**은 환경 모델 없이 오직 에이전트와 환경의 상호작용 경험만을 통해 학습하는 접근법입니다.

#### 2.2.1. 특징
- **경험 기반**: (상태, 행동, 보상, 다음상태) 샘플만 활용
- **환경 독립적**: 복잡한 환경에서도 모델링 없이 적용 가능
- **robust**: 환경 변화에 적응적으로 대응
- **샘플 복잡성**: 많은 경험이 필요할 수 있음

#### 2.2.2. 주요 알고리즘
- **가치 기반 (Value-Based)**
  - Q-Learning
  - SARSA
  - DQN (Deep Q-Network)
- **정책 기반 (Policy-Based)**
  - REINFORCE
  - Actor-Critic
  - PPO (Proximal Policy Optimization)

#### 2.2.3. 적용 예시
- **Atari 게임**: 복잡한 게임 규칙을 모델링하지 않고 직접 플레이
- **실세계 로봇**: 불완전한 센서와 복잡한 물리 환경
- **온라인 광고**: 사용자 행동의 복잡성으로 인한 모델링 어려움

## 3. 모델 기반 학습의 세부 접근법

### 3.1. 알려진 모델 활용 (Known Model)

환경의 전이 확률과 보상 함수가 완전히 알려진 경우입니다.

**장점:**
- 최적 정책을 정확히 계산 가능
- 매우 효율적인 학습

**동적 계획법 사용:**
```
정책 반복 알고리즘:
1. 정책 평가: V^π 계산
2. 정책 개선: π' = argmax_a Q^π(s,a)
3. 수렴까지 반복
```

### 3.2. 학습된 모델 활용 (Learned Model)

상호작용 경험을 통해 환경 모델을 추정하여 사용하는 방법입니다.

**과정:**
1. **데이터 수집**: 환경과의 상호작용 경험 축적
2. **모델 학습**: 상태 전이와 보상 함수 추정
3. **계획**: 학습된 모델로 정책 또는 가치 함수 계산
4. **실행**: 계산된 정책으로 행동

**도전 과제:**
- **모델 오차**: 부정확한 모델로 인한 성능 저하
- **탐험-활용**: 모델 개선을 위한 데이터 수집 vs 현재 모델 활용

## 4. 하이브리드 접근법

### 4.1. Dyna-Q

모델 프리와 모델 기반 방법을 결합한 대표적인 알고리즘입니다.

**동작 원리:**
1. **직접 학습**: 실제 경험으로 Q 함수 업데이트
2. **간접 학습**: 학습된 모델로 가상 경험 생성하여 추가 업데이트

### 4.2. 모델 보조 방법들 (Model-Assisted Methods)

- **계획을 통한 탐험 개선**
- **샘플 효율성 향상**
- **안전한 정책 학습**

---

## 예제 및 풀이 (Examples and Solutions)

### 예제 1: 격자 세계 모델 정의

**문제:** 다음과 같은 3×3 격자 세계에서 환경 모델을 정의하시오. 에이전트는 4방향(상, 하, 좌, 우)으로 이동할 수 있으며, 벽이나 경계에 부딪히면 제자리에 머물고, 목표 지점 G에 도달하면 에피소드가 종료됩니다.

```
[S] [ ] [G]
[ ] [X] [ ]
[ ] [ ] [ ]
```

여기서 S는 시작점, G는 목표점(+10 보상), X는 장애물입니다. 각 이동마다 -1의 비용이 발생합니다.

**풀이:**

1. **상태 공간 정의:**
   - S = {(0,0), (0,1), (0,2), (1,0), (1,2), (2,0), (2,1), (2,2)}
   - 장애물 (1,1)은 상태에서 제외

2. **상태 전이 확률 P(s'|s, a) 정의:**
   
   예시: 상태 (0,0)에서의 전이
   - P((0,1)|(0,0), 우) = 1.0 (오른쪽으로 이동 성공)
   - P((0,0)|(0,0), 좌) = 1.0 (벽에 막혀 제자리)
   - P((1,0)|(0,0), 하) = 1.0 (아래로 이동 성공)
   - P((0,0)|(0,0), 상) = 1.0 (경계에 막혀 제자리)

3. **보상 함수 R(s, a, s') 정의:**
   - R(s, a, (0,2)) = +10 - 1 = 9 (목표 도달)
   - R(s, a, s') = -1 (일반적인 이동)

**답:** 위의 전이 확률과 보상 함수가 이 격자 세계의 완전한 모델을 구성합니다.

### 예제 2: 모델 기반 vs 모델 프리 비교

**문제:** 다음 두 시나리오에서 모델 기반과 모델 프리 접근법 중 어느 것이 더 적합한지 판단하고 이유를 설명하시오.

**시나리오 A:** 새로운 보드게임 AI 개발
- 게임 규칙이 명확하게 정의됨
- 모든 상태 전이가 결정론적
- 제한된 컴퓨팅 시간 내에 최적 성능 필요

**시나리오 B:** 실제 환경에서의 로봇 내비게이션
- 센서 노이즈와 예측 불가능한 장애물
- 환경이 동적으로 변화
- 안전성이 중요하지만 점진적 개선도 허용

**풀이:**

**시나리오 A - 모델 기반 학습이 적합:**
- **이유:**
  1. **명확한 규칙**: 게임 규칙이 완전히 알려져 있어 정확한 모델 구성 가능
  2. **결정론적 환경**: 상태 전이가 예측 가능하여 계획 기반 접근 효과적
  3. **최적성 요구**: 동적 계획법으로 최적 정책 보장 가능
  4. **계산 효율성**: 실제 게임 플레이 없이도 시뮬레이션으로 빠른 학습

**시나리오 B - 모델 프리 학습이 적합:**
- **이유:**
  1. **모델링 어려움**: 센서 노이즈와 동적 환경으로 정확한 모델 구성 곤란
  2. **환경 변화**: 모델 프리 방법이 변화하는 환경에 더 적응적
  3. **강건성**: 불완전한 정보에도 점진적으로 성능 향상 가능
  4. **안전 학습**: 실제 경험을 통한 조심스러운 정책 개발

**답:** 시나리오 A는 모델 기반, 시나리오 B는 모델 프리 학습이 적합합니다.

### 예제 3: 간단한 모델 학습

**문제:** 에이전트가 다음과 같은 경험 데이터를 수집했습니다. 이 데이터로부터 상태 전이 확률 P(s₂|s₁, a₁)을 추정하시오.

경험 데이터:
- (s₁, a₁, s₁): 3번 관찰
- (s₁, a₁, s₂): 7번 관찰
- 총 10번의 (s₁, a₁) 경험

**풀이:**

최대 우도 추정(Maximum Likelihood Estimation)을 사용합니다:

P(s₂|s₁, a₁) = Count(s₁, a₁, s₂) / Count(s₁, a₁)
              = 7 / 10
              = 0.7

검증:
- P(s₁|s₁, a₁) = 3/10 = 0.3
- P(s₂|s₁, a₁) = 7/10 = 0.7
- 합계: 0.3 + 0.7 = 1.0 ✓

**답:** P(s₂|s₁, a₁) = 0.7

**해설:** 이는 경험 데이터로부터 환경 모델을 학습하는 기본적인 방법입니다. 더 많은 데이터가 수집될수록 추정의 정확도가 향상됩니다.
