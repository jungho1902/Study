# 트랜스포머 (Transformer) 아키텍처

**트랜스포머(Transformer)**는 2017년 구글의 논문 "Attention Is All You Need"에서 처음 제안된 모델로, 기존의 순환 신경망(RNN) 구조를 완전히 배제하고 **어텐션(Attention) 메커니즘**만으로 시퀀스 데이터를 처리하는 혁신적인 아키텍처입니다.

트랜스포머는 병렬 처리의 이점을 극대화하여 학습 속도를 크게 향상시켰고, 자연어 처리(NLP) 분야에서 전례 없는 성능을 달성하며 현대 AI 모델의 기반이 되었습니다.

## 1. 트랜스포머의 전체 구조: 인코더-디코더

트랜스포머는 Seq2Seq 모델과 마찬가지로, 입력을 처리하는 **인코더(Encoder)**와 출력을 생성하는 **디코더(Decoder)** 스택으로 구성됩니다.
- **인코더 스택:** 여러 개의 동일한 인코더 레이어를 쌓은 구조. 입력 시퀀스의 각 단어에 대한 문맥적인 표현(representation)을 생성합니다.
- **디코더 스택:** 여러 개의 동일한 디코더 레이어를 쌓은 구조. 인코더의 출력과 이전에 생성된 출력 단어들을 입력으로 받아, 다음 단어를 예측합니다.

![Transformer Architecture](https://i.imgur.com/3mMLKx5.png)

## 2. 핵심 구성 요소

### 2.1. 셀프 어텐션 (Self-Attention)

- **개념:** "자기 자신"에게 어텐션을 적용한다는 의미로, **하나의 시퀀스 내에서** 단어들 간의 관계와 의존성을 직접적으로 계산하여 문맥을 파악하는 메커니즘입니다.
- **예시:** "The animal didn't cross the street because **it** was too tired." 라는 문장에서, 'it'이 'animal'을 가리키는 것인지 'street'을 가리키는 것인지를 파악하기 위해, 'it'과 문장 내 다른 모든 단어 간의 연관성을 계산합니다.
- **동작 원리: Query, Key, Value (Q, K, V)**
  1.  각 단어(입력 임베딩)에 대해 세 가지 다른 벡터, 즉 **Query**, **Key**, **Value** 벡터를 생성합니다. 이 벡터들은 학습 가능한 가중치 행렬을 곱하여 만들어집니다.
  2.  **Query (Q):** 현재 단어를 나타내는 '질문' 벡터입니다.
  3.  **Key (K):** 시퀀스 내의 다른 모든 단어들을 나타내는 '꼬리표' 벡터입니다.
  4.  **Value (V):** 시퀀스 내의 다른 모든 단어들의 실제 '의미'를 담고 있는 벡터입니다.
  5.  현재 단어의 **Query** 벡터를 다른 모든 단어의 **Key** 벡터와 내적(dot-product)하여 **어텐션 점수(Attention Score)**를 계산합니다. 이는 현재 단어와 다른 단어 간의 연관성을 나타냅니다.
  6.  점수를 스케일링(scaling, $\sqrt{d_k}$로 나눔)하고 소프트맥스(Softmax)를 적용하여 **어텐션 가중치(Attention Weights)**를 얻습니다.
  7.  이 가중치를 각 단어의 **Value** 벡터에 곱하여 가중합을 구하면, 현재 단어에 대한 문맥적인 표현이 완성됩니다.

### 2.2. 멀티-헤드 어텐션 (Multi-Head Attention)

- **개념:** 하나의 셀프 어텐션을 수행하는 대신, 여러 개의 "헤드(head)"가 **병렬적으로** 셀프 어텐션을 수행하는 구조입니다.
- **동작 방식:**
  1.  Q, K, V 벡터를 `h`개의 헤드로 나눕니다.
  2.  각 헤드는 독립적으로 셀프 어텐션을 수행하여, 서로 다른 관점의 문맥 정보(예: 한 헤드는 문법적 관계, 다른 헤드는 의미적 관계 등)를 학습합니다.
  3.  각 헤드에서 나온 결과들을 모두 연결(concatenate)하고, 선형 변환(Linear layer)을 통해 최종 출력을 만듭니다.
- **효과:** 모델이 다양한 측면에서 정보의 관계를 종합적으로 파악할 수 있게 하여 표현력을 높입니다.

### 2.3. 포지셔널 인코딩 (Positional Encoding)

- **문제점:** 트랜스포머는 RNN과 달리 순환 구조가 없기 때문에, 단어의 순서 정보를 자체적으로 알 수 없습니다. "I go to school"과 "school go I to"를 동일하게 처리하는 문제가 발생합니다.
- **해결책:** **포지셔널 인코딩(Positional Encoding)**은 각 단어의 **위치 정보**를 담고 있는 벡터를 만들어 입력 임베딩에 더해주는 방식입니다.
- **원리:** `sin`과 `cos` 함수를 사용하여 각 위치마다 고유한 패턴을 가지는 위치 벡터를 생성합니다. 이 벡터 덕분에 모델은 단어 간의 상대적인 위치 관계를 학습할 수 있습니다.

### 2.4. 잔차 연결 및 층 정규화 (Add & Norm)
- 각 서브 레이어(Multi-Head Attention, Feed-Forward Network)의 출력은 **잔차 연결(Residual Connection, Add)**과 **층 정규화(Layer Normalization, Norm)**를 거칩니다.
- **잔차 연결:** 서브 레이어의 입력을 출력에 더해주는 것(ResNet의 스킵 연결과 동일). 층이 깊어져도 그래디언트가 잘 흐르도록 하여 학습을 안정화합니다.
- **층 정규화:** 각 샘플(단어)에 대해 독립적으로 정규화를 수행하여, 학습 과정을 안정시키고 속도를 높입니다.

### 2.5. 피드 포워드 신경망 (Position-wise Feed-Forward Networks)
- 각 인코더와 디코더 레이어에는 멀티-헤드 어텐션 이후에 **완전 연결 피드 포워드 신경망(FFN)**이 위치합니다.
- 이 신경망은 어텐션을 통해 얻은 문맥 정보를 한 번 더 비선형적으로 변환하여 표현력을 높이는 역할을 합니다.

이러한 구성 요소들의 결합을 통해 트랜스포머는 병렬 처리에 최적화되면서도, 시퀀스 내의 복잡한 문맥 관계를 효과적으로 학습하는 강력한 모델로 자리 잡았습니다.
