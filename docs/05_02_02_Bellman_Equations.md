# 5.2.2. 벨만 방정식 (Bellman Equations)

**벨만 방정식(Bellman Equations)**은 현재 상태의 가치 함수와 다음 상태의 가치 함수 사이의 관계를 나타내는 방정식으로, 강화학습에서 최적의 정책을 찾는 데 핵심적인 역할을 합니다.

- **벨만 기대 방정식 (Bellman Expectation Equation):**
  특정 정책 π를 따를 때, 가치 함수들이 만족해야 하는 관계식입니다. 현재 상태의 가치는 즉각적인 보상과 다음 상태의 가치(감가율 적용)의 기댓값의 합으로 표현됩니다.
  - 상태 가치 함수: `Vπ(s) = E[Rt+1 + γVπ(St+1) | St=s]`
  - 행동 가치 함수: `Qπ(s, a) = E[Rt+1 + γQπ(St+1, At+1) | St=s, At=a]`

- **벨만 최적 방정식 (Bellman Optimality Equation):**
  최적 정책(optimal policy)을 따를 때의 최적 가치 함수가 만족하는 관계식입니다. 최적 가치 함수는 가능한 모든 행동 중에서 가장 높은 가치를 선택함으로써 얻어집니다. 이 방정식을 풀면 최적의 정책을 찾을 수 있습니다.
  - 최적 상태 가치 함수: `V*(s) = max_a E[Rt+1 + γV*(St+1) | St=s, At=a]`
  - 최적 행동 가치 함수: `Q*(s, a) = E[Rt+1 + γ max_a' Q*(St+1, a') | St=s, At=a]`
