# 파트 10: AI 기반 로봇 응용 (AI-Powered Robotic Applications)

## 10.4. 휴머노이드 및 다리 로봇 (Humanoids and Legged Robots)

### 10.4.4. 강화학습 기반 보행 및 동작 생성

**들어가며**

지금까지 다룬 ZMP 기반 제어나 전신 제어는 로봇의 정확한 동역학 모델에 크게 의존합니다. 하지만 완벽한 모델을 만드는 것은 매우 어렵고, 모델링되지 않은 현상(예: 모터의 백래시, 지면과의 복잡한 마찰)에 대처하는 데 한계가 있습니다. 또한, 달리기, 점프, 넘어졌다가 일어나기와 같은 매우 동적이고 복잡한 동작을 전통적인 방식으로 설계하는 것은 거의 불가능에 가깝습니다. **강화학습(Reinforcement Learning, RL)**은 이러한 한계를 극복하기 위한 강력한 대안으로 떠오르고 있습니다. 로봇이 마치 아기처럼, 수많은 **시행착오(Trial-and-Error)**를 통해 스스로 걷고 움직이는 법을 배우게 하는 것입니다.

---

**1. 왜 다리 로봇에 강화학습을 사용하는가?**

- **복잡한 동역학의 자동 학습:** 강화학습 에이전트(로봇)는 환경과의 상호작용을 통해, 명시적인 동역학 모델 없이도 어떻게 움직여야 목표를 달성할 수 있는지 스스로 터득합니다. 이는 모델링의 어려움을 해결해 줍니다.
- **고도의 동적 기술 습득:** 전통적인 제어기로는 구현하기 극도로 어려운 달리기, 점프, 방향 전환, 장애물 넘기 등 민첩하고 역동적인 동작을 학습할 수 있습니다.
- **환경 적응성 및 강건성:** 다양한 지형(평지, 경사, 험지)과 예기치 않은 외부 힘(미는 힘)이 존재하는 **시뮬레이션 환경**에서 학습함으로써, 변화하는 실제 환경에 강건하게 대처하는 능력을 기를 수 있습니다. (Sim-to-Real 문제, 파트 10.2.3 연계)

---

**2. 강화학습 기반 보행 제어 프레임워크**

다리 로봇의 보행을 강화학습으로 학습시키는 일반적인 과정은 다음과 같습니다.

- **에이전트 (Agent):** 로봇의 제어 정책(Policy). 즉, 로봇의 뇌.
- **환경 (Environment):** 로봇과 로봇이 상호작용하는 물리 법칙이 적용된 시뮬레이션 세계 (예: MuJoCo, Isaac Gym).
- **상태 (State / Observation):** 에이전트가 환경을 관찰하여 얻는 정보. 제어에 필요한 로봇의 현재 상태입니다.
  - 예: 몸통의 위치/자세/속도, 각 관절의 각도/속도, 발의 접촉 여부, 목표 속도 등.
- **행동 (Action):** 에이전트가 상태를 바탕으로 내리는 결정. 각 관절에 가할 목표 각도(PD 제어) 또는 토크.
- **보상 (Reward):** 에이전트가 한 행동이 얼마나 좋았는지를 평가하는 신호. **보상 설계(Reward Design)**는 강화학습 기반 보행에서 가장 중요하고 창의적인 부분입니다.

- **보상 함수의 설계 예시:**
  - **`R_total = w_1*R_fwd + w_2*R_alive + w_3*R_ctrl + w_4*R_contact + ...`**
  - **`+` 긍정적 보상:**
    - `R_fwd`: 목표 방향(앞으로)으로 이동하는 속도에 비례하는 보상. (가장 중요)
    - `R_alive`: 넘어지지 않고 살아있는 매 순간마다 작은 보상.
  - **`-` 부정적 보상 (패널티):**
    - `R_ctrl`: 너무 과격한 움직임을 방지하기 위해, 관절 토크나 가속도의 제곱에 비례하는 패널티. (에너지 효율)
    - `R_contact`: 의도치 않은 신체 부위(예: 무릎, 허벅지)가 땅에 닿았을 때 큰 패널티.
    - `R_shake`: 몸통이 너무 흔들리는 것을 방지하기 위한 패널티.
    - `R_slip`: 발이 땅에서 미끄러질 때 패널티.

이러한 보상 함수를 잘 설계함으로써, 개발자는 로봇이 '효율적이고', '안정적이며', '자연스러운' 걸음걸이를 학습하도록 유도할 수 있습니다.

---

**3. 주요 학습 알고리즘 및 기술**

- **정책 기반 알고리즘 (Policy-based Algorithms):** 로봇 제어와 같은 연속적인 행동 공간(Continuous Action Space) 문제에는 주로 PPO, SAC, DDPG와 같은 정책 기반 알고리즘이 사용됩니다. (파트 5.3 연계)
  - **PPO (Proximal Policy Optimization):** 안정성과 데이터 효율성 사이의 균형이 잘 잡혀 있어 가장 널리 사용되는 알고리즘 중 하나입니다.
  - **SAC (Soft Actor-Critic):** 탐험(Exploration) 성능을 극대화하여 복잡한 기술을 더 잘 학습하는 경향이 있습니다.

- **Sim-to-Real 전달:**
  - **도메인 랜덤화 (Domain Randomization):** 시뮬레이션 환경의 물리 파라미터(중력, 마찰, 모터 힘 등)와 외형(색상, 질감)을 매 에피소드마다 무작위로 변경하여, 에이전트가 어떤 환경 변화에도 강건하게 대처하는 정책을 학습하도록 합니다.
  - **시스템 식별 (System Identification):** 실제 로봇의 물리적 특성(예: 관절의 마찰력)을 측정하여 시뮬레이션 모델을 최대한 현실과 가깝게 만드는 기술입니다.

- **학습 커리큘럼 (Curriculum Learning):**
  - **개념:** 처음부터 너무 어려운 과제를 주는 대신, 쉬운 과제부터 점진적으로 어려운 과제로 난이도를 높여가며 학습시키는 방법입니다.
  - **예시:** 처음에는 평지에서 걷는 것만 학습시키고, 이것이 안정화되면 점차 경사로나 장애물이 있는 환경을 제시하여 더 어려운 기술을 배우게 합니다.

**결론**

강화학습은 다리 로봇 제어의 새로운 지평을 열고 있습니다. 보상 함수 설계와 Sim-to-Real이라는 큰 과제가 여전히 존재하지만, 강화학습을 통해 로봇은 인간이 설계한 것보다 더 뛰어나고 효율적인 움직임을 스스로 발견해내고 있습니다. Boston Dynamics의 Atlas나 Anybotics의 ANYmal과 같은 최첨단 로봇들이 보여주는 놀라운 움직임의 상당 부분은 바로 이러한 강화학습 기술에 힘입은 것입니다. 앞으로 강화학습은 로봇이 미지의 환경을 탐사하고, 재난 현장을 누비며, 인간의 일상 속으로 들어오는 것을 가능하게 할 핵심 동력이 될 것입니다.
