# 5.3.3. 정책 기반 학습 (Policy-Based Learning)

**정책 기반 학습(Policy-Based Learning)**은 가치 함수를 거치지 않고, 최적의 정책(Policy)을 직접적으로 학습하는 방식입니다. 정책은 보통 파라미터 θ로 표현된 함수(πθ)로 나타내며, 이 파라미터를 업데이트하여 정책을 개선합니다.

이 방식은 연속적인 행동 공간을 다루거나, 결정론적 정책보다 확률적인 정책이 더 유리한 문제에 효과적입니다.

- **정책 경사 (Policy Gradient, PG):** 정책의 좋음(성능)을 나타내는 목적 함수(J(θ))를 정의하고, 이 목적 함수를 크게 만드는 방향(Gradient Ascent)으로 정책 파라미터 θ를 업데이트하는 기법입니다. "좋은 행동"의 확률은 높이고 "나쁜 행동"의 확률은 낮추도록 학습이 진행됩니다.

- **REINFORCE 알고리즘:** 가장 기본적인 정책 경사 알고리즘입니다. 에이전트가 하나의 에피소드(episode)를 처음부터 끝까지 모두 경험한 후에, 그 결과로 얻어진 전체 보상(Return)을 이용해 정책을 업데이트합니다. 이 때문에 학습의 분산(variance)이 크고 수렴이 느릴 수 있다는 단점이 있습니다.
