# 강화학습 (Reinforcement Learning)

## 1. 강화학습이란?

**강화학습(Reinforcement Learning, RL)**은 지도학습, 비지도학습과는 다른 독특한 패러다임을 가진 머신러닝의 한 분야입니다. 정답(레이블)이 주어진 데이터를 학습하는 대신, **에이전트(Agent)**라는 학습 주체가 **환경(Environment)**과 상호작용하며 **보상(Reward)**을 최대화하는 방향으로 행동을 학습합니다.

- **핵심 목표:** 어떤 **상태(State)**에서 어떤 **행동(Action)**을 취해야 가장 큰 누적 보상을 얻을 수 있는지를 학습하는 것. 즉, 최적의 **정책(Policy)**을 찾는 것이 목표입니다. 이 과정은 수많은 **시행착오(Trial and Error)**를 통해 이루어집니다.

### 비유: 강아지 훈련시키기
강화학습은 강아지를 훈련시키는 과정과 매우 유사합니다.
- **에이전트 (Agent):** 강아지
- **환경 (Environment):** 주인과 집 안
- **상태 (State):** 주인이 "앉아!"라고 말한 상황
- **행동 (Action):** 강아지가 할 수 있는 여러 행동 (앉기, 눕기, 짖기 등)
- **보상 (Reward):** 주인이 주는 간식 (긍정적 보상) 또는 칭찬

주인이 "앉아!"라고 했을 때 강아지가 우연히 앉으면 간식을 받습니다. 다른 행동을 하면 아무것도 받지 못합니다. 이 과정이 반복되면, 강아지는 '앉는' 행동이 간식이라는 보상으로 이어진다는 것을 학습하고, "앉아!"라는 말을 들었을 때 앉을 확률이 높아집니다.

## 2. 강화학습의 수학적 프레임워크: 마르코프 결정 과정 (MDP)
강화학습 문제는 보통 **마르코프 결정 과정(Markov Decision Process, MDP)**이라는 수학적 모델로 형식화됩니다. MDP는 (S, A, P, R, γ)라는 5개의 요소로 구성됩니다.
- **S (States):** 가능한 모든 상태의 집합
- **A (Actions):** 가능한 모든 행동의 집합
- **P (Transition Probability):** 상태 `s`에서 행동 `a`를 했을 때, 다음 상태 `s'`가 될 확률. `P(s' | s, a)`
- **R (Reward Function):** 상태 `s`에서 행동 `a`를 했을 때 즉시 받게 되는 보상. `R(s, a)`
- **γ (Discount Factor, 감가율):** 현재의 보상과 미래의 보상 중 무엇을 더 중요하게 생각할지를 결정하는 0과 1 사이의 값. 1에 가까울수록 미래의 보상을 더 중요하게 생각합니다.

강화학습의 목표는 이 MDP 안에서 누적 보상의 기댓값을 최대로 만드는 정책 `π(a|s)` (상태 `s`에서 행동 `a`를 할 확률)을 찾는 것입니다.

## 3. 강화학습의 핵심 구성요소
- **에이전트 (Agent):** 학습의 주체이자 의사결정자. 정책(Policy)에 따라 행동합니다.
- **환경 (Environment):** 에이전트가 상호작용하는 외부 세계.
- **상태 (State, S):** 특정 시점에서 환경에 대한 완전한 정보.
- **행동 (Action, A):** 에이전트가 특정 상태에서 취할 수 있는 움직임.
- **보상 (Reward, R):** 에이전트의 행동에 대한 즉각적인 피드백.
- **정책 (Policy, π):** 에이전트의 '뇌' 또는 '전략'. 상태에 따른 행동을 결정합니다.

## 4. 주요 학습 방법론
최적의 정책을 찾기 위해, 강화학습은 크게 두 가지 접근법을 사용합니다.

- **가치 기반 학습 (Value-based Learning):** 각 상태 또는 행동의 '가치(Value)'를 학습합니다. 여기서 가치란, 현재 상태에서 특정 정책을 따랐을 때 미래까지 얻을 수 있는 누적 보상의 기댓값입니다. 최적의 가치 함수를 알면, 매 순간 가장 높은 가치를 가진 행동을 선택함으로써 최적의 정책을 따를 수 있습니다.
  - **대표 알고리즘:** Q-러닝(Q-Learning), DQN(Deep Q-Network)
- **정책 기반 학습 (Policy-based Learning):** 가치 함수를 거치지 않고, 어떤 행동을 할지 결정하는 정책(Policy) 자체를 직접 최적화합니다. 정책을 하나의 함수로 보고, 누적 보상을 최대로 만드는 함수의 파라미터를 직접 찾아 나갑니다.
  - **대표 알고리즘:** REINFORCE, A2C/A3C, TRPO

**액터-크리틱(Actor-Critic)** 방법은 이 두 가지를 결합한 형태로, 정책(Actor)과 가치 함수(Critic)를 모두 학습하여 안정성과 효율성을 높인 진보된 방식입니다.

## 5. 탐험과 활용의 딜레마 (Exploration vs. Exploitation Dilemma)
강화학습에서 에이전트는 항상 어려운 선택에 직면합니다.
- **활용 (Exploitation):** 지금까지의 경험상 가장 큰 보상을 줄 것으로 알려진 행동을 선택하는 것.
- **탐험 (Exploration):** 당장의 보상은 불확실하더라도, 미래에 더 큰 보상을 가져다줄 가능성을 찾기 위해 새로운 행동을 시도하는 것.

최적의 정책을 찾기 위해서는 이 둘 사이의 균형을 잘 맞추는 것이 매우 중요합니다. (예: ε-greedy 기법)

## 6. 주요 적용 분야
강화학습은 순차적인 의사결정이 필요한 복잡한 문제 해결에 강점을 보입니다.
- **게임 AI:** 바둑(알파고), 체스, 아타리 게임 등 복잡한 전략 게임에서 인간 챔피언을 능가하는 AI 개발.
- **로보틱스:** 로봇이 걷거나, 물건을 집는 등 복잡한 동작을 스스로 학습하도록 훈련.
- **자율주행:** 신호등, 다른 차량, 보행자 등 시시각각 변하는 도로 상황에 맞춰 최적의 주행 경로와 행동을 결정.
- **자원 관리 및 최적화:** 데이터 센터의 냉각 시스템 에너지 효율 최적화, 금융 포트폴리오 관리 등.

---

## 7. 핵심 요약 (Key Takeaways)
- **정의:** 강화학습은 에이전트가 환경과 상호작용하며 **시행착오**를 통해 **누적 보상**을 최대화하는 **정책**을 학습하는 방식입니다.
- **수학적 모델:** 강화학습 문제는 주로 **마르코프 결정 과정(MDP)**으로 정의됩니다.
- **핵심 딜레마:** 현재까지의 최선을 선택하는 **활용(Exploitation)**과 더 나은 선택지를 찾아 나서는 **탐험(Exploration)** 사이의 균형이 중요합니다.
- **주요 접근법:** 상태/행동의 '가치'를 배우는 **가치 기반 학습**과 행동 '전략' 자체를 배우는 **정책 기반 학습**으로 나뉩니다.
- **강점:** 명확한 정답이 없는 순차적 의사결정 문제에서 강력한 성능을 발휘합니다.

지금까지 우리는 데이터를 학습하는 세 가지 주요 패러다임인 지도, 비지도, 강화학습에 대해 알아보았습니다. 각각의 방식은 서로 다른 종류의 문제를 해결하는 데 특화되어 있으며, 이들을 이해하는 것은 머신러닝의 광대한 세계를 탐험하는 첫걸음입니다.
