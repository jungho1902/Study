# 편향-분산 트레이드오프 (Bias-Variance Tradeoff)

머신러닝 모델의 예측 오차(Error)는 크게 **편향(Bias)**, **분산(Variance)**, 그리고 **줄일 수 없는 오차(Irreducible Error)** 세 가지 요소로 구성됩니다. 이 중 편향과 분산은 모델의 복잡도에 따라 서로 상충하는 관계를 가지는데, 이를 **편향-분산 트레이드오프(Bias-Variance Tradeoff)**라고 합니다.

> **Total Error = Bias² + Variance + Irreducible Error**

이 트레이드오프를 이해하는 것은 과대적합과 과소적합을 피하고 최적의 모델을 만드는 데 매우 중요합니다.

---

### 1. 편향 (Bias)

- **정의:** **편향**은 모델이 실제 데이터의 복잡한 관계를 얼마나 잘 단순화했는지에서 발생하는 오차입니다. 즉, 모델의 예측값들이 실제값과 평균적으로 얼마나 멀리 떨어져 있는지를 나타냅니다.
- **높은 편향 (High Bias):**
  - 모델이 너무 단순하여 데이터에 내재된 패턴을 제대로 학습하지 못하는 상태를 의미합니다.
  - 예측값들이 실제값 주변이 아닌, 한쪽으로 치우쳐서 형성됩니다.
  - 이는 **과소적합(Underfitting)**의 주요 원인이 됩니다.

### 2. 분산 (Variance)

- **정의:** **분산**은 훈련 데이터셋의 작은 변화에 모델이 얼마나 민감하게 반응하는지를 측정합니다. 즉, 훈련 데이터가 조금만 바뀌어도 예측값들이 얼마나 크게 변동하는지를 나타냅니다.
- **높은 분산 (High Variance):**
  - 모델이 너무 복잡하여 훈련 데이터의 노이즈까지 학습해버린 상태를 의미합니다.
  - 훈련 데이터에 대한 예측은 매우 정확하지만, 새로운 테스트 데이터에 대한 예측은 크게 빗나갑니다. 예측값들이 평균적으로는 실제값 주변에 있더라도 매우 넓게 흩어져 있습니다.
  - 이는 **과대적합(Overfitting)**의 주요 원인이 됩니다.

---

### 3. 트레이드오프 관계 (The Trade-off)

편향과 분산은 모델의 **복잡도(complexity)**와 밀접한 관계를 맺으며, 일반적으로 하나를 낮추면 다른 하나가 높아지는 상충 관계에 있습니다.

- **단순한 모델 (Low Complexity):**
  - 데이터의 큰 흐름만을 학습하므로, 훈련 데이터가 바뀌어도 예측이 크게 변하지 않습니다. (**낮은 분산**)
  - 하지만 데이터의 복잡한 패턴을 놓치기 쉬워, 예측이 실제값과 동떨어질 수 있습니다. (**높은 편향**)
  - **→ 과소적합 경향**

- **복잡한 모델 (High Complexity):**
  - 데이터의 세세한 부분까지 학습하므로, 훈련 데이터의 노이즈에도 민감하게 반응하여 예측이 크게 변동합니다. (**높은 분산**)
  - 하지만 데이터의 복잡한 패턴을 잘 포착하여, 예측이 실제값 주변에 모일 수 있습니다. (**낮은 편향**)
  - **→ 과대적합 경향**

![Bias-Variance Tradeoff](https://upload.wikimedia.org/wikipedia/commons/thumb/4/44/Bias-variance-tradeoff-with-bullseye-and-darts-1.png/600px-Bias-variance-tradeoff-with-bullseye-and-darts-1.png)
*(이미지 출처: 위키미디어)*

**과녁 맞히기 비유:**
- **과녁의 중심:** 실제 정답 값
- **탄착군:** 모델의 예측값들
- **낮은 편향, 낮은 분산 (Good Model):** 탄착군이 과녁 중앙에 밀집해 있음 (우리가 원하는 이상적인 모델)
- **높은 편향, 낮은 분산 (Underfitting):** 탄착군이 과녁 중앙에서 벗어난 곳에 밀집해 있음
- **낮은 편향, 높은 분산 (Overfitting):** 탄착군이 과녁 중앙 주변에 넓게 퍼져 있음
- **높은 편향, 높은 분산 (Bad Model):** 탄착군이 과녁 중앙에서 벗어난 곳에 넓게 퍼져 있음

**머신러닝 모델링의 목표는 총 오차(Total Error)를 최소화하는 것입니다.** 이를 위해서는 편향과 분산이 균형을 이루는, 즉 **과소적합과 과대적합 사이의 최적의 지점(sweet spot)**을 찾는 것이 중요합니다. 이는 적절한 모델 복잡도를 선택하고, 규제(Regularization)나 교차 검증 같은 기법을 통해 달성할 수 있습니다.
