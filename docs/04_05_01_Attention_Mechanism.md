# 어텐션 메커니즘 (Attention Mechanism)

## 1. 어텐션의 등장 배경: Seq2Seq 모델의 한계

기존의 RNN 기반 **Seq2Seq (Sequence-to-Sequence)** 모델은 인코더(Encoder)가 입력 시퀀스의 모든 정보를 고정된 크기의 **문맥 벡터(Context Vector)** 하나에 압축하고, 디코더(Decoder)가 이 문맥 벡터만을 이용해 출력 시퀀스를 생성하는 구조입니다.

이 구조는 다음과 같은 한계를 가집니다.
- **정보 병목 현상 (Information Bottleneck):** 입력 시퀀스가 길어질수록, 모든 정보를 하나의 벡터에 압축하는 과정에서 정보 손실이 발생합니다. 특히, 시퀀스 앞부분의 정보는 뒤로 갈수록 희석되기 쉽습니다.
- **장기 의존성 문제:** 인코더와 디코더 사이의 거리가 멀어질수록 그래디언트 전달이 어려워져 학습이 비효율적입니다.

**어텐션(Attention) 메커니즘**은 이러한 한계를 극복하기 위해 제안되었습니다. 어텐션의 핵심 아이디어는, 디코더가 매 시점마다 출력 단어를 예측할 때, **입력 시퀀스의 특정 부분에 더 집중(attention)하여** 관련 있는 정보만을 선택적으로 활용하는 것입니다.

## 2. 어텐션의 동작 원리

어텐션은 디코더의 각 타임 스텝에서 다음과 같은 과정을 통해 동작합니다.

1.  **어텐션 점수(Attention Score) 계산:**
    - 디코더의 현재 은닉 상태($h_t$)를 인코더의 모든 시점의 은닉 상태($\bar{h}_s$)들과 비교하여, 둘 사이의 **연관성(유사도)**을 나타내는 점수를 계산합니다.
    - 점수 계산 방법에는 dot-product, scaled dot-product, general 등 다양한 방식이 있습니다.
    - `score(h_t, \bar{h}_s)`

2.  **어텐션 가중치(Attention Weight) 계산:**
    - 계산된 모든 어텐션 점수들을 **소프트맥스(Softmax) 함수**에 통과시켜, 총합이 1인 확률 분포, 즉 어텐션 가중치($\alpha_t$)를 얻습니다.
    - 이 가중치는 디코더가 현재 시점에서 입력 시퀀스의 각 단어에 얼마나 '집중'해야 하는지를 나타냅니다.
    - `α_t = softmax(scores)`

3.  **문맥 벡터(Context Vector) 계산:**
    - 계산된 어텐션 가중치($\alpha_{ts}$)와 인코더의 각 시점의 은닉 상태($\bar{h}_s$)를 **가중합(weighted sum)**하여 새로운 문맥 벡터($c_t$)를 만듭니다.
    - 이 문맥 벡터는 현재 디코더 시점에서 가장 관련성이 높은 입력 정보들을 요약한 결과물입니다.
    - $c_t = \sum_s \alpha_{ts} \bar{h}_s$

4.  **최종 출력 계산:**
    - 계산된 문맥 벡터($c_t$)와 디코더의 현재 은닉 상태($h_t$)를 결합(concatenate)하여 최종 출력을 예측하는 데 사용합니다.

**시각적 이해:**
디코더가 "je"를 예측할 때, 인코더의 "I"에 높은 어텐션 가중치를 부여하고, "suis"를 예측할 때는 "am"에 높은 가중치를 부여하는 것을 볼 수 있습니다.
![Attention Mechanism](https://i.imgur.com/TjKcpA9.png)

## 3. 어텐션의 효과

- **정보 손실 방지:** 디코더는 매 시점마다 인코더의 모든 정보에 직접 접근할 수 있으므로, 고정된 크기의 벡터에 정보를 압축할 때 발생하는 병목 현상과 정보 손실 문제를 해결합니다.
- **장기 의존성 문제 해결:** 인코더와 디코더 사이에 직접적인 연결(shortcut)을 제공하여 그래디언트가 잘 흐를 수 있도록 돕고, 먼 거리의 단어 간의 관계도 효과적으로 학습할 수 있습니다.
- **해석 가능성(Interpretability) 제공:** 어텐션 가중치를 시각화하면, 모델이 어떤 입력 단어에 집중하여 특정 출력 단어를 예측했는지 직관적으로 파악할 수 있어 모델의 동작을 이해하는 데 도움이 됩니다.

어텐션 메커니즘은 기계 번역의 성능을 획기적으로 향상시켰으며, 이후 **트랜스포머(Transformer)** 모델의 핵심 아이디어로 발전하여 현대 자연어 처리의 패러다임을 바꾸는 계기가 되었습니다.
