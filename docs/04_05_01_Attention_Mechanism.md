# 어텐션 메커니즘 (Attention Mechanism)

## 1. 어텐션의 등장 배경: Seq2Seq 모델의 한계

기존의 RNN 기반 **Seq2Seq (Sequence-to-Sequence)** 모델은 인코더(Encoder)가 입력 시퀀스의 모든 정보를 고정된 크기의 **문맥 벡터(Context Vector)** 하나에 압축하고, 디코더(Decoder)가 이 문맥 벡터만을 이용해 출력 시퀀스를 생성하는 구조입니다.

이 구조는 다음과 같은 한계를 가집니다.
- **정보 병목 현상 (Information Bottleneck):** 입력 시퀀스가 길어질수록, 모든 정보를 하나의 벡터에 압축하는 과정에서 정보 손실이 발생합니다. 특히, 시퀀스 앞부분의 정보는 뒤로 갈수록 희석되기 쉽습니다.
- **장기 의존성 문제:** 인코더와 디코더 사이의 거리가 멀어질수록 그래디언트 전달이 어려워져 학습이 비효율적입니다.

**어텐션(Attention) 메커니즘**은 이러한 한계를 극복하기 위해 제안되었습니다. 어텐션의 핵심 아이디어는, 디코더가 매 시점마다 출력 단어를 예측할 때, **입력 시퀀스의 특정 부분에 더 집중(attention)하여** 관련 있는 정보만을 선택적으로 활용하는 것입니다.

## 2. 어텐션의 동작 원리

어텐션은 디코더의 각 타임 스텝에서 다음과 같은 과정을 통해 동작합니다.

1.  **어텐션 점수(Attention Score) 계산:**
    - 디코더의 현재 은닉 상태($h_t$)를 인코더의 모든 시점의 은닉 상태($\bar{h}_s$)들과 비교하여, 둘 사이의 **연관성(유사도)**을 나타내는 점수를 계산합니다.
    - 점수 계산 방법에는 dot-product, scaled dot-product, general 등 다양한 방식이 있습니다.
    - `score(h_t, \bar{h}_s)`

2.  **어텐션 가중치(Attention Weight) 계산:**
    - 계산된 모든 어텐션 점수들을 **소프트맥스(Softmax) 함수**에 통과시켜, 총합이 1인 확률 분포, 즉 어텐션 가중치($\alpha_t$)를 얻습니다.
    - 이 가중치는 디코더가 현재 시점에서 입력 시퀀스의 각 단어에 얼마나 '집중'해야 하는지를 나타냅니다.
    - `α_t = softmax(scores)`

3.  **문맥 벡터(Context Vector) 계산:**
    - 계산된 어텐션 가중치($\alpha_{ts}$)와 인코더의 각 시점의 은닉 상태($\bar{h}_s$)를 **가중합(weighted sum)**하여 새로운 문맥 벡터($c_t$)를 만듭니다.
    - 이 문맥 벡터는 현재 디코더 시점에서 가장 관련성이 높은 입력 정보들을 요약한 결과물입니다.
    - $c_t = \sum_s \alpha_{ts} \bar{h}_s$

4.  **최종 출력 계산:**
    - 계산된 문맥 벡터($c_t$)와 디코더의 현재 은닉 상태($h_t$)를 결합(concatenate)하여 최종 출력을 예측하는 데 사용합니다.

**예시: 기계 번역에서의 어텐션**

영어 문장 "I am a student"를 프랑스어 "Je suis étudiant"로 번역하는 과정:

```
입력: I    am    a     student
출력: Je   suis  un    étudiant

어텐션 가중치 (높을수록 더 집중):
- "Je" 예측 시:  [0.8, 0.1, 0.1, 0.0]   ("I"에 집중)
- "suis" 예측 시: [0.1, 0.8, 0.1, 0.0]  ("am"에 집중)
- "un" 예측 시:   [0.0, 0.0, 0.9, 0.1]  ("a"에 집중)
- "étudiant" 예측 시: [0.0, 0.0, 0.1, 0.9] ("student"에 집중)
```

이처럼 어텐션은 출력 단어마다 가장 관련성이 높은 입력 단어에 집중합니다.

## 3. 어텐션의 효과

- **정보 손실 방지:** 디코더는 매 시점마다 인코더의 모든 정보에 직접 접근할 수 있으므로, 고정된 크기의 벡터에 정보를 압축할 때 발생하는 병목 현상과 정보 손실 문제를 해결합니다.
- **장기 의존성 문제 해결:** 인코더와 디코더 사이에 직접적인 연결(shortcut)을 제공하여 그래디언트가 잘 흐를 수 있도록 돕고, 먼 거리의 단어 간의 관계도 효과적으로 학습할 수 있습니다.
- **해석 가능성(Interpretability) 제공:** 어텐션 가중치를 시각화하면, 모델이 어떤 입력 단어에 집중하여 특정 출력 단어를 예측했는지 직관적으로 파악할 수 있어 모델의 동작을 이해하는 데 도움이 됩니다.

## 4. 어텐션의 종류

### 4.1. 내적 어텐션 (Dot-Product Attention)
가장 간단한 형태로, 쿼리(Query)와 키(Key) 벡터의 내적을 어텐션 점수로 사용합니다.
```
score(q, k) = q · k
```

### 4.2. 스케일드 내적 어텐션 (Scaled Dot-Product Attention)
내적 어텐션에 스케일링 팩터를 추가하여 안정적인 학습을 돕습니다.
```
score(q, k) = (q · k) / √d_k
```
여기서 d_k는 키 벡터의 차원입니다.

### 4.3. 가법 어텐션 (Additive Attention)
신경망을 사용하여 어텐션 점수를 계산하는 방식입니다.
```
score(q, k) = v^T * tanh(W_q * q + W_k * k)
```

## 5. 실제 활용 사례

- **검색 엔진:** 사용자 질의와 문서 간의 관련성 계산
- **추천 시스템:** 사용자 선호도와 아이템 특성 간의 매칭
- **이미지 캡셔닝:** 이미지의 특정 영역과 생성되는 단어 간의 연관성
- **문서 요약:** 긴 문서에서 중요한 문장들에 집중하여 요약 생성

어텐션 메커니즘은 기계 번역의 성능을 획기적으로 향상시켰으며, 이후 **트랜스포머(Transformer)** 모델의 핵심 아이디어로 발전하여 현대 자연어 처리의 패러다임을 바꾸는 계기가 되었습니다.
