# 3.1.2. 결정 트리 (Decision Trees)

**결정 트리(Decision Tree)**는 데이터에 대한 일련의 질문(규칙)을 통해 데이터를 분류하거나 예측하는 지도 학습 모델입니다. 스무고개 놀이처럼, 나무(Tree) 구조의 꼭대기에서부터 시작하여 각 단계(노드)에서 특정 기준에 따라 데이터를 나누면서 아래로 내려가 최종 결론(리프 노드)에 도달합니다.

결정 트리는 그 구조가 직관적이어서 모델의 예측 과정을 사람이 쉽게 이해하고 해석할 수 있다는 큰 장점이 있습니다. 이 때문에 **화이트박스 모델(White-box Model)**이라고도 불립니다.

## 1. 결정 트리의 구조

- **루트 노드 (Root Node)**: 트리의 가장 최상위에 위치한 노드. 전체 데이터를 포함하며, 첫 번째 분기(질문)가 시작되는 지점입니다.
- **중간 노드 (Internal Node)**: 데이터를 특정 기준에 따라 두 개 이상의 서브 그룹으로 나누는 노드. 각 노드는 하나의 특성(feature)에 대한 질문(규칙)을 나타냅니다.
- **가지 (Branch / Edge)**: 노드와 노드를 연결하는 선. 중간 노드의 질문에 대한 답변(결과)을 나타냅니다.
- **리프 노드 (Leaf Node / Terminal Node)**: 트리의 가장 마지막에 위치한 노드. 더 이상 데이터가 분기되지 않으며, 최종적인 분류 결과(클래스) 또는 예측값(회귀)을 나타냅니다.

![Decision Tree Structure](https://upload.wikimedia.org/wikipedia/commons/thumb/e/eb/Decision_tree_model.png/400px-Decision_tree_model.png)
*(이미지 출처: 위키미디어)*

## 2. 결정 트리 학습의 핵심: 최적의 분기 찾기

결정 트리의 학습 목표는 주어진 데이터를 가장 잘 분류할 수 있는 일련의 질문(규칙)을 찾는 것입니다. 이를 위해 각 노드에서 데이터를 어떻게 나누는 것이 가장 효율적인지 판단할 기준이 필요합니다.

가장 효율적인 분기는, 분기 이후에 각 그룹(자식 노드)의 데이터가 가능한 한 **동일한 클래스**로 구성되도록 하는 것입니다. 즉, 각 그룹의 **불순도(Impurity)**를 최소화하는 방향으로 학습을 진행합니다.

### 2.1. 불순도 측정 지표

불순도를 측정하는 대표적인 지표는 **지니 불순도(Gini Impurity)**와 **엔트로피(Entropy)**입니다.

#### 1) 지니 불순도 (Gini Impurity)

지니 불순도는 특정 노드의 데이터가 얼마나 다른 클래스들로 섞여 있는지를 측정하는 지표입니다. 한 노드에서 랜덤하게 뽑은 데이터 포인트를 랜덤하게 분류했을 때 틀릴 확률을 의미합니다.

> **Gini = 1 - Σ (pᵢ)²**
> (pᵢ: 해당 노드에서 클래스 i에 속하는 샘플의 비율)

- **지니 불순도가 0이면**: 해당 노드의 모든 데이터가 하나의 클래스로만 이루어진 **순수 노드(Pure Node)**입니다.
- **지니 불순도가 높으면**: 여러 클래스의 데이터가 비슷하게 섞여 있다는 의미입니다.

결정 트리는 각 분기마다 **지니 불순도를 가장 많이 감소시키는** 특성과 그 기준값을 찾아 분기를 수행합니다.

#### 2) 엔트로피 (Entropy)와 정보 이득 (Information Gain)

엔트로피는 정보 이론에서 유래한 개념으로, 데이터의 **불확실성** 또는 **무질서도**를 측정하는 지표입니다.

> **Entropy = - Σ (pᵢ * log₂(pᵢ))**
> (pᵢ: 해당 노드에서 클래스 i에 속하는 샘플의 비율)

- **엔트로피가 0이면**: 모든 데이터가 하나의 클래스에 속해 불확실성이 없는 상태입니다.
- **엔트로피가 높으면**: 여러 클래스가 균등하게 섞여 있어 불확실성이 높은 상태입니다.

**정보 이득(Information Gain)**은 특정 특성을 사용하여 데이터를 분기하기 전과 후의 엔트로피 차이를 의미합니다. 즉, 해당 분기를 통해 얻게 되는 정보의 양(불확실성 감소량)을 나타냅니다.

> **Information Gain = Entropy(부모 노드) - Σ [ (자식 노드의 샘플 수 / 부모 노드의 샘플 수) * Entropy(자식 노드) ]**

결정 트리는 **정보 이득을 최대화하는** 방향으로 분기를 수행합니다. 정보 이득이 최대가 된다는 것은 지니 불순도가 최소가 되는 것과 유사한 의미를 가집니다.

## 3. 과대적합과 가지치기 (Pruning)

결정 트리는 불순도가 0이 될 때까지, 즉 모든 리프 노드가 완벽하게 순수해질 때까지 계속해서 분기를 수행하는 경향이 있습니다. 이렇게 되면 모델이 훈련 데이터의 아주 사소한 특징이나 노이즈까지 학습하게 되어 **과대적합(Overfitting)**이 발생하기 매우 쉽습니다.

과대적합을 방지하기 위해 **가지치기(Pruning)**라는 규제 기법을 사용합니다.

- **사전 가지치기 (Pre-pruning)**: 트리가 완전히 성장하기 전에 미리 특정 조건이 되면 성장을 멈추는 방식입니다.
  - `max_depth`: 트리의 최대 깊이를 제한합니다.
  - `min_samples_split`: 노드를 분기하기 위한 최소 샘플 수를 지정합니다.
  - `min_samples_leaf`: 리프 노드가 되기 위한 최소 샘플 수를 지정합니다.
- **사후 가지치기 (Post-pruning)**: 일단 트리를 최대한 성장시킨 후, 검증 데이터셋을 사용하여 불필요한 가지를 제거하는 방식입니다. 일반적으로 성능은 더 좋지만 계산 비용이 더 큽니다.

## 4. 결정 트리의 장단점

### 장점
- **해석의 용이성**: 모델의 결정 과정을 시각적으로 이해하기 쉬워 비전문가에게 설명하기 좋습니다.
- **적은 데이터 전처리**: 특성 스케일링(정규화/표준화)이 필요 없습니다.
- **다양한 데이터 타입 처리**: 수치형 데이터와 범주형 데이터를 모두 다룰 수 있습니다.

### 단점
- **과대적합 경향**: 규제(가지치기)를 제대로 하지 않으면 훈련 데이터에 과대적합되기 매우 쉽습니다.
- **불안정성**: 훈련 데이터의 작은 변화에도 트리의 구조가 크게 바뀔 수 있습니다.
- **편향 문제**: 특정 클래스가 데이터의 대부분을 차지할 경우, 해당 클래스에 편향된 트리가 생성될 수 있습니다.
- **경계선 한계**: 축에 평행한(axis-parallel) 결정 경계만 만들 수 있어, 데이터의 경계가 사선 형태일 경우 비효율적인 계단 형태의 경계를 만듭니다.
