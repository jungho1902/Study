# 퍼셉트론 (Perceptron)과 다층 퍼셉트론 (MLP)

## 1. 퍼셉트론 (Perceptron)

### 1.1. 개요
퍼셉트론은 인공 신경망의 가장 기본적인 단위로, 1957년 프랑크 로젠블랫(Frank Rosenblatt)에 의해 고안되었습니다. 다수의 입력을 받아 하나의 출력을 내보내는 간단한 선형 분류기(Linear Classifier)입니다.

- **입력 (Inputs):** 여러 개의 입력 값 ($x_1, x_2, ..., x_n$)
- **가중치 (Weights):** 각 입력 값의 중요도를 나타내는 가중치 ($w_1, w_2, ..., w_n$)
- **편향 (Bias):** 뉴런이 얼마나 쉽게 활성화되는지를 조절하는 값 ($b$)
- **활성화 함수 (Activation Function):** 계산된 값을 최종 출력 신호로 변환하는 함수 (주로 계단 함수 사용)

### 1.2. 동작 원리
1. 각 입력 값($x_i$)에 해당 가중치($w_i$)를 곱합니다.
2. 곱한 값들을 모두 더하고 편향($b$)을 더하여 **가중합(Weighted Sum)**을 계산합니다.
   $$ z = (x_1w_1 + x_2w_2 + ... + x_nw_n) + b = \sum_{i=1}^{n} x_iw_i + b $$
3. 계산된 가중합을 활성화 함수(예: 계단 함수)에 통과시켜 최종 출력을 결정합니다.
   $$ y = \begin{cases} 1 & \text{if } z \ge \theta \text{ (임계값)} \\ 0 & \text{if } z < \theta \end{cases} $$
   (편향을 임계값의 음수, 즉 $b = -\theta$로 간주하면, $z \ge 0$일 때 1, 그렇지 않으면 0으로 단순화할 수 있습니다.)

### 1.3. 한계
퍼셉트론은 **선형 분리(Linearly Separable)** 가능한 문제만 해결할 수 있습니다. 예를 들어, AND, OR 게이트는 구현할 수 있지만, XOR 게이트와 같이 선형으로 나눌 수 없는 문제는 해결하지 못합니다.

---

## 2. 다층 퍼셉트론 (Multi-Layer Perceptron, MLP)

### 2.1. 개요
다층 퍼셉트론(MLP)은 퍼셉트론의 한계를 극복하기 위해 제안된 구조로, 입력층(Input Layer)과 출력층(Output Layer) 사이에 하나 이상의 **은닉층(Hidden Layer)**을 추가한 신경망입니다.

- **입력층 (Input Layer):** 외부로부터 데이터를 받는 층
- **은닉층 (Hidden Layer):** 입력층과 출력층 사이에 위치하며, 입력 데이터의 특징을 추출하고 학습하는 역할을 합니다. MLP는 하나 이상의 은닉층을 가질 수 있습니다.
- **출력층 (Output Layer):** 최종 결과를 출력하는 층

### 2.2. 특징
- **비선형 활성화 함수:** MLP는 계단 함수 대신 시그모이드(Sigmoid), ReLU 등과 같은 **비선형(Non-linear) 활성화 함수**를 사용합니다. 이를 통해 선형으로 분리할 수 없는 복잡한 문제(예: XOR)를 해결할 수 있습니다.
- **역전파 알고리즘:** MLP는 **역전파(Backpropagation)** 알고리즘을 사용하여 가중치를 학습(업데이트)합니다. 출력 값과 실제 값의 오차를 계산하고, 이 오차를 다시 뒤쪽으로 전파하면서 각 층의 가중치를 미분을 통해 조정합니다.
- **범용 근사 정리 (Universal Approximation Theorem):** 하나의 은닉층을 가진 MLP는 충분한 수의 뉴런이 있다면 어떠한 연속 함수도 근사할 수 있음이 알려져 있습니다. 이는 MLP가 매우 강력한 모델임을 시사합니다.

### 2.3. MLP와 딥러닝
은닉층이 2개 이상인 MLP를 보통 **심층 신경망(Deep Neural Network, DNN)**이라고 부르며, 이는 딥러닝의 핵심적인 기초가 됩니다. MLP는 분류(Classification), 회귀(Regression) 등 다양한 문제에 적용될 수 있는 기본적인 딥러닝 모델입니다.
