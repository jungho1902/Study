# 그래디언트 (Gradient)

그래디언트는 다변수 함수에서 모든 변수에 대한 편미분을 벡터 형태로 묶은 것입니다. 그래디언트는 함수의 특정 지점에서 함수 값이 **가장 빠르게 증가하는 방향**과 그 **변화율**을 동시에 나타내는 매우 중요한 벡터입니다.

---

### 1. 그래디언트의 정의

함수 $`f(x_1, x_2, ..., x_n)`$이 주어졌을 때, 이 함수의 그래디언트는 **델(del)** 연산자 $`\nabla`$를 사용하여 $`\nabla f`$로 표기하며, 다음과 같이 정의됩니다.

$`\nabla f(x_1, ..., x_n) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}`$

- **결과:** 그래디언트는 입력 변수와 같은 차원을 가지는 **벡터**입니다.
- **구성:** 각 성분은 해당 변수에 대한 편미분 값입니다.

### 2. 그래디언트의 의미와 중요성

그래디언트는 머신러닝, 특히 딥러닝 모델을 최적화하는 데 있어 가장 핵심적인 개념입니다.

- **방향 (Direction):** 그래디언트 벡터 $`\nabla f`$는 함수 $`f`$의 값이 **가장 가파르게 증가하는 방향**을 가리킵니다.
- **크기 (Magnitude):** 그래디언트 벡터의 크기 $`\|\nabla f\|`$는 그 가장 가파른 방향으로의 **변화율(기울기)**을 나타냅니다. 크기가 클수록 함수 값이 더 빠르게 변한다는 의미입니다.

### 3. 경사 하강법 (Gradient Descent)

경사 하강법은 함수의 최솟값을 찾기 위한 대표적인 최적화 알고리즘입니다. 손실 함수 $`L(\theta)`$를 최소화하는 파라미터 $`\theta`$를 찾는 것이 목표입니다.

그래디언트가 함수 값이 가장 빠르게 **증가**하는 방향을 가리키므로, 함수의 최솟값을 찾기 위해서는 그래디언트의 **반대 방향**으로 이동해야 합니다.

경사 하강법의 파라미터 업데이트 규칙은 다음과 같습니다.

$`\theta_{new} = \theta_{old} - \eta \nabla L(\theta_{old})`$

- $`\theta`$: 모델의 파라미터(가중치, 편향 등) 벡터.
- $`\eta`$ (에타): **학습률(learning rate)**. 그래디언트 방향으로 얼마나 큰 보폭으로 이동할지를 결정하는 스칼라 값입니다.
- $`\nabla L(\theta)`$: 파라미터 $`\theta`$에 대한 손실 함수의 그래디언트.

이 과정을 반복하면, 파라미터 $`\theta`$는 손실 함수 $`L`$의 값이 점차 낮아지는 방향으로 업데이트되어, 최종적으로는 손실이 최소가 되는 지점(local minimum)에 수렴하게 됩니다.

---

## 예제 및 풀이 (Examples and Solutions)

### 예제 1: 함수의 그래디언트 계산

**문제:** 함수 $`f(x, y) = 3x^2y + 2x^3 + y^4`$의 그래디언트 $`\nabla f`$를 구하시오.

**풀이:**
그래디언트는 각 변수에 대한 편미분을 성분으로 갖는 벡터입니다. $`\nabla f = \begin{bmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{bmatrix}`$.
이 함수의 각 편미분은 '편미분' 챕터에서 이미 계산했습니다.

1.  **$`x`$에 대한 편미분:**
    $`\frac{\partial f}{\partial x} = 6xy + 6x^2`$

2.  **$`y`$에 대한 편미분:**
    $`\frac{\partial f}{\partial y} = 3x^2 + 4y^3`$

3.  **그래디언트 벡터 조합:**
    위에서 구한 편미분들을 벡터 형태로 조합합니다.

**답:**
$`\nabla f(x, y) = \begin{bmatrix} 6xy + 6x^2 \\ 3x^2 + 4y^3 \end{bmatrix}`$

**의미:**
만약 특정 지점, 예를 들어 $`(x, y) = (1, 1)`$에서의 그래디언트를 계산하면,
$`\nabla f(1, 1) = \begin{bmatrix} 6(1)(1) + 6(1)^2 \\ 3(1)^2 + 4(1)^3 \end{bmatrix} = \begin{bmatrix} 12 \\ 7 \end{bmatrix}`$
이는 점 (1, 1)에서 함수 $`f`$의 값이 $`[12, 7]`$ 방향으로 가장 가파르게 증가한다는 것을 의미합니다. 경사 하강법을 적용한다면, 이 지점에서는 $`[-12, -7]`$ 방향으로 파라미터를 업데이트해야 합니다.
