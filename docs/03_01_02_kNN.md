# 3.1.2. k-최근접 이웃 (k-Nearest Neighbors, k-NN)

**k-최근접 이웃(k-Nearest Neighbors, k-NN)**은 가장 직관적이고 간단한 머신러닝 알고리즘 중 하나입니다. "가까운 데이터들끼리는 비슷할 것이다"라는 단순한 가정에서 출발하며, 분류(Classification)와 회귀(Regression) 문제에 모두 사용할 수 있습니다.

k-NN은 별도의 모델 훈련 과정이 없는 **게으른 학습(Lazy Learning)** 또는 **인스턴스 기반 학습(Instance-based Learning)** 알고리즘으로 분류됩니다. 훈련 데이터를 저장해두었다가 예측 요청이 들어오면 그때 계산을 시작합니다.

## 1. k-NN의 동작 원리 (분류 기준)

k-NN의 예측 과정은 다음과 같은 단계로 이루어집니다.

1.  **k값 설정**: 예측에 참고할 주변 데이터의 개수, 즉 'k'를 정합니다. k는 사용자가 지정하는 하이퍼파라미터입니다.
2.  **거리 계산**: 새로운 데이터 포인트(예측 대상)와 훈련 데이터셋의 모든 데이터 포인트 사이의 **거리**를 계산합니다.
3.  **최근접 이웃 선택**: 계산된 거리상으로 가장 가까운 **k개**의 데이터 포인트를 선택합니다.
4.  **클래스 예측 (투표)**: 선택된 k개의 이웃들이 어떤 클래스에 속해있는지 확인하고, 다수결 원칙(Majority Vote)에 따라 가장 많은 표를 얻은 클래스를 새로운 데이터의 예측 클래스로 결정합니다.

![k-NN Example](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/K-Nearest_Neighbors_Example.svg/400px-K-Nearest_Neighbors_Example.svg.png)
*(이미지 출처: 위키미디어)*

위 그림에서, 새로운 데이터(녹색 원)의 클래스를 예측할 때,
- `k=3`이면, 가장 가까운 3개(파란색 사각형 2개, 빨간색 삼각형 1개)를 보고 다수결에 따라 **파란색 사각형**으로 예측합니다.
- `k=5`이면, 가장 가까운 5개(빨간색 삼각형 3개, 파란색 사각형 2개)를 보고 다수결에 따라 **빨간색 삼각형**으로 예측합니다.

## 2. 주요 개념

### 2.1. k값의 중요성

`k`는 k-NN 모델의 성능에 큰 영향을 미치는 중요한 하이퍼파라미터입니다.
- **k가 너무 작으면 (e.g., k=1)**: 모델이 노이즈나 이상치에 매우 민감해집니다. 훈련 데이터의 국소적인 특성에 과도하게 의존하여 **과대적합(Overfitting)**될 가능성이 높습니다.
- **k가 너무 크면**: 결정 경계가 너무 단순해져 데이터의 패턴을 제대로 학습하지 못하는 **과소적합(Underfitting)**이 발생할 수 있습니다. 예를 들어 k를 전체 데이터 수로 설정하면 항상 가장 많은 데이터를 가진 클래스로만 예측하게 됩니다.

따라서 적절한 k값을 찾는 것이 중요하며, 보통 교차 검증(Cross-Validation)을 통해 최적의 k를 찾습니다. 일반적으로 k는 홀수로 설정하여 동률(tie)이 발생하는 상황을 방지합니다.

### 2.2. 거리 측정 방법 (Distance Metric)

두 데이터 포인트 사이의 거리를 측정하는 방법은 다양하며, 어떤 방법을 선택하느냐에 따라 모델의 성능이 달라질 수 있습니다.
- **유클리드 거리 (Euclidean Distance)**: 두 점 사이의 직선 거리를 계산하며 가장 일반적으로 사용됩니다.
  > `sqrt(Σ(aᵢ - bᵢ)²) `
- **맨해튼 거리 (Manhattan Distance)**: 좌표축을 따라 이동하는, 즉 각 축의 차이의 절댓값을 합한 거리입니다.
  > `Σ|aᵢ - bᵢ|`
- **민코프스키 거리 (Minkowski Distance)**: 유클리드 거리와 맨해튼 거리를 일반화한 거리입니다.

### 2.3. 데이터 정규화/표준화의 필요성

k-NN은 거리를 기반으로 동작하기 때문에, 데이터의 **스케일(scale)**에 매우 민감합니다. 예를 들어, 한 특성은 0~1 사이의 값을 갖고 다른 특성은 0~1000 사이의 값을 갖는다면, 스케일이 큰 특성이 거리 계산을 거의 독점하게 됩니다.

따라서 k-NN을 사용하기 전에는 **특성 스케일링(Feature Scaling)**, 즉 모든 특성의 범위를 비슷하게 맞춰주는 정규화(Normalization)나 표준화(Standardization) 과정이 필수적입니다.

## 3. k-NN의 장단점

### 장점
- **간단하고 직관적**: 알고리즘이 매우 단순하여 이해하고 구현하기 쉽습니다.
- **별도의 훈련 과정 없음**: 모델을 미리 훈련시킬 필요가 없어 빠릅니다. (단, 이는 예측 시점에서는 단점이 됩니다.)
- **유연성**: 분류와 회귀 문제에 모두 적용 가능하며, 복잡한 결정 경계도 만들어낼 수 있습니다.

### 단점
- **게으른 학습의 한계**: 예측 시점에 모든 훈련 데이터와의 거리를 계산해야 하므로, 데이터가 많아질수록 예측 속도가 매우 느려집니다.
- **차원의 저주 (Curse of Dimensionality)**: 특성(feature)의 수가 매우 많아지면(고차원 데이터), 데이터 간의 거리가 거의 무의미해져 성능이 급격히 저하됩니다.
- **메모리 사용량**: 훈련 데이터 전체를 메모리에 저장해야 하므로 많은 메모리가 필요합니다.
- **특성 스케일링 필수**: 데이터 스케일에 민감하여 전처리 과정이 중요합니다.
