# 5.1.3. 가치 함수 (Value Function)

**가치 함수(Value Function)**는 강화학습에서 특정 상태나 특정 상태에서의 특정 행동이 얼마나 "좋은지"를 수치적으로 평가하는 함수입니다. 구체적으로, 현재 위치에서 앞으로 받을 것으로 기대되는 **누적 보상(Cumulative Reward)**의 총합을 나타냅니다. 가치 함수는 정책의 성능을 평가하고 개선하는 데 핵심적인 역할을 합니다.

가치 함수는 에이전트가 "어떤 상태가 더 좋은가?" 또는 "어떤 행동이 더 좋은가?"를 판단할 수 있게 해주는 척도 역할을 합니다. 예를 들어, 체스 게임에서 현재 보드 상황이 얼마나 유리한지를 평가하거나, 자율주행에서 특정 교차로에서 직진과 우회전 중 어떤 행동이 더 안전하고 효율적인지 판단하는 데 사용될 수 있습니다.

## 1. 가치 함수의 종류

### 1.1. 상태 가치 함수 (State-Value Function)

**상태 가치 함수 V^π(s)**는 특정 정책 π를 따를 때, 상태 s에서 시작하여 얻을 수 있는 기대 누적 보상을 나타냅니다.

> **V^π(s) = E_π[G_t | S_t = s]**

여기서:
- **G_t**: 시점 t부터의 누적 할인 보상 (Return)
- **E_π[·]**: 정책 π에 따른 기댓값
- **S_t = s**: 시점 t에서의 상태가 s

**누적 할인 보상 G_t**는 다음과 같이 정의됩니다:

> **G_t = R_{t+1} + γR_{t+2} + γ²R_{t+3} + ... = Σ_{k=0}^∞ γ^k R_{t+k+1}**

여기서:
- **γ (gamma)**: 할인 인수(Discount Factor), 0 ≤ γ ≤ 1
- **R_t**: 시점 t에서의 즉시 보상

### 1.2. 행동 가치 함수 (Action-Value Function)

**행동 가치 함수 Q^π(s, a)**는 특정 정책 π를 따를 때, 상태 s에서 행동 a를 선택한 후 얻을 수 있는 기대 누적 보상을 나타냅니다.

> **Q^π(s, a) = E_π[G_t | S_t = s, A_t = a]**

Q 함수는 "Q-function" 또는 "Quality function"이라고도 불리며, 특정 상태-행동 쌍의 "품질"을 평가합니다.

### 1.3. 상태 가치 함수와 행동 가치 함수의 관계

두 가치 함수는 다음과 같은 관계를 가집니다:

> **V^π(s) = Σ_a π(a|s) Q^π(s, a)**

이는 상태 s에서의 가치가 해당 상태에서 가능한 모든 행동의 가치를 정책의 확률에 따라 가중평균한 값과 같다는 의미입니다.

## 2. 벨만 방정식 (Bellman Equation)

### 2.1. 상태 가치 함수의 벨만 방정식

**벨만 방정식**은 가치 함수의 재귀적 관계를 나타내는 핵심 방정식입니다.

> **V^π(s) = Σ_a π(a|s) Σ_{s'} P(s'|s,a) [R(s,a,s') + γV^π(s')]**

이 방정식은 다음을 의미합니다:
- 현재 상태의 가치 = 즉시 보상 + (할인된) 다음 상태의 가치

### 2.2. 행동 가치 함수의 벨만 방정식

> **Q^π(s, a) = Σ_{s'} P(s'|s,a) [R(s,a,s') + γ Σ_{a'} π(a'|s') Q^π(s', a')]**

### 2.3. 벨만 최적성 방정식 (Bellman Optimality Equation)

최적 가치 함수들은 다음 방정식을 만족합니다:

- **최적 상태 가치 함수:**
  > **V*(s) = max_a Σ_{s'} P(s'|s,a) [R(s,a,s') + γV*(s')]**

- **최적 행동 가치 함수:**
  > **Q*(s, a) = Σ_{s'} P(s'|s,a) [R(s,a,s') + γ max_{a'} Q*(s', a')]**

## 3. 할인 인수 (Discount Factor)

### 3.1. 할인 인수의 역할

**할인 인수 γ**는 미래 보상의 현재 가치를 조절하는 매개변수입니다:

- **γ = 0**: 근시안적(myopic) 행동, 즉시 보상만 고려
- **γ = 1**: 모든 미래 보상을 동등하게 고려
- **0 < γ < 1**: 미래 보상을 점진적으로 할인

### 3.2. 할인 인수의 효과

1. **수렴성 보장**: 무한한 시간 범위에서도 가치 함수가 유한한 값을 가지도록 함
2. **불확실성 모델링**: 미래로 갈수록 예측이 어려워지는 현실을 반영
3. **학습 안정성**: 알고리즘의 수렴성과 안정성을 향상

## 4. 가치 함수의 활용

### 4.1. 정책 평가 (Policy Evaluation)

주어진 정책의 성능을 평가하기 위해 가치 함수를 계산하는 과정입니다.

### 4.2. 정책 개선 (Policy Improvement)

가치 함수를 이용해 더 좋은 정책을 찾는 과정입니다:

> **π'(s) = argmax_a Q^π(s, a)**

### 4.3. 최적 정책 도출

최적 가치 함수로부터 최적 정책을 구할 수 있습니다:

> **π*(s) = argmax_a Q*(s, a)**

---

## 예제 및 풀이 (Examples and Solutions)

### 예제 1: 단순한 2-상태 MDP에서의 가치 함수 계산

**문제:** 다음과 같은 마르코프 결정 과정(MDP)이 주어져 있습니다:
- 상태: S = {s₁, s₂}
- 행동: A = {a₁, a₂}
- 정책 π: π(a₁|s₁) = 0.8, π(a₂|s₁) = 0.2, π(a₁|s₂) = 0.3, π(a₂|s₂) = 0.7
- 상태 전이 확률과 보상:
  - P(s₁|s₁, a₁) = 0.7, R(s₁, a₁, s₁) = 2
  - P(s₂|s₁, a₁) = 0.3, R(s₁, a₁, s₂) = 5
  - P(s₁|s₁, a₂) = 0.4, R(s₁, a₂, s₁) = 1
  - P(s₂|s₁, a₂) = 0.6, R(s₁, a₂, s₂) = 3
  - P(s₁|s₂, a₁) = 0.2, R(s₂, a₁, s₁) = 4
  - P(s₂|s₂, a₁) = 0.8, R(s₂, a₁, s₂) = 1
  - P(s₁|s₂, a₂) = 0.5, R(s₂, a₂, s₁) = 6
  - P(s₂|s₂, a₂) = 0.5, R(s₂, a₂, s₂) = 2

할인 인수 γ = 0.9일 때, 상태 가치 함수 V^π(s₁)과 V^π(s₂)를 계산하시오.

**풀이:**

벨만 방정식을 사용하여 연립방정식을 세워 풉니다.

V^π(s₁) = Σ_a π(a|s₁) Σ_{s'} P(s'|s₁,a) [R(s₁,a,s') + γV^π(s')]

1. **V^π(s₁) 계산:**
   
   V^π(s₁) = 0.8 × [0.7 × (2 + 0.9 × V^π(s₁)) + 0.3 × (5 + 0.9 × V^π(s₂))]
            + 0.2 × [0.4 × (1 + 0.9 × V^π(s₁)) + 0.6 × (3 + 0.9 × V^π(s₂))]

   정리하면:
   V^π(s₁) = 0.8 × [1.4 + 0.63V^π(s₁) + 1.5 + 0.27V^π(s₂)]
            + 0.2 × [0.4 + 0.36V^π(s₁) + 1.8 + 0.54V^π(s₂)]

   V^π(s₁) = 2.32 + 0.504V^π(s₁) + 0.216V^π(s₂) + 0.44 + 0.072V^π(s₁) + 0.108V^π(s₂)

   V^π(s₁) = 2.76 + 0.576V^π(s₁) + 0.324V^π(s₂)

   **V^π(s₁) = 6.51 + 0.765V^π(s₂)** ... (식 1)

2. **V^π(s₂) 계산:**
   
   V^π(s₂) = 0.3 × [0.2 × (4 + 0.9 × V^π(s₁)) + 0.8 × (1 + 0.9 × V^π(s₂))]
            + 0.7 × [0.5 × (6 + 0.9 × V^π(s₁)) + 0.5 × (2 + 0.9 × V^π(s₂))]

   정리하면:
   V^π(s₂) = 0.3 × [0.8 + 0.18V^π(s₁) + 0.8 + 0.72V^π(s₂)]
            + 0.7 × [3 + 0.45V^π(s₁) + 1 + 0.45V^π(s₂)]

   V^π(s₂) = 0.48 + 0.054V^π(s₁) + 0.216V^π(s₂) + 2.8 + 0.315V^π(s₁) + 0.315V^π(s₂)

   V^π(s₂) = 3.28 + 0.369V^π(s₁) + 0.531V^π(s₂)

   **V^π(s₂) = 7.0 + 0.787V^π(s₁)** ... (식 2)

3. **연립방정식 풀이:**
   
   식 1에 식 2를 대입:
   V^π(s₁) = 6.51 + 0.765(7.0 + 0.787V^π(s₁))
   V^π(s₁) = 6.51 + 5.355 + 0.602V^π(s₁)
   0.398V^π(s₁) = 11.865
   
   **V^π(s₁) = 29.81**
   
   V^π(s₂) = 7.0 + 0.787 × 29.81 = 7.0 + 23.46
   
   **V^π(s₂) = 30.46**

**답:** V^π(s₁) = 29.81, V^π(s₂) = 30.46

### 예제 2: Q 함수와 V 함수의 관계

**문제:** 어떤 상태 s에서 3개의 행동이 가능하고, 각 행동의 Q 값이 다음과 같습니다:
- Q^π(s, a₁) = 15
- Q^π(s, a₂) = 20  
- Q^π(s, a₃) = 10

정책 π가 다음과 같을 때 상태 s의 가치 V^π(s)를 계산하시오:
- π(a₁|s) = 0.2
- π(a₂|s) = 0.5
- π(a₃|s) = 0.3

**풀이:**

상태 가치 함수와 행동 가치 함수의 관계식을 사용합니다:

V^π(s) = Σ_a π(a|s) Q^π(s, a)

계산:
V^π(s) = 0.2 × 15 + 0.5 × 20 + 0.3 × 10
       = 3 + 10 + 3
       = 16

**답:** V^π(s) = 16

**해설:** 이 예제는 상태 가치가 각 행동 가치의 정책 확률에 따른 가중평균임을 보여줍니다. 가장 높은 Q 값을 가진 행동 a₂가 가장 높은 확률(0.5)로 선택되어 전체 상태 가치에 가장 큰 기여를 합니다.
