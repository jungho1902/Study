# 3.2.2. 주성분 분석 (Principal Component Analysis, PCA)

**차원 축소(Dimensionality Reduction)**는 데이터의 특성(feature)이 너무 많을 때(고차원 데이터), 데이터의 중요한 정보는 최대한 유지하면서 특성의 수를 줄이는 기법입니다. 이는 "차원의 저주"를 피하고, 모델의 학습 속도를 높이며, 데이터를 시각화하는 데 도움을 줍니다.

**주성분 분석(PCA)**은 가장 널리 사용되는 대표적인 차원 축소 알고리즘입니다. PCA는 기존의 특성들을 선형적으로 결합하여, 서로 상관관계가 없는 새로운 축(특성)인 **주성분(Principal Components)**을 찾아냅니다.

## 1. PCA의 핵심 아이디어: 분산의 최대화

PCA의 핵심 목표는 데이터가 가장 넓게 퍼져 있는, 즉 **분산(variance)이 가장 큰 방향**을 찾는 것입니다. 이 방향이 데이터의 가장 중요한 정보를 담고 있다고 가정하기 때문입니다.

- **첫 번째 주성분 (PC1)**: 데이터의 분산이 가장 큰 방향을 나타내는 축입니다.
- **두 번째 주성분 (PC2)**: PC1과 **직교(orthogonal)**하는 (서로 상관관계가 없는) 방향 중에서, 남아있는 분산이 가장 큰 방향을 나타내는 축입니다.
- **세 번째 주성분 (PC3)**: PC1, PC2와 모두 직교하는 방향 중에서, 다음으로 분산이 가장 큰 축입니다.

이러한 방식으로 PCA는 기존의 특성 공간을, 데이터의 분산을 최대한 보존하는 새로운 주성분 축으로 변환합니다.

## 2. PCA의 동작 원리 (개념적 단계)

1.  **데이터 표준화 (Standardization)**:
    - PCA는 각 특성의 분산을 기반으로 동작하므로, 특성들의 스케일에 매우 민감합니다.
    - 따라서, PCA를 적용하기 전에 각 특성의 평균을 0, 표준편차를 1로 만드는 **데이터 표준화** 과정이 필수적입니다.

2.  **공분산 행렬 계산 (Covariance Matrix)**:
    - 표준화된 데이터의 특성들이 서로 어떻게 변하는지를 나타내는 공분산 행렬을 계산합니다.
    - 공분산 행렬은 특성 간의 상관관계를 보여줍니다.

3.  **고유값 분해 (Eigendecomposition)**:
    - 계산된 공분산 행렬에 대해 **고유값(Eigenvalue)**과 **고유벡터(Eigenvector)**를 계산합니다.
    - **고유벡터**: 데이터의 분산이 가장 큰 방향을 나타내며, 이것이 바로 **주성분(Principal Component)**이 됩니다.
    - **고유값**: 해당 고유벡터 방향으로 데이터가 어느 정도의 분산(중요도)을 가지는지를 나타내는 값입니다.

4.  **주성분 선택 및 변환**:
    - 고유값이 큰 순서대로 고유벡터(주성분)를 정렬합니다.
    - 사용자가 원하는 차원의 수(k)만큼, 또는 특정 비율(예: 전체 분산의 95%)의 정보를 보존할 수 있을 만큼의 상위 주성분을 선택합니다.
    - 원본 데이터를 선택된 k개의 주성분 축에 **투영(projection)**하여, 차원이 축소된 새로운 데이터셋을 얻습니다.

## 3. 설명된 분산 (Explained Variance)

각 주성분이 원본 데이터의 전체 분산 중 얼마나 많은 부분을 설명하는지를 나타내는 지표입니다. 예를 들어, PC1의 설명된 분산이 0.75라면, PC1 하나만으로도 원본 데이터의 변동성의 75%를 설명할 수 있다는 의미입니다.

이 설명된 분산 비율을 보고 몇 개의 주성분을 선택할지 결정할 수 있습니다. 일반적으로 "설명된 분산의 총합이 95% 이상이 되도록" 주성분을 선택하는 방법을 많이 사용합니다.

## 4. PCA의 장단점

### 장점
- **차원 축소**: 모델 학습에 필요한 계산 비용을 줄이고, "차원의 저주" 문제를 완화합니다.
- **다중공선성 제거**: 주성분들은 서로 직교하므로(상관관계가 0), 특성 간의 다중공선성 문제를 해결할 수 있습니다.
- **시각화**: 고차원 데이터를 2차원 또는 3차원으로 축소하여 시각적으로 탐색할 수 있게 해줍니다.
- **노이즈 감소**: 분산이 작은 주성분(주로 노이즈)을 제거함으로써 데이터의 노이즈를 줄이는 효과가 있습니다.

### 단점
- **해석의 어려움**: 주성분은 기존 특성들의 선형 결합으로 만들어지므로, 각 주성분이 어떤 의미를 갖는지 해석하기 어려울 수 있습니다.
- **정보 손실**: 차원을 축소하는 과정에서 일부 정보가 손실될 수 있습니다.
- **선형성 가정**: PCA는 데이터 내의 관계가 선형적이라고 가정합니다. 비선형 구조를 가진 데이터에는 적합하지 않을 수 있습니다. (이 경우 커널 PCA와 같은 비선형 차원 축소 기법을 사용합니다.)
- **스케일링 필수**: 데이터 스케일링 전처리 과정이 필수적입니다.

---

## 예제 및 풀이 (Examples and Solutions)

### 예제 1: PCA의 개념적 적용

**문제:** 학생들의 '키(cm)'와 '몸무게(kg)'라는 2개의 특성을 가진 데이터가 있다고 가정합시다. 이 2차원 데이터를 PCA를 통해 1차원으로 축소하는 과정을 개념적으로 설명하시오.

**풀이:**

1.  **데이터 분포 상상:**
    - '키'와 '몸무게'는 강한 양의 상관관계를 가질 가능성이 높습니다.
    - 2D 좌표 평면에 데이터를 점으로 나타내면, 점들은 대략 우상향하는 길쭉한 타원(elliptical) 형태로 분포할 것입니다.

2.  **주성분 찾기:**
    - **PC1 (첫 번째 주성분):** 이 길쭉한 타원 모양의 데이터가 가장 길게 퍼져 있는 방향, 즉 데이터의 분산이 가장 큰 방향으로 새로운 축(PC1)을 설정합니다. 이 축은 '키'와 '몸무게' 정보를 모두 종합하여 "전반적인 체격(overall physique)"을 나타내는 새로운 특성으로 해석될 수 있습니다.
    - **PC2 (두 번째 주성분):** PC1에 직교하는 방향으로 두 번째 축(PC2)이 설정됩니다. 이는 "전반적인 체격"으로 설명되지 않는 나머지 변동성(예: 마른 체형 vs. 통통한 체형)을 나타낼 수 있습니다.

3.  **차원 축소 (1차원으로):**
    - **주성분 선택:** 두 주성분 중 고유값이 훨씬 더 큰, 즉 데이터의 분산을 대부분 설명하는 PC1을 선택합니다.
    - **데이터 투영:** 모든 데이터 포인트들을 PC1 축 위로 직교 투영(orthogonal projection)합니다.

**답:**
원본 데이터의 각 학생은 `(키, 몸무게)`라는 2차원 벡터로 표현되었습니다. PCA를 통해 차원을 축소한 후, 각 학생은 **PC1 축 위의 단일 값**, 즉 "전반적인 체격" 점수라는 1차원 스칼라 값으로 표현됩니다.

**결론:**
이 과정을 통해, 원래의 2개 특성이 가진 정보의 대부분을 유지하면서도, 데이터를 더 다루기 쉬운 1개의 특성으로 성공적으로 축소할 수 있습니다.
