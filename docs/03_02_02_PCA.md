# 3.2.2. 주성분 분석 (Principal Component Analysis, PCA)

**차원 축소(Dimensionality Reduction)**는 데이터의 특성(feature)이 너무 많을 때(고차원 데이터), 데이터의 중요한 정보는 최대한 유지하면서 특성의 수를 줄이는 기법입니다. 이는 "차원의 저주"를 피하고, 모델의 학습 속도를 높이며, 데이터를 시각화하는 데 도움을 줍니다.

**주성분 분석(PCA)**은 가장 널리 사용되는 대표적인 차원 축소 알고리즘입니다. PCA는 기존의 특성들을 선형적으로 결합하여, 서로 상관관계가 없는 새로운 축(특성)인 **주성분(Principal Components)**을 찾아냅니다.

## 1. PCA의 핵심 아이디어: 분산의 최대화

PCA의 핵심 목표는 데이터가 가장 넓게 퍼져 있는, 즉 **분산(variance)이 가장 큰 방향**을 찾는 것입니다. 이 방향이 데이터의 가장 중요한 정보를 담고 있다고 가정하기 때문입니다.

- **첫 번째 주성분 (PC1)**: 데이터의 분산이 가장 큰 방향을 나타내는 축입니다.
- **두 번째 주성분 (PC2)**: PC1과 **직교(orthogonal)**하는 (서로 상관관계가 없는) 방향 중에서, 남아있는 분산이 가장 큰 방향을 나타내는 축입니다.
- **세 번째 주성분 (PC3)**: PC1, PC2와 모두 직교하는 방향 중에서, 다음으로 분산이 가장 큰 축입니다.

이러한 방식으로 PCA는 기존의 특성 공간을, 데이터의 분산을 최대한 보존하는 새로운 주성분 축으로 변환합니다.

## 2. PCA의 동작 원리 (개념적 단계)

1.  **데이터 표준화 (Standardization)**:
    - PCA는 각 특성의 분산을 기반으로 동작하므로, 특성들의 스케일에 매우 민감합니다.
    - 따라서, PCA를 적용하기 전에 각 특성의 평균을 0, 표준편차를 1로 만드는 **데이터 표준화** 과정이 필수적입니다.

2.  **공분산 행렬 계산 (Covariance Matrix)**:
    - 표준화된 데이터의 특성들이 서로 어떻게 변하는지를 나타내는 공분산 행렬을 계산합니다.
    - 공분산 행렬은 특성 간의 상관관계를 보여줍니다.

3.  **고유값 분해 (Eigendecomposition)**:
    - 계산된 공분산 행렬에 대해 **고유값(Eigenvalue)**과 **고유벡터(Eigenvector)**를 계산합니다.
    - **고유벡터**: 데이터의 분산이 가장 큰 방향을 나타내며, 이것이 바로 **주성분(Principal Component)**이 됩니다.
    - **고유값**: 해당 고유벡터 방향으로 데이터가 어느 정도의 분산(중요도)을 가지는지를 나타내는 값입니다.

4.  **주성분 선택 및 변환**:
    - 고유값이 큰 순서대로 고유벡터(주성분)를 정렬합니다.
    - 사용자가 원하는 차원의 수(k)만큼, 또는 특정 비율(예: 전체 분산의 95%)의 정보를 보존할 수 있을 만큼의 상위 주성분을 선택합니다.
    - 원본 데이터를 선택된 k개의 주성분 축에 **투영(projection)**하여, 차원이 축소된 새로운 데이터셋을 얻습니다.

## 3. 설명된 분산 (Explained Variance)

각 주성분이 원본 데이터의 전체 분산 중 얼마나 많은 부분을 설명하는지를 나타내는 지표입니다. 예를 들어, PC1의 설명된 분산이 0.75라면, PC1 하나만으로도 원본 데이터의 변동성의 75%를 설명할 수 있다는 의미입니다.

이 설명된 분산 비율을 보고 몇 개의 주성분을 선택할지 결정할 수 있습니다. 일반적으로 "설명된 분산의 총합이 95% 이상이 되도록" 주성분을 선택하는 방법을 많이 사용합니다.

## 4. PCA의 장단점

### 장점
- **차원 축소**: 모델 학습에 필요한 계산 비용을 줄이고, "차원의 저주" 문제를 완화합니다.
- **다중공선성 제거**: 주성분들은 서로 직교하므로(상관관계가 0), 특성 간의 다중공선성 문제를 해결할 수 있습니다.
- **시각화**: 고차원 데이터를 2차원 또는 3차원으로 축소하여 시각적으로 탐색할 수 있게 해줍니다.
- **노이즈 감소**: 분산이 작은 주성분(주로 노이즈)을 제거함으로써 데이터의 노이즈를 줄이는 효과가 있습니다.

### 단점
- **해석의 어려움**: 주성분은 기존 특성들의 선형 결합으로 만들어지므로, 각 주성분이 어떤 의미를 갖는지 해석하기 어려울 수 있습니다.
- **정보 손실**: 차원을 축소하는 과정에서 일부 정보가 손실될 수 있습니다.
- **선형성 가정**: PCA는 데이터 내의 관계가 선형적이라고 가정합니다. 비선형 구조를 가진 데이터에는 적합하지 않을 수 있습니다. (이 경우 커널 PCA와 같은 비선형 차원 축소 기법을 사용합니다.)
- **스케일링 필수**: 데이터 스케일링 전처리 과정이 필수적입니다.
