# 5.3.1. 모델 프리 학습 (Model-Free Learning)

**모델 프리 학습(Model-Free Learning)**은 환경의 내부 동작 원리(상태 전이 확률, 보상 함수)를 미리 알지 못한 상황에서, 오직 에이전트가 환경과 직접 상호작용하며 얻는 경험 데이터를 통해 최적의 정책이나 가치 함수를 학습하는 강화학습 접근 방법입니다.

모델 프리 학습은 현실 세계의 대부분 문제에서 핵심적인 역할을 합니다. 실제 환경에서는 모든 가능한 상황과 그 결과를 미리 예측하기 어렵기 때문입니다. 예를 들어, 자율주행차는 도로의 모든 상황을 미리 알 수 없으며, 게임 AI는 상대방의 모든 전략을 예측할 수 없습니다. 이런 상황에서 모델 프리 학습은 실제 경험을 통해 점진적으로 성능을 향상시키는 강력한 도구가 됩니다.

## 1. 모델 프리 학습의 기본 개념

### 1.1. 핵심 특징

- **경험 기반 학습**: (상태, 행동, 보상, 다음상태) 4-tuple 샘플만으로 학습
- **환경 모델 불필요**: P(s'|s,a)나 R(s,a,s')를 알 필요 없음
- **온라인 학습**: 환경과 상호작용하면서 실시간으로 정책 개선
- **일반화 능력**: 다양한 환경에 적용 가능한 범용성

### 1.2. 학습 데이터: 경험 샘플

모델 프리 학습에서 사용하는 기본 데이터는 **경험 샘플(Experience Sample)**입니다:

> **(s_t, a_t, r_{t+1}, s_{t+1})**

- **s_t**: 시점 t에서의 현재 상태
- **a_t**: 시점 t에서 선택한 행동  
- **r_{t+1}**: 행동 후 받은 즉시 보상
- **s_{t+1}**: 행동 후의 다음 상태

## 2. 모델 프리 학습의 분류

모델 프리 학습은 학습하는 대상에 따라 크게 두 가지 방법으로 나뉩니다:

### 2.1. 가치 기반 학습 (Value-Based Learning)

**가치 함수를 학습**하여 간접적으로 정책을 도출하는 방법입니다.

- **학습 대상**: V(s) 또는 Q(s, a)
- **정책 도출**: π(s) = argmax_a Q(s, a)
- **대표 알고리즘**: Q-Learning, SARSA, DQN
- **특징**: 명확한 가치 추정, off-policy 학습 가능

### 2.2. 정책 기반 학습 (Policy-Based Learning)

**정책을 직접 학습**하는 방법입니다.

- **학습 대상**: π(a|s)
- **최적화 방법**: 정책 경사(Policy Gradient)
- **대표 알고리즘**: REINFORCE, Actor-Critic, PPO
- **특징**: 연속 행동 공간 처리 용이, 확률적 정책 학습

### 2.3. 액터-크리틱 (Actor-Critic)

가치 기반과 정책 기반을 결합한 하이브리드 방법입니다.

- **액터(Actor)**: 정책 π(a|s)를 학습
- **크리틱(Critic)**: 가치 함수 V(s) 또는 Q(s,a)를 학습
- **장점**: 두 방법의 장점을 결합, 안정적인 학습

## 3. 주요 가치 기반 알고리즘

### 3.1. Q-Learning

**Q-Learning**은 off-policy 시간차 학습 방법으로, 최적 Q 함수를 직접 학습합니다.

#### 3.1.1. 알고리즘 원리

Q-Learning은 다음 업데이트 규칙을 사용합니다:

> **Q(s, a) ← Q(s, a) + α [r + γ max_{a'} Q(s', a') - Q(s, a)]**

여기서:
- **α**: 학습률 (Learning Rate)
- **γ**: 할인 인수 (Discount Factor)  
- **r**: 즉시 보상
- **s'**: 다음 상태

#### 3.1.2. 핵심 특징

- **Off-policy**: 행동 정책과 학습 정책이 다를 수 있음
- **최적성 보장**: 충분한 탐험 조건하에서 최적 Q 함수로 수렴
- **단순함**: 구현이 간단하고 이해하기 쉬움

#### 3.1.3. 의사코드

```
Q-Learning 알고리즘:
1. Q(s,a) 초기화 (모든 s, a에 대해)
2. 에피소드마다:
   a. 상태 s에서 시작
   b. 에피소드가 끝날 때까지:
      - ε-greedy로 행동 a 선택
      - 행동 a 실행 → 보상 r, 다음 상태 s' 관찰
      - Q(s,a) ← Q(s,a) + α[r + γ max Q(s',a') - Q(s,a)]
      - s ← s'
```

### 3.2. SARSA (State-Action-Reward-State-Action)

**SARSA**는 on-policy 시간차 학습 방법입니다.

#### 3.2.1. 알고리즘 원리

> **Q(s, a) ← Q(s, a) + α [r + γ Q(s', a') - Q(s, a)]**

Q-Learning과의 차이점:
- Q-Learning: `max_{a'} Q(s', a')` 사용 (최적 행동)
- SARSA: `Q(s', a')` 사용 (실제 선택한 행동)

#### 3.2.2. 핵심 특징

- **On-policy**: 실제 행동하는 정책을 학습
- **보수적**: 더 안전한 정책 학습 경향
- **현실적**: 실제 경험과 일치하는 학습

### 3.3. Q-Learning vs SARSA 비교

| 특징 | Q-Learning | SARSA |
|------|------------|-------|
| 학습 방식 | Off-policy | On-policy |
| 업데이트 | max Q(s',a') | Q(s',a') (실제 행동) |
| 수렴 특성 | 최적 정책 | 현재 정책에 맞는 가치 |
| 탐험 영향 | 탐험이 학습에 직접 영향 없음 | 탐험 방식이 학습에 직접 영향 |
| 안전성 | 위험한 행동도 학습 가능 | 더 보수적, 안전한 경향 |

## 4. 정책 기반 학습

### 4.1. 정책 경사 방법 (Policy Gradient Methods)

정책을 매개변수 θ로 표현하고, 경사 상승법으로 최적화하는 방법입니다.

#### 4.1.1. 목적 함수

정책의 성능을 다음과 같이 정의합니다:

> **J(θ) = E_{τ~π_θ}[R(τ)]**

여기서 τ는 궤적(trajectory), R(τ)는 궤적의 총 보상입니다.

#### 4.1.2. 정책 경사 정리 (Policy Gradient Theorem)

> **∇_θ J(θ) = E_{π_θ}[∇_θ log π_θ(a|s) Q^π(s,a)]**

### 4.2. REINFORCE 알고리즘

가장 기본적인 정책 경사 알고리즘입니다.

#### 4.2.1. 업데이트 규칙

> **θ ← θ + α ∇_θ log π_θ(a_t|s_t) G_t**

여기서 G_t는 시점 t부터의 실제 수익(return)입니다.

#### 4.2.2. 특징
- **Monte Carlo 방식**: 에피소드 완료 후 업데이트
- **높은 분산**: 샘플링으로 인한 불안정성
- **편향 없음**: 이론적으로 정확한 경사 추정

## 5. 탐험 vs 활용 (Exploration vs Exploitation)

모델 프리 학습에서 핵심적인 딜레마입니다.

### 5.1. 탐험-활용 딜레마

- **탐험(Exploration)**: 새로운 행동을 시도해 더 나은 정책 발견
- **활용(Exploitation)**: 현재 알고 있는 최선의 행동을 사용
- **딜레마**: 둘 사이의 적절한 균형이 필요

### 5.2. 주요 탐험 전략

#### 5.2.1. ε-greedy

> **π(a|s) = {1-ε+ε/|A| if a = argmax Q(s,a), ε/|A| otherwise}**

- **장점**: 구현 간단, 이론적 보장
- **단점**: 모든 행동을 균등하게 탐험

#### 5.2.2. Boltzmann Exploration (Softmax)

> **π(a|s) = exp(Q(s,a)/τ) / Σ_b exp(Q(s,b)/τ)**

- **τ (온도)**: 탐험 정도 조절
- **장점**: 가치에 비례한 탐험 확률
- **단점**: 온도 매개변수 조절 필요

#### 5.2.3. Upper Confidence Bound (UCB)

> **a_t = argmax_a [Q_t(a) + c√(ln t / N_t(a))]**

- **불확실성 고려**: 적게 시도된 행동에 높은 우선순위
- **적응적**: 시간에 따라 탐험 감소

## 6. 함수 근사 (Function Approximation)

상태나 행동 공간이 클 때 가치 함수나 정책을 근사하는 방법입니다.

### 6.1. 필요성

- **차원의 저주**: 상태/행동 공간이 커질수록 메모리/계산 문제
- **일반화**: 경험하지 않은 상태에 대한 추론 필요
- **효율성**: 저장 공간과 계산 시간 절약

### 6.2. 방법들

- **선형 함수 근사**: Q(s,a) = θ^T φ(s,a)
- **신경망**: Deep Q-Network (DQN), Policy Network
- **커널 방법**: Support Vector Regression
- **결정 트리**: Tree-based 근사

---

## 예제 및 풀이 (Examples and Solutions)

### 예제 1: Q-Learning 업데이트 계산

**문제:** 다음 상황에서 Q-Learning 업데이트 후의 Q(s₁, a₁) 값을 계산하시오.

- 현재 Q(s₁, a₁) = 3.2
- 행동 a₁을 선택 후 보상 r = 5 받음
- 다음 상태 s₂에서 가능한 Q 값들: Q(s₂, a₁) = 2.1, Q(s₂, a₂) = 4.3, Q(s₂, a₃) = 1.8
- 학습률 α = 0.1, 할인 인수 γ = 0.9

**풀이:**

Q-Learning 업데이트 공식을 사용합니다:

Q(s, a) ← Q(s, a) + α [r + γ max_{a'} Q(s', a') - Q(s, a)]

1. **다음 상태에서 최대 Q 값 찾기:**
   max_{a'} Q(s₂, a') = max(2.1, 4.3, 1.8) = 4.3

2. **TD 타겟 계산:**
   TD 타겟 = r + γ max_{a'} Q(s₂, a') = 5 + 0.9 × 4.3 = 5 + 3.87 = 8.87

3. **TD 오차 계산:**
   TD 오차 = 8.87 - 3.2 = 5.67

4. **Q 값 업데이트:**
   Q_new(s₁, a₁) = 3.2 + 0.1 × 5.67 = 3.2 + 0.567 = 3.767

**답:** Q(s₁, a₁) = 3.767

### 예제 2: SARSA vs Q-Learning 비교

**문제:** 다음과 같은 절벽 걷기(Cliff Walking) 환경에서 SARSA와 Q-Learning의 학습된 정책을 비교 설명하시오.

```
[S] [ ] [ ] [G]
[C] [C] [C] [ ]
```

여기서 S는 시작점, G는 목표점(+10 보상), C는 절벽(-100 보상)입니다. 각 이동마다 -1의 비용이 발생하고, 에이전트는 ε-greedy(ε=0.1) 정책을 사용합니다.

**풀이:**

**Q-Learning (Off-policy):**
- **학습 특성**: 최적 정책(절벽 근처의 최단 경로)을 학습
- **최적 경로**: S → 위 → 위 → 위 → G (총 보상: 10 - 4 = 6)
- **실제 행동**: 학습 중에는 ε-greedy로 인해 가끔 절벽에 떨어짐
- **결과**: 이론적 최적 경로를 찾지만 실행 시 위험 존재

**SARSA (On-policy):**
- **학습 특성**: 실제 행동 정책(ε-greedy)을 고려한 안전한 정책 학습
- **안전 경로**: S → 아래 → 아래 → 위 → 위 → 위 → 위 → G (총 보상: 10 - 7 = 3)
- **실제 행동**: 탐험으로 인한 절벽 낙하 위험을 고려
- **결과**: 보상은 낮지만 안전한 경로 선택

**답:** Q-Learning은 이론적 최적 경로를, SARSA는 실용적으로 안전한 경로를 학습합니다.

### 예제 3: ε-greedy 정책에서의 행동 선택 확률

**문제:** 어떤 상태 s에서 3개의 행동이 가능하고, Q 값이 다음과 같을 때 ε-greedy 정책(ε = 0.2)에서 각 행동이 선택될 확률을 계산하시오.

- Q(s, a₁) = 5.0
- Q(s, a₂) = 3.2  
- Q(s, a₃) = 7.1

**풀이:**

1. **최적 행동 식별:**
   argmax_a Q(s, a) = a₃ (Q(s, a₃) = 7.1이 최대)

2. **ε-greedy 확률 계산:**
   
   최적 행동 a₃의 선택 확률:
   P(a₃) = (1 - ε) + ε/|A| = (1 - 0.2) + 0.2/3 = 0.8 + 0.067 = 0.867

   비최적 행동들의 선택 확률:
   P(a₁) = P(a₂) = ε/|A| = 0.2/3 = 0.067

3. **검증:**
   P(a₁) + P(a₂) + P(a₃) = 0.067 + 0.067 + 0.867 = 1.001 ≈ 1.0 ✓

**답:**
- P(a₁) = 0.067 (6.7%)
- P(a₂) = 0.067 (6.7%)  
- P(a₃) = 0.867 (86.7%)

**해설:** ε-greedy는 최적 행동을 대부분의 시간(86.7%) 선택하면서도, 탐험을 위해 다른 행동들을 소수의 시간(각 6.7%) 선택하는 균형잡힌 전략을 보여줍니다.
