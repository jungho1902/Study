# 설명 가능한 AI (Explainable AI, XAI)

**설명 가능한 AI(Explainable AI, XAI)**는 인공지능 모델, 특히 딥러닝과 같이 복잡한 **블랙박스(Black Box) 모델**의 예측 결과와 그 결정 과정을 사람이 이해할 수 있는 형태로 설명하고 해석하는 것을 목표로 하는 기술 및 연구 분야입니다.

AI 시스템이 금융, 의료, 법률 등 중요한 결정을 내리는 데 사용되면서, "왜 AI가 그런 결정을 내렸는가?"에 대한 답을 제공하는 것이 필수적이 되었습니다. XAI는 모델의 투명성, 신뢰성, 공정성을 확보하기 위한 핵심적인 요소입니다.

## 1. 왜 XAI가 필요한가?

- **신뢰 및 채택 (Trust & Adoption):** 사용자와 이해관계자들이 AI 시스템의 결정 과정을 이해할 수 있을 때, 그 결과를 더 신뢰하고 시스템을 채택하게 됩니다. 의사가 AI의 진단 결과를 신뢰하고 활용하려면, AI가 어떤 근거로 그런 진단을 내렸는지 알아야 합니다.
- **디버깅 및 성능 개선 (Debugging & Improvement):** 모델이 왜 잘못된 예측을 했는지 이해하면, 모델의 약점을 파악하고 성능을 개선하는 데 도움이 됩니다.
- **공정성 및 편향성 검증 (Fairness & Bias Auditing):** 모델이 특정 민감 정보(성별, 인종 등)에 기반하여 차별적인 결정을 내리는지 확인하고, 편향성을 감지하고 완화하는 데 필수적입니다.
- **규제 준수 (Regulatory Compliance):** 유럽의 GDPR과 같은 데이터 보호 규제는 "설명을 요구할 권리"를 명시하고 있어, 특정 산업 분야에서는 AI 결정에 대한 설명 제공이 법적으로 요구됩니다.
- **새로운 지식 발견 (Knowledge Discovery):** 모델이 데이터로부터 학습한 패턴을 분석함으로써, 인간이 미처 발견하지 못했던 새로운 인사이트나 지식을 얻을 수 있습니다.

## 2. XAI 기술의 분류

XAI 기술은 설명의 범위, 모델의 종류, 설명 방식에 따라 다양하게 분류될 수 있습니다.

### 2.1. 설명의 범위에 따른 분류

- **전역적 설명 (Global Explanation):** 모델 **전체**가 어떻게 동작하는지에 대한 종합적인 설명을 제공합니다. 모델이 어떤 특징(feature)을 중요하게 생각하는지, 특징 간의 상호작용은 어떠한지 등 전반적인 행동 방식을 이해하는 데 중점을 둡니다.
- **지역적 설명 (Local Explanation):** **하나의 특정 예측**에 대해 "왜 이 데이터에 대해 이런 예측을 했는가?"를 설명합니다. 개별적인 결정의 근거를 파악하는 데 사용됩니다.

### 2.2. 모델의 종류에 따른 분류

- **모델 내재적 방법 (Intrinsic Methods):** 모델 자체가 구조적으로 단순하여 해석이 용이한 경우입니다.
  - 예: 선형 회귀(Linear Regression), 로지스틱 회귀(Logistic Regression), 결정 트리(Decision Tree) 등. 이 모델들은 어떤 특징이 결과에 얼마나 영향을 미쳤는지 계수(coefficient)나 규칙(rule)을 통해 명확하게 알 수 있습니다.
- **모델 사후적 방법 (Post-hoc Methods):** 이미 학습된 블랙박스 모델(딥러닝, 앙상블 모델 등)에 대해, 모델의 내부 구조를 수정하지 않고 입력과 출력의 관계를 분석하여 설명을 생성하는 방식입니다. 대부분의 XAI 연구가 여기에 해당합니다.

## 3. 대표적인 XAI 알고리즘 (모델 사후적 방법)

### 3.1. LIME (Local Interpretable Model-agnostic Explanations)

- **지역적 설명(Local)**을 제공하며, 어떤 모델에든 적용 가능한 **모델 불가지론적(Model-agnostic)** 방법입니다.
- **핵심 아이디어:** 복잡한 블랙박스 모델의 전체를 이해하려는 대신, **설명하려는 특정 데이터 포인트 주변에서는 모델이 단순한 선형 모델처럼 행동할 것**이라고 가정합니다.
- **작동 방식:**
  1. 설명하고 싶은 원본 데이터 하나를 선택합니다.
  2. 원본 데이터 주변에서 약간의 노이즈를 주거나 일부 특징을 제외하여 수많은 가상의 샘플들을 생성합니다.
  3. 이 가상 샘플들을 원래의 블랙박스 모델에 넣어 예측값을 얻습니다.
  4. 원본 데이터에 가까운 샘플에 더 높은 가중치를 부여하여, 이 가상 샘플들과 예측값들을 가장 잘 설명하는 **단순하고 해석 가능한 모델(예: 선형 회귀)**을 학습시킵니다.
  5. 이 단순 모델의 계수(coefficient)를 통해, 어떤 특징이 해당 예측에 긍정적 또는 부정적인 영향을 미쳤는지 설명합니다.

### 3.2. SHAP (SHapley Additive exPlanations)

- 게임 이론(Game Theory)의 **섀플리 값(Shapley Value)** 개념을 AI 모델 설명에 적용한 방법입니다. LIME과 마찬가지로 지역적 설명을 제공하며, 전역적 설명으로도 확장이 가능합니다.
- **핵심 아이디어:** 모델의 예측에 대한 각 특징(feature)의 **기여도**를 공정하게 배분합니다. 즉, 특정 특징이 예측에 얼마나 기여했는지를 정량적으로 계산합니다.
- **섀플리 값의 의미:** "여러 특징들이 협력하여 예측을 만들어낼 때, 각 특징이 평균적으로 얼마나 기여했는가?"를 나타내는 값입니다.
  - **긍정적 SHAP 값:** 해당 특징이 예측 확률을 높이는 방향으로 기여했음을 의미합니다.
  - **부정적 SHAP 값:** 해당 특징이 예측 확률을 낮추는 방향으로 기여했음을 의미합니다.
- **장점:**
  - **이론적 기반:** 게임 이론에 기반하여, 기여도를 공정하게 배분하는 유일한 방법임이 수학적으로 증명되었습니다. (Local Accuracy, Missingness, Consistency)
  - **전역적 설명:** 각 데이터에 대한 지역적 설명(SHAP 값)들을 모아서, 모델 전체에서 각 특징이 미치는 영향이나 특징 간의 상호작용을 시각적으로 분석할 수 있습니다.

XAI는 단순히 모델의 예측을 설명하는 것을 넘어, 인간과 AI가 협력하고, AI 기술을 더 책임감 있고 윤리적으로 발전시키기 위한 필수적인 다리 역할을 합니다.
