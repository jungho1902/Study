# 배치 정규화 (Batch Normalization)

## 1. 배치 정규화의 개념과 필요성

### 1.1. 내부 공변량 변화 (Internal Covariate Shift)
신경망이 학습하는 동안, 각 층의 가중치(파라미터)는 계속해서 업데이트됩니다. 이로 인해 특정 층의 입장에서 볼 때, 그 이전 층들의 출력이 계속해서 변하게 됩니다. 즉, **각 층에 들어오는 입력 데이터의 분포가 학습 과정 중에 계속 바뀌는 현상**이 발생하는데, 이를 **내부 공변량 변화(Internal Covariate Shift)**라고 합니다.

이러한 변화는 다음과 같은 문제를 야기합니다.
- **학습 속도 저하:** 각 층은 계속해서 변하는 입력 분포에 적응해야 하므로 학습이 불안정해지고 속도가 느려집니다.
- **기울기 소실/폭주 문제:** 입력 분포가 활성화 함수의 비선형적인 구간(예: Sigmoid의 양쪽 끝)에 치우치게 되면, 그래디언트가 0에 가까워지거나(소실) 매우 커지는(폭주) 문제가 발생하기 쉽습니다.
- **가중치 초기화에 대한 높은 의존성:** 가중치 초기값에 따라 학습 결과가 크게 달라질 수 있습니다.

### 1.2. 배치 정규화의 역할
**배치 정규화(Batch Normalization)**는 이러한 내부 공변량 변화 문제를 해결하기 위해 제안된 기법입니다. 각 층의 활성화 함수(Activation Function)를 통과하기 전의 입력값을 **미니배치(mini-batch) 단위**로 정규화하여, 입력 분포를 평균이 0, 분산이 1인 정규분포에 가깝게 만듭니다.

이를 통해 각 층의 입력 분포를 안정적으로 유지하여 학습 과정을 원활하게 만듭니다.

## 2. 배치 정규화의 과정

배치 정규화는 미니배치 데이터에 대해 다음과 같은 단계로 적용됩니다.

1.  **미니배치 평균 계산:** 미니배치 내의 데이터들의 평균($\mu_B$)을 구합니다.
2.  **미니배치 분산 계산:** 미니배치 내의 데이터들의 분산($\sigma_B^2$)을 구합니다.
3.  **정규화:** 각 데이터를 평균과 분산을 이용해 정규화합니다. (안정성을 위해 분모에 작은 값 $\epsilon$을 더합니다.)
    $$ \hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} $$
4.  **크기 변환 및 이동 (Scale and Shift):** 정규화된 데이터는 활성화 함수의 표현력을 제한할 수 있습니다. 예를 들어, Sigmoid 함수는 입력이 0 근처일 때 거의 선형 함수처럼 동작합니다. 이를 방지하기 위해, 학습 가능한 파라미터인 **감마($\gamma$, scale)**와 **베타($\beta$, shift)**를 도입하여 원래의 표현력을 복원할 수 있도록 합니다.
    $$ y_i = \gamma \hat{x}_i + \beta $$
    - 만약 모델이 $\gamma = \sqrt{\sigma_B^2 + \epsilon}$ 이고 $\beta = \mu_B$ 라고 학습한다면, 배치 정규화는 항등 변환(identity transform)이 되어 원래의 입력을 그대로 복원할 수 있습니다. 즉, 모델이 스스로 정규화의 필요성을 판단하게 됩니다.

이 과정은 신경망의 각 층에서 활성화 함수 이전에 독립적으로 적용됩니다.

## 3. 배치 정규화의 효과

- **학습 속도 향상:** 각 층의 입력 분포가 안정화되어 학습이 더 빠르고 원활하게 진행됩니다.
- **기울기 소실/폭주 문제 완화:** 활성화 함수의 입력값을 적절한 범위로 유지하여 그래디언트가 사라지거나 폭주하는 것을 방지합니다.
- **가중치 초기화에 대한 의존성 감소:** 초기값이 다소 불안정하더라도 배치 정규화가 이를 보정해주므로, 초기화의 중요성이 줄어듭니다.
- **규제(Regularization) 효과:** 미니배치의 평균과 분산을 사용하므로, 학습 과정에 약간의 노이즈(noise)가 추가됩니다. 이는 드롭아웃(Dropout)과 유사한 규제 효과를 가져와 모델의 일반화 성능을 높이는 데 도움을 줄 수 있습니다.

**주의할 점:**
- **추론(Inference) 시:** 추론 시점에는 미니배치가 없으므로, 학습 과정에서 계산해 둔 전체 훈련 데이터의 이동 평균(moving average)된 평균과 분산을 사용하여 정규화를 수행합니다.
- **배치 크기 의존성:** 배치 크기가 너무 작으면, 미니배치의 평균과 분산이 전체 데이터의 통계치를 잘 대표하지 못해 성능이 저하될 수 있습니다.

배치 정규화는 현대 딥러닝 모델에서 안정성과 성능을 높이기 위해 널리 사용되는 필수적인 기법 중 하나입니다.
