# 파트 1: AI/ML 학습을 위한 기초 (Foundations)

## 1.1. 수학 (Mathematics)

### 1.1.2. 미적분 (Calculus)

#### 1. 극한 (Limits)과 연속 (Continuity)

**들어가며**

미적분은 '변화'를 다루는 수학의 한 분야입니다. 머신러닝 모델이 손실 함수(Loss Function)의 값을 최소화하기 위해 파라미터를 점진적으로 '변화'시키는 과정(경사 하강법)이나, 데이터의 미세한 변화가 모델의 예측에 어떤 영향을 미치는지 이해하는 데 미적분의 개념은 필수적입니다. 그 중에서도 **극한**과 **연속**은 미적분의 모든 논리를 떠받치는 가장 근본적인 아이디어입니다.

---

**1. 극한 (Limit)**

- **정의:** 함수의 극한이란, 입력 변수 $`x`$가 특정 값 $`a`$에 **'무한히 가까워질 때'**, 함수 값 $`f(x)`$가 어떤 값 $`L`$에 가까워지는지를 나타냅니다.
- **핵심 아이디어:** "도착하는가?"가 아니라 **"어디를 향해 가는가?"** 에 대한 개념입니다. $`x`$가 $`a`$가 되는 실제 순간에는 관심이 없고, $`a`$에 아주 가까워지는 그 '경향성'에 주목합니다.
- **표기법:**
  - $`\lim_{x \to a} f(x) = L`$
  - 위 식은 "$`x`$가 $`a`$에 가까워질 때 $`f(x)`$의 극한은 $`L`$이다"라고 읽습니다.

- **직관적인 예시:**
  - 당신이 벽을 향해 걸어간다고 상상해보세요. 첫 번째에는 남은 거리의 절반만큼, 두 번째에도 남은 거리의 절반만큼, 이런 식으로 계속 걸어갑니다. 당신은 '논리적으로' 벽에 영원히 닿을 수 없지만, 당신의 '목표 지점(극한값)'은 명백히 '벽'입니다.
  - 함수 $`f(x) = x + 1`$ 에서 $`x`$가 2에 가까워진다고 해봅시다.
    - $`x`$ = 1.9 -> $`f(x)`$ = 2.9
    - $`x`$ = 1.99 -> $`f(x)`$ = 2.99
    - $`x`$ = 1.999 -> $`f(x)`$ = 2.999
    - 반대로, $`x`$ = 2.1 -> $`f(x)`$ = 3.1
    - $`x`$ = 2.01 -> $`f(x)`$ = 3.01
    - $`x`$가 2보다 크든 작든, 2에 가까워질수록 $`f(x)`$는 3을 향해 다가갑니다. 따라서 $`\lim_{x \to 2} (x+1) = 3`$ 입니다.

- **AI/ML에서의 중요성:**
  - **미분(Derivatives)**의 정의 자체가 극한을 기반으로 합니다. 미분은 모델의 파라미터를 어느 방향으로 얼마나 업데이트해야 할지 알려주는 '기울기(gradient)'를 계산하는 핵심 도구입니다.
  - 모델의 복잡한 동작을 수학적으로 분석하고 이해하는 기초를 제공합니다.

**2. 연속 (Continuity)**

- **정의:** 어떤 함수가 특정 지점에서 **'연속'**이라는 것은, 그 함수의 그래프가 그 지점에서 **끊어지지 않고 부드럽게 이어져 있다**는 의미입니다.
- **직관적인 설명:** 그래프를 그릴 때 펜을 떼지 않고 한 번에 그릴 수 있다면 그 함수는 연속 함수입니다.
- **수학적 조건:** 함수 $`f(x)`$가 점 $`x=a`$에서 연속이려면 다음 세 가지 조건을 모두 만족해야 합니다.
  1.  **함수값이 정의되어야 한다:** $`f(a)`$가 존재해야 합니다. (그래프에 구멍이 뚫려있지 않아야 함)
  2.  **극한값이 존재해야 한다:** $`\lim_{x \to a} f(x)`$가 존재해야 합니다. (그래프가 갑자기 점프하지 않아야 함)
  3.  **함수값과 극한값이 같아야 한다:** $`\lim_{x \to a} f(x) = f(a)`$ 입니다. (극한을 향해 다가가는 목표 지점이 실제 함수값과 일치해야 함)

- **예시:**
  - **연속 함수:** $`f(x) = x^2`$, $`f(x) = \sin(x)`$ 등 대부분의 다항함수, 삼각함수는 모든 지점에서 연속입니다.
  - **불연속 함수:**
    - $`f(x) = 1/x`$ 는 $`x=0`$에서 함수값이 정의되지 않아 불연속입니다.
    - 계단 함수(Step function)는 특정 지점에서 극한값이 존재하지 않거나(좌극한/우극한이 다름) 함수값과 달라 불연속입니다.

- **AI/ML에서의 중요성:**
  - **최적화:** 우리가 최적화하려는 손실 함수(Loss function)가 연속적일 때, 경사 하강법 같은 최적화 알고리즘이 안정적으로 작동하여 최적점을 찾아갈 수 있습니다. 만약 손실 함수가 여기저기 끊어져 있다면, 기울기 정보가 무의미해지거나 최적점을 찾기 매우 어려워집니다.
  - **활성화 함수:** 딥러닝에서 사용되는 시그모이드(Sigmoid)나 Tanh 같은 활성화 함수들은 미분 가능하며 연속적인 함수입니다. 이는 역전파(Backpropagation) 과정에서 그래디언트가 원활하게 흐를 수 있도록 하여 안정적인 학습을 가능하게 합니다. (ReLU는 $`x=0`$에서 첨점을 가지지만, 전체적으로 연속이며 대부분의 구간에서 미분 가능하여 널리 사용됩니다.)

---

**정리**

- **극한**은 '어디로 향하는가'에 대한 개념으로, 미분의 기초를 형성합니다.
- **연속**은 '그래프가 끊어지지 않았는가'에 대한 개념으로, 안정적인 모델 최적화의 기반이 됩니다.

이 두 가지 개념은 앞으로 배울 미분과 적분을 이해하기 위한 필수적인 첫 단추입니다.
