# 3.1.2. 로지스틱 회귀 (Logistic Regression)

**로지스틱 회귀(Logistic Regression)**는 이름에 '회귀'가 포함되어 있지만, 실제로는 **분류(Classification)** 문제를 해결하기 위한 지도 학습 알고리즘입니다. 특히, 두 개의 클래스(예: Yes/No, 0/1, 합격/불합격) 중 하나를 예측하는 **이진 분류(Binary Classification)** 문제에 널리 사용됩니다.

로지스틱 회귀는 선형 회귀의 아이디어를 기반으로 하지만, 출력을 0과 1 사이의 확률 값으로 변환하여 분류를 수행합니다.

## 1. 시그모이드 함수 (Sigmoid Function)

선형 회귀의 출력값 `Wx + b`는 음의 무한대에서 양의 무한대까지 어떤 값이든 가질 수 있습니다. 분류 문제에 이를 직접 적용하기는 어렵습니다. 로지스틱 회귀는 이 문제를 해결하기 위해 **시그모이드 함수(Sigmoid Function)**, 또는 **로지스틱 함수(Logistic Function)**를 사용합니다.

> **g(z) = 1 / (1 + e⁻ᶻ)**

시그모이드 함수는 입력값 `z` (여기서는 선형 회귀의 출력 `Wx + b`)를 0과 1 사이의 값으로 변환하는 역할을 합니다. 이 출력값은 특정 클래스에 속할 **확률**로 해석할 수 있습니다.

- `z`가 매우 큰 양수이면, `e⁻ᶻ`는 0에 가까워져 함수 값은 1에 수렴합니다.
- `z`가 매우 큰 음수이면, `e⁻ᶻ`는 무한대에 가까워져 함수 값은 0에 수렴합니다.
- `z`가 0이면, 함수 값은 0.5가 됩니다.

![Sigmoid Function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/600px-Logistic-curve.svg.png)
*(이미지 출처: 위키미디어)*

## 2. 가설 함수 (Hypothesis Function)

로지스틱 회귀의 가설 함수는 선형 회귀의 결과를 시그모이드 함수에 통과시킨 형태입니다.

> **H(x) = 1 / (1 + e⁻⁽ᵂˣ⁺ᵇ⁾)**

이 가설 함수 `H(x)`의 출력은 "주어진 입력 `x`에 대해 클래스가 1일 확률" (P(y=1|x))을 의미합니다.

- `H(x) > 0.5` 이면, 클래스를 1로 예측합니다.
- `H(x) < 0.5` 이면, 클래스를 0으로 예측합니다.

이때 `H(x) = 0.5`가 되는 지점을 **결정 경계(Decision Boundary)**라고 하며, 두 클래스를 구분하는 기준선(또는 면)이 됩니다.

## 3. 비용 함수 (Cost Function)

선형 회귀에서 사용했던 평균 제곱 오차(MSE)를 로지스틱 회귀의 비용 함수로 사용하면, 비용 함수가 여러 개의 지역 최솟값을 갖는 비볼록(non-convex) 함수 형태가 되어 경사 하강법으로 최적의 해를 찾기 어렵습니다.

따라서 로지스틱 회귀는 **로그 손실(Log Loss)**, 또는 **이진 교차 엔트로피(Binary Cross-Entropy)**라는 새로운 비용 함수를 사용합니다.

> **if y=1:  Cost = -log(H(x))**
> **if y=0:  Cost = -log(1 - H(x))**

- **실제값이 1일 때 (y=1):**
  - 모델이 1로 잘 예측하면 (`H(x) → 1`), `log(H(x))`는 0에 가까워져 비용이 작아집니다.
  - 모델이 0으로 잘못 예측하면 (`H(x) → 0`), `log(H(x))`는 음의 무한대로 발산하여 비용이 매우 커집니다.
- **실제값이 0일 때 (y=0):**
  - 모델이 0으로 잘 예측하면 (`H(x) → 0`), `log(1-H(x))`는 0에 가까워져 비용이 작아집니다.
  - 모델이 1로 잘못 예측하면 (`H(x) → 1`), `log(1-H(x))`는 음의 무한대로 발산하여 비용이 매우 커집니다.

이 두 경우를 하나의 식으로 통합하면 다음과 같습니다.

> **Cost(W, b) = -(1/m) * Σ [ y⁽ⁱ⁾log(H(x⁽ⁱ⁾)) + (1-y⁽ⁱ⁾)log(1-H(x⁽ⁱ⁾)) ]**

이 비용 함수는 볼록(convex) 함수 형태를 띠므로, 경사 하강법을 통해 안정적으로 비용을 최소화하는 최적의 가중치(W, b)를 찾을 수 있습니다.

---

## 예제 및 풀이 (Examples and Solutions)

### 예제 1: 시험 합격/불합격 예측

**문제:** '공부 시간'에 따른 '시험 합격 여부'를 예측하는 모델을 만든다고 가정합시다. '합격'은 클래스 1, '불합격'은 클래스 0입니다. 로지스틱 회귀 모델이 어떻게 예측을 수행하는지 개념적으로 설명하시오.

**풀이:**

1.  **선형 함수 계산:**
    - 먼저 모델은 입력 데이터 '공부 시간(x)'에 대해 선형 함수 `z = Wx + b`를 계산합니다. 이 `z` 값은 어떤 실수 값이든 될 수 있습니다.
    - 예를 들어, `W=1.5`, `b=-4` 라고 가정해 봅시다.
      - 공부 시간이 2시간(`x=2`)이면, `z = 1.5*2 - 4 = -1`
      - 공부 시간이 4시간(`x=4`)이면, `z = 1.5*4 - 4 = 2`

2.  **시그모이드 함수 적용 (확률 계산):**
    - 계산된 `z` 값을 시그모이드 함수에 넣어 0과 1 사이의 확률값으로 변환합니다.
    - **공부 시간이 2시간일 때:**
      - `H(x) = 1 / (1 + e⁻⁽⁻¹⁾) = 1 / (1 + e¹) ≈ 1 / 3.718 ≈ 0.27`
      - 즉, 합격할 확률이 27%라고 예측합니다.
    - **공부 시간이 4시간일 때:**
      - `H(x) = 1 / (1 + e⁻²) ≈ 1 / (1 + 0.135) ≈ 0.88`
      - 즉, 합격할 확률이 88%라고 예측합니다.

3.  **최종 분류 (결정 경계):**
    - 일반적으로 확률이 0.5 이상이면 클래스 1(합격), 0.5 미만이면 클래스 0(불합격)으로 최종 예측합니다.
    - 따라서, 공부 시간이 2시간인 학생은 '불합격', 4시간인 학생은 '합격'으로 예측됩니다.

**해설:**
이처럼 로지스틱 회귀는 선형 함수의 결과를 시그모이드 함수를 통해 '확률'로 변환하고, 그 확률을 기준으로 특정 임계값(보통 0.5)을 넘는지 여부에 따라 두 개의 클래스 중 하나로 분류하는 작업을 수행합니다. 모델의 학습은 비용 함수(Log Loss)를 최소화하는 최적의 `W`와 `b`를 찾는 과정입니다.
