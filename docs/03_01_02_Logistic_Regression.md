# 3.1.2. 로지스틱 회귀 (Logistic Regression)

**로지스틱 회귀(Logistic Regression)**는 이름에 '회귀'가 포함되어 있지만, 실제로는 **분류(Classification)** 문제를 해결하기 위한 지도 학습 알고리즘입니다. 특히, 두 개의 클래스(예: Yes/No, 0/1, 합격/불합격) 중 하나를 예측하는 **이진 분류(Binary Classification)** 문제에 널리 사용됩니다.

로지스틱 회귀는 선형 회귀의 아이디어를 기반으로 하지만, 출력을 0과 1 사이의 확률 값으로 변환하여 분류를 수행합니다.

## 1. 시그모이드 함수 (Sigmoid Function)

선형 회귀의 출력값 `Wx + b`는 음의 무한대에서 양의 무한대까지 어떤 값이든 가질 수 있습니다. 분류 문제에 이를 직접 적용하기는 어렵습니다. 로지스틱 회귀는 이 문제를 해결하기 위해 **시그모이드 함수(Sigmoid Function)**, 또는 **로지스틱 함수(Logistic Function)**를 사용합니다.

> **g(z) = 1 / (1 + e⁻ᶻ)**

시그모이드 함수는 입력값 `z` (여기서는 선형 회귀의 출력 `Wx + b`)를 0과 1 사이의 값으로 변환하는 역할을 합니다. 이 출력값은 특정 클래스에 속할 **확률**로 해석할 수 있습니다.

- `z`가 매우 큰 양수이면, `e⁻ᶻ`는 0에 가까워져 함수 값은 1에 수렴합니다.
- `z`가 매우 큰 음수이면, `e⁻ᶻ`는 무한대에 가까워져 함수 값은 0에 수렴합니다.
- `z`가 0이면, 함수 값은 0.5가 됩니다.

![Sigmoid Function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/600px-Logistic-curve.svg.png)
*(이미지 출처: 위키미디어)*

## 2. 가설 함수 (Hypothesis Function)

로지스틱 회귀의 가설 함수는 선형 회귀의 결과를 시그모이드 함수에 통과시킨 형태입니다.

> **H(x) = 1 / (1 + e⁻⁽ᵂˣ⁺ᵇ⁾)**

이 가설 함수 `H(x)`의 출력은 "주어진 입력 `x`에 대해 클래스가 1일 확률" (P(y=1|x))을 의미합니다.

- `H(x) > 0.5` 이면, 클래스를 1로 예측합니다.
- `H(x) < 0.5` 이면, 클래스를 0으로 예측합니다.

이때 `H(x) = 0.5`가 되는 지점을 **결정 경계(Decision Boundary)**라고 하며, 두 클래스를 구분하는 기준선(또는 면)이 됩니다.

## 3. 비용 함수 (Cost Function)

선형 회귀에서 사용했던 평균 제곱 오차(MSE)를 로지스틱 회귀의 비용 함수로 사용하면, 비용 함수가 여러 개의 지역 최솟값을 갖는 비볼록(non-convex) 함수 형태가 되어 경사 하강법으로 최적의 해를 찾기 어렵습니다.

따라서 로지스틱 회귀는 **로그 손실(Log Loss)**, 또는 **이진 교차 엔트로피(Binary Cross-Entropy)**라는 새로운 비용 함수를 사용합니다.

> **if y=1:  Cost = -log(H(x))**
> **if y=0:  Cost = -log(1 - H(x))**

- **실제값이 1일 때 (y=1):**
  - 모델이 1로 잘 예측하면 (`H(x) → 1`), `log(H(x))`는 0에 가까워져 비용이 작아집니다.
  - 모델이 0으로 잘못 예측하면 (`H(x) → 0`), `log(H(x))`는 음의 무한대로 발산하여 비용이 매우 커집니다.
- **실제값이 0일 때 (y=0):**
  - 모델이 0으로 잘 예측하면 (`H(x) → 0`), `log(1-H(x))`는 0에 가까워져 비용이 작아집니다.
  - 모델이 1로 잘못 예측하면 (`H(x) → 1`), `log(1-H(x))`는 음의 무한대로 발산하여 비용이 매우 커집니다.

이 두 경우를 하나의 식으로 통합하면 다음과 같습니다.

> **Cost(W, b) = -(1/m) * Σ [ y⁽ⁱ⁾log(H(x⁽ⁱ⁾)) + (1-y⁽ⁱ⁾)log(1-H(x⁽ⁱ⁾)) ]**

이 비용 함수는 볼록(convex) 함수 형태를 띠므로, 경사 하강법을 통해 안정적으로 비용을 최소화하는 최적의 가중치(W, b)를 찾을 수 있습니다.
