# 프라이버시 보호 AI (Privacy-Preserving AI)

AI 모델, 특히 딥러닝 모델을 학습시키기 위해서는 방대한 양의 데이터가 필요합니다. 이 데이터에는 개인의 건강 정보, 금융 정보, 위치 정보 등 매우 민감한 정보가 포함될 수 있습니다. AI 기술이 발전함에 따라, 이러한 개인정보를 안전하게 보호하면서도 데이터의 유용성을 최대한 활용하는 **프라이버시 보호 AI(Privacy-Preserving AI)** 기술의 중요성이 커지고 있습니다.

이는 사용자의 신뢰를 확보하고 GDPR(유럽 일반 개인정보 보호법)과 같은 엄격한 데이터 규제를 준수하기 위해 필수적입니다.

## 1. 주요 프라이버시 위협

- **데이터 유출:** 학습 데이터가 저장된 중앙 서버가 해킹당하여 민감한 정보가 직접적으로 유출될 수 있습니다.
- **추론 공격 (Inference Attack):** 악의적인 사용자가 학습된 AI 모델 자체에 반복적으로 쿼리(query)를 보내거나, 모델의 출력값을 분석하여 학습 데이터에 포함된 특정 개인의 정보를 역으로 추론해내는 공격입니다.
  - **멤버십 추론 공격 (Membership Inference Attack):** 특정인의 데이터가 모델의 학습 데이터셋에 포함되었는지 여부를 알아내는 공격.
  - **모델 역전 공격 (Model Inversion Attack):** 모델의 예측 결과로부터 원본 학습 데이터의 특징이나 민감한 속성을 복원하려는 공격.

## 2. 대표적인 프라이버시 보호 기술

### 2.1. 연합 학습 (Federated Learning, FL)

- **핵심 아이디어:** "데이터를 한 곳에 모으지 말고, 모델을 데이터가 있는 곳으로 보내자." 즉, **개인정보를 중앙 서버로 전송하지 않고, 각자의 로컬 기기(예: 스마트폰, 병원 컴퓨터)에서 모델을 학습**시키는 분산형 머신러닝 방식입니다.
- **작동 방식:**
  1. **모델 배포:** 중앙 서버가 초기 모델 또는 업데이트된 글로벌 모델을 각 클라이언트(사용자 기기)에게 보냅니다.
  2. **로컬 학습:** 각 클라이언트는 자신의 로컬 데이터만을 사용하여 모델을 학습(업데이트)합니다. 이때 데이터는 기기 밖으로 나가지 않습니다.
  3. **모델 업데이트 정보 전송:** 클라이언트는 학습을 통해 변경된 모델의 가중치(weight)나 그래디언트(gradient)와 같은 **업데이트 정보**만을 중앙 서버로 전송합니다. 원본 데이터가 아닌, 모델의 변경 사항만 보내는 것이 핵심입니다.
  4. **모델 취합 및 업데이트:** 중앙 서버는 여러 클라이언트로부터 받은 업데이트 정보들을 안전하게 취합(aggregation)하여 글로벌 모델을 개선합니다. (예: Secure Aggregation)
  5. 이 과정을 반복합니다.
- **장점:**
  - **강력한 프라이버시 보호:** 원본 데이터가 로컬 기기를 떠나지 않으므로 데이터 유출의 위험이 원천적으로 줄어듭니다.
  - **통신 효율성:** 대용량의 원본 데이터 대신, 상대적으로 크기가 작은 모델 업데이트 정보만 전송하므로 통신 비용을 절감할 수 있습니다.
- **한계:** 모델 업데이트 정보 자체도 여전히 개인의 정보를 일부 포함할 수 있어, 차등 정보보호와 같은 다른 기술과 함께 사용될 때 더 강력한 프라이버시를 보장할 수 있습니다.

### 2.2. 차등 정보보호 (Differential Privacy, DP)

- **핵심 아이디어:** 데이터셋에 특정 개인의 데이터가 포함되거나 포함되지 않더라도, 분석 결과(통계, 모델 등)가 거의 동일하게 나오도록 하여 **개인을 식별할 수 없게 만드는** 강력한 수학적 정의의 프라이버시 보호 기술입니다.
- **작동 방식:**
  - 데이터 분석 과정이나 모델의 학습 과정, 또는 결과값에 신중하게 제어된 **무작위 노이즈(random noise)**를 주입합니다.
  - 이 노이즈는 개별 데이터 포인트가 결과에 미치는 영향을 희석시켜, 특정인의 정보가 결과에 미친 영향을 구분할 수 없게 만듭니다.
- **프라이버시 예산 (Privacy Budget, ε):**
  - 주입되는 노이즈의 양을 조절하는 파라미터입니다.
  - `ε` 값이 **작을수록** 더 많은 노이즈가 주입되어 **프라이버시 보호 수준은 높아지지만**, 데이터의 **유용성(정확도)은 낮아집니다.**
  - `ε` 값이 **클수록** 노이즈가 적게 주입되어 **데이터 유용성은 높아지지만**, **프라이버시 보호 수준은 낮아집니다.**
  - 이 둘 사이의 트레이드오프(trade-off) 관계를 고려하여 적절한 `ε` 값을 설정하는 것이 중요합니다.
- **적용:**
  - **DP-SGD (Differentially Private Stochastic Gradient Descent):** 딥러닝 모델의 학습 과정(SGD)에서 그래디언트에 노이즈를 주입하여, 학습된 모델이 차등 정보보호를 만족하도록 합니다.

### 2.3. 동형 암호 (Homomorphic Encryption)

- **핵심 아이디어:** 데이터를 **암호화된 상태 그대로** 연산을 수행할 수 있게 하는 암호화 기법입니다.
- **작동 방식:**
  1. 사용자는 자신의 데이터를 동형 암호화하여 서버로 보냅니다.
  2. 서버는 암호화된 데이터 위에서 직접 모델 학습이나 예측과 같은 연산을 수행합니다. 서버는 데이터의 원본 내용을 전혀 알 수 없습니다.
  3. 서버는 연산이 끝난 암호화된 결과를 사용자에게 다시 보냅니다.
  4. 사용자는 자신의 비밀 키(private key)로 이 결과를 복호화하여 평문(plaintext) 결과를 얻습니다.
- **장점:** 이론적으로 매우 강력한 프라이버시를 보장합니다.
- **단점:** 연산 속도가 매우 느리고 계산 오버헤드가 커서, 아직까지 복잡한 딥러닝 모델에 광범위하게 적용하기에는 실용적인 한계가 있습니다.

이러한 기술들은 AI의 혜택을 누리는 동시에, 정보 주체의 프라이버시 권리를 존중하는 책임감 있는 AI 생태계를 구축하는 데 핵심적인 역할을 합니다.
