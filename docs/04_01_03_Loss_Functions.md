# 손실 함수 (Loss Functions)

## 1. 손실 함수의 역할

**손실 함수(Loss Function)** 또는 **비용 함수(Cost Function)**, **목적 함수(Objective Function)**는 머신러닝 모델의 예측 값과 실제 정답 값 사이의 **차이(오차)**를 측정하는 함수입니다.

- **역할:** 모델이 얼마나 '잘못' 예측했는지를 정량적으로 나타냅니다.
- **목표:** 머신러닝 학습의 목표는 이 손실 함수의 값을 **최소화(Minimize)**하는 것입니다. 손실 함수를 최소화하는 방향으로 모델의 파라미터(가중치와 편향)를 업데이트함으로써 모델의 성능을 향상시킵니다.

손실 함수는 해결하고자 하는 문제의 종류(회귀, 분류 등)에 따라 다르게 선택됩니다.

---

## 2. 회귀 (Regression) 손실 함수

회귀는 연속적인 값을 예측하는 문제입니다.

### 2.1. 평균 제곱 오차 (Mean Squared Error, MSE)

$$ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$

- $y_i$: 실제 정답 값
- $\hat{y}_i$: 모델의 예측 값
- $n$: 데이터 샘플의 수

- **특징:**
  - 오차의 제곱을 사용하므로, 예측 값과 실제 값의 차이가 클수록 페널티를 더 크게 부과합니다.
  - 이로 인해 이상치(Outlier)에 민감하게 반응할 수 있습니다.
  - 제곱을 하기 때문에 미분하기 쉽고, 경사 하강법과 같은 최적화 알고리즘에 사용하기 편리합니다.

### 2.2. 평균 절대 오차 (Mean Absolute Error, MAE)

$$ \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| $$

- **특징:**
  - 오차의 절댓값을 사용하므로, 오차의 크기를 직관적으로 해석하기 쉽습니다.
  - 제곱을 하지 않으므로 이상치에 대해 MSE보다 덜 민감(Robust)합니다.
  - 절댓값 함수는 $x=0$ 지점에서 미분이 불가능하지만, 해당 지점에서는 서브그래디언트(subgradient)를 사용하여 최적화를 계속할 수 있습니다.

---

## 3. 분류 (Classification) 손실 함수

분류는 주어진 입력이 어떤 클래스에 속하는지를 예측하는 문제입니다.

### 3.1. 이진 교차 엔트로피 (Binary Cross-Entropy)

$$ \text{BCE} = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)] $$

- $y_i$: 실제 레이블 (0 또는 1)
- $\hat{y}_i$: 모델이 예측한 클래스 1에 대한 확률 (0과 1 사이의 값, 보통 시그모이드 함수 출력)

- **특징:**
  - **이진 분류(Binary Classification)** 문제(예: 스팸/햄, 합격/불합격)를 위해 특별히 설계된 손실 함수입니다.
  - 모델의 예측 확률이 실제 레이블과 멀어질수록 손실 값이 기하급수적으로 증가합니다.
    - 실제가 1($y_i=1$)일 때: 예측 확률($\hat{y}_i$)이 1에 가까워지면 $-\log(\hat{y}_i)$는 0에 수렴하고, 0에 가까워지면 무한대로 발산합니다.
    - 실제가 0($y_i=0$)일 때: 예측 확률($\hat{y}_i$)이 0에 가까워지면 $-\log(1-\hat{y}_i)$는 0에 수렴하고, 1에 가까워지면 무한대로 발산합니다.

### 3.2. 범주형 교차 엔트로피 (Categorical Cross-Entropy)

$$ \text{CCE} = -\frac{1}{n} \sum_{i=1}^{n} \sum_{j=1}^{C} y_{ij} \log(\hat{y}_{ij}) $$

- $C$: 클래스의 총 개수
- $y_{ij}$: $i$번째 데이터가 클래스 $j$에 속하면 1, 아니면 0 (원-핫 인코딩된 레이블)
- $\hat{y}_{ij}$: 모델이 예측한 $i$번째 데이터가 클래스 $j$에 속할 확률 (소프트맥스 함수 출력)

- **특징:**
  - **다중 클래스 분류(Multi-class Classification)** 문제(예: 숫자 이미지 분류, 품종 분류)에 사용됩니다.
  - 정답 클래스에 해당하는 예측 확률에 대해서만 손실을 계산합니다.
  - 예를 들어, 정답이 `[0, 0, 1]`(세 번째 클래스)이고 예측 확률이 `[0.1, 0.2, 0.7]`이라면, 손실은 $-1 \times \log(0.7)$만으로 계산됩니다. 예측 확률 0.7이 1에 가까울수록 손실은 0에 가까워집니다.

- **참고: Sparse Categorical Cross-Entropy**
  - 원-핫 인코딩된 레이블 대신, 정수 형태의 레이블(예: `0, 1, 2, ...`)을 사용할 때 사용되는 버전입니다. 내부적으로 정수 레이블을 원-핫 인코딩처럼 처리하여 계산하므로, 메모리 효율성이 좋습니다. 기능적으로는 CCE와 동일합니다.
