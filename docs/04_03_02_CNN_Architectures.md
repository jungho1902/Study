# 주요 CNN 아키텍처

컴퓨터 비전 분야의 발전에 따라 다양한 합성곱 신경망(CNN) 아키텍처가 제안되었습니다. 이들은 이미지넷 대규모 시각 인식 챌린지(ILSVRC)와 같은 대회에서 높은 성능을 보이며 딥러닝의 발전을 이끌었습니다.

---

## 1. LeNet-5
- **저자:** 얀 르쿤 (Yann LeCun) 등 (1998)
- **특징:**
  - 현대적인 CNN 구조의 시초로 여겨지는 모델입니다.
  - **[합성곱 -> 풀링 -> 합성곱 -> 풀링 -> 완전 연결 레이어]** 의 기본적인 구조를 가지고 있습니다.
  - 주로 손글씨 숫자 인식(MNIST 데이터셋)과 같은 간단한 이미지 분류 문제에 사용되었습니다.
  - 활성화 함수로 Sigmoid나 Tanh를 사용했습니다.

---

## 2. AlexNet
- **저자:** 알렉스 크리제브스키 (Alex Krizhevsky) 등 (2012)
- **특징:**
  - 2012년 ILSVRC에서 우승하며 딥러닝의 부흥을 이끈 모델입니다.
  - LeNet-5보다 훨씬 더 깊고 넓은 구조(8개 층)를 가집니다.
  - **주요 기여:**
    - **ReLU 활성화 함수 사용:** Sigmoid나 Tanh 대신 ReLU를 사용하여 학습 속도를 크게 향상시키고 기울기 소실 문제를 완화했습니다.
    - **드롭아웃 (Dropout) 사용:** 완전 연결 레이어에 드롭아웃을 적용하여 과대적합을 효과적으로 방지했습니다.
    - **GPU 활용:** 2개의 GPU를 병렬로 사용하여 대규모 연산을 효율적으로 처리했습니다.

---

## 3. VGGNet
- **저자:** 옥스포드 대학 VGG 그룹 (2014)
- **특징:**
  - **3x3 크기의 작은 필터**만을 사용하여 신경망의 깊이를 크게 늘렸습니다 (16개 또는 19개 층).
  - 더 작은 필터를 여러 겹 쌓는 것이 더 큰 필터를 한 번 사용하는 것보다 효과적인 이유:
    1.  **파라미터 수 감소:** 5x5 필터 하나보다 3x3 필터 두 개를 쌓는 것이 파라미터 수가 더 적습니다.
    2.  **비선형성 증가:** 필터 사이에 활성화 함수(ReLU)를 더 많이 사용할 수 있어 모델의 표현력이 향상됩니다.
  - 구조가 매우 간단하고 직관적이어서 이해하고 구현하기 쉽습니다.
  - 단점은 깊이가 깊어지면서 파라미터 수가 너무 많아지고 메모리 사용량이 크다는 것입니다.

---

## 4. GoogLeNet (Inception)
- **저자:** 구글 (2014)
- **특징:**
  - VGGNet과 같은 해에 ILSVRC에서 더 나은 성능으로 우승했습니다.
  - **인셉션 모듈(Inception Module)**이라는 독창적인 블록을 제안했습니다.
    - 하나의 입력에 대해 **1x1, 3x3, 5x5 합성곱과 3x3 최대 풀링을 병렬적으로 적용**한 후, 그 결과를 모두 합칩니다.
    - 이를 통해 모델이 다양한 크기의 특징을 동시에 학습할 수 있습니다.
  - **1x1 합성곱 활용:**
    - 채널(channel) 수를 줄여 **차원을 축소**함으로써, 연산량을 크게 줄이고 파라미터 수를 감소시킵니다.
    - VGGNet보다 훨씬 깊은 22개 층을 가지면서도 파라미터 수는 AlexNet보다도 적습니다.

---

## 5. ResNet (Residual Networks)
- **저자:** 마이크로소프트 (2015)
- **특징:**
  - 2015년 ILSVRC에서 압도적인 성능으로 우승하며, "신경망은 깊을수록 좋은가?"라는 질문에 중요한 해답을 제시했습니다.
  - **잔차 학습(Residual Learning)**과 **스킵 연결(Skip Connection)**이라는 개념을 도입했습니다.
    - 스킵 연결은 몇 개의 층을 건너뛰어 입력을 출력에 바로 더해주는 구조입니다.
    - 신경망이 변화량(잔차, residual)만을 학습하도록 유도하여, 층이 매우 깊어져도(예: 152개 층) 기울기 소실 문제 없이 학습이 잘 되도록 합니다.
    - `H(x) = F(x) + x` 형태. 여기서 `F(x)`가 학습할 잔차.
  - 이 구조 덕분에 이전 모델들보다 훨씬 더 깊은 신경망을 안정적으로 학습시킬 수 있게 되었고, 많은 최신 CNN 아키텍처의 기반이 되었습니다.

---

## 6. DenseNet (Densely Connected Convolutional Networks)
- **저자:** 코넬 대학 등 (2017)
- **특징:**
  - ResNet의 스킵 연결을 극대화한 아이디어에서 출발했습니다.
  - 각 층이 그 이전의 **모든 층**으로부터 입력을 받습니다. 즉, 각 층의 특징 맵이 그 이후의 모든 층에 연결됩니다 (채널 방향으로 합쳐짐).
  - **장점:**
    - **특징 재사용(Feature Reuse) 극대화:** 이전 층의 정보가 손실 없이 뒤쪽으로 전달됩니다.
    - **기울기 소실 문제 완화:** 그래디언트가 각 층에 직접적으로 전달되어 학습이 효율적입니다.
    - **적은 파라미터:** ResNet보다 적은 파라미터로 더 좋은 성능을 보이는 경우가 많습니다.
  - **단점:**
    - 특징 맵을 계속 채널 방향으로 쌓기 때문에 메모리 사용량이 매우 클 수 있습니다.
