# 주요 GNN 아키텍처

메시지 전달(Message Passing)이라는 기본 원리를 바탕으로, 이웃의 정보를 어떻게 집계(aggregate)하고 업데이트(update)할 것인지에 따라 다양한 그래프 신경망(GNN) 아키텍처가 제안되었습니다.

---

## 1. GCN (Graph Convolutional Network)

- **개념:** GCN은 이미지 처리에서 사용되는 CNN(합성곱)의 개념을 그래프 데이터에 적용한 모델입니다. 스펙트럼 기반(spectral-based) 접근법에서 시작되었지만, 메시지 전달 관점에서 더 간단하게 이해할 수 있습니다.
- **핵심 아이디어:** 이웃 노드들과 자기 자신의 특징(feature)을 **평균**내어 새로운 표현을 만듭니다.
- **동작 방식 (메시지 전달 관점):**
  1.  **메시지 집계:** 각 노드는 모든 이웃 노드들의 특징 벡터를 단순 합산(Sum) 또는 평균(Mean)하여 집계합니다.
  2.  **업데이트:** 집계된 이웃의 정보와 자기 자신의 정보를 합치고, 학습 가능한 가중치 행렬을 곱한 뒤 비선형 활성화 함수를 적용하여 노드 표현을 업데이트합니다.
- **수식 (단순화된 형태):**
  $$ H^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)}) $$
  - $\tilde{A} = A + I$: 인접 행렬 A에 자기 자신과의 연결(self-loop)을 추가.
  - $\tilde{D}$: $\tilde{A}$의 차수 행렬(degree matrix).
  - $H^{(l)}$: l번째 레이어의 노드 특징 행렬.
  - $W^{(l)}$: l번째 레이어의 학습 가능한 가중치 행렬.
  - $\sigma$: 비선형 활성화 함수 (예: ReLU).
- **특징:**
  - 구조가 간단하고 계산이 효율적입니다.
  - 대부분의 GNN 연구에서 성능 비교를 위한 강력한 베이스라인 모델로 사용됩니다.
  - **Transductive Learning:** 학습 과정에서 전체 그래프의 구조(인접 행렬)를 알아야 하므로, 학습 중에 보지 못한 새로운 노드나 그래프에 적용하기 어렵다는 한계가 있습니다.

---

## 2. GraphSAGE (Graph SAmple and aggreGatE)

- **개념:** GCN의 한계를 극복하고, 대규모 그래프와 **새로운 노드에 대한 예측(Inductive Learning)**이 가능하도록 설계된 모델입니다.
- **핵심 아이디어:** 모든 이웃을 사용하는 대신, 각 노드마다 **고정된 수의 이웃을 무작위로 샘플링**하여 정보를 집계합니다.
- **동작 방식:**
  1.  **샘플링:** 각 노드에 대해 정해진 수(K)의 이웃을 무작위로 샘플링합니다.
  2.  **집계:** 샘플링된 이웃들의 특징 벡터를 다양한 방법으로 집계합니다. GCN과 달리, 더 복잡한 집계 함수를 사용할 수 있습니다.
      - **Mean Aggregator:** 이웃들의 특징 벡터를 평균냅니다 (GCN과 유사).
      - **LSTM Aggregator:** 이웃들을 특정 순서로 정렬하여 LSTM 모델의 입력으로 사용합니다.
      - **Pooling Aggregator:** 각 이웃의 특징 벡터에 신경망을 적용한 후, Max-pooling 또는 Mean-pooling을 수행합니다.
  3.  **업데이트:** 집계된 이웃 정보와 자기 자신의 정보를 결합하여 노드 표현을 업데이트합니다.
- **특징:**
  - **Inductive Learning:** 학습 중에 보지 못한 노드나 그래프가 들어와도, 그 노드의 이웃을 샘플링하고 집계하는 방식으로 임베딩을 생성할 수 있습니다.
  - **확장성(Scalability):** 전체 그래프가 아닌, 샘플링된 일부 이웃만 보기 때문에 대규모 그래프에 적용하기 용이합니다.

---

## 3. GAT (Graph Attention Network)

- **개념:** 트랜스포머의 어텐션 메커니즘을 그래프에 적용한 모델입니다.
- **핵심 아이디어:** 모든 이웃을 동일한 가중치로 평균내는 GCN과 달리, **이웃 노드마다 다른 중요도(가중치)**를 학습하여 중요한 이웃의 정보에 더 '집중(attention)'합니다.
- **동작 방식:**
  1.  **어텐션 계수(Attention Coefficient) 계산:** 각 노드는 자신의 모든 이웃에 대해 어텐션 점수를 계산합니다. 이 점수는 두 노드 특징 벡터를 입력으로 하는 작은 신경망을 통해 계산되며, 두 노드 간의 관계가 얼마나 중요한지를 나타냅니다.
  2.  **어텐션 가중치(Attention Weight) 계산:** 계산된 어텐션 점수들을 **소프트맥스(Softmax) 함수**를 통해 정규화하여, 총합이 1인 어텐션 가중치를 얻습니다.
  3.  **업데이트:** 이 어텐션 가중치를 각 이웃 노드의 특징 벡터에 곱하여 가중합을 구하고, 이를 통해 노드 표현을 업데이트합니다.
  4.  **멀티-헤드 어텐션(Multi-Head Attention):** 트랜스포머와 마찬가지로, 여러 개의 독립적인 어텐션 메커니즘(헤드)을 병렬로 사용하여, 다양한 관점의 관계를 학습하고 모델의 표현력을 높입니다.
- **특징:**
  - **중요한 이웃에 집중:** 모델이 데이터로부터 노드 간의 관계 중요도를 직접 학습하므로, 더 정교한 정보 집계가 가능합니다.
  - **Inductive Learning 가능:** 어텐션 가중치는 노드 쌍에 대해서만 계산되므로, 새로운 노드가 들어와도 적용할 수 있습니다.
  - GCN, GraphSAGE 등과 비교하여 많은 벤치마크에서 높은 성능을 보입니다.
