# 5.1.2. 정책 (Policy)

**정책(Policy)**은 강화학습에서 에이전트가 특정 상태(State)에서 어떤 행동(Action)을 선택할지 결정하는 전략 또는 규칙입니다. 정책은 에이전트의 행동 방식을 완전히 정의하며, 강화학습의 최종 목표는 누적 보상을 최대화하는 **최적의 정책(Optimal Policy)**을 찾는 것입니다.

정책은 에이전트의 "두뇌" 역할을 하며, 현재 상태를 관찰하여 다음에 취할 행동을 결정합니다. 예를 들어, 체스 게임에서 현재 보드 상태를 보고 다음 수를 결정하는 것이나, 자율주행차가 현재 도로 상황을 파악하고 가속, 브레이크, 조향 등의 행동을 결정하는 것이 정책의 예입니다.

## 1. 정책의 기본 개념

### 1.1. 수학적 표기

정책은 일반적으로 **π(pi)**로 표기됩니다. 정책의 수학적 정의는 다음과 같습니다:

- **π : S → A** (결정론적 정책의 경우)
- **π : S × A → [0, 1]** (확률적 정책의 경우)

여기서 S는 상태 공간(State Space), A는 행동 공간(Action Space)을 의미합니다.

### 1.2. 정책의 종류

#### 1.2.1. 결정론적 정책 (Deterministic Policy)

결정론적 정책은 특정 상태에서 **항상 동일한 행동**을 선택하는 정책입니다.

> **π(s) = a**

- **특징:**
  - 주어진 상태 s에 대해 단일 행동 a를 반환
  - 예측 가능하고 일관된 행동 패턴
  - 계산이 단순하고 빠름

- **예시:** 미로 탈출 게임에서 각 위치마다 정해진 방향(위, 아래, 왼쪽, 오른쪽)으로만 이동하는 정책

#### 1.2.2. 확률적 정책 (Stochastic Policy)

확률적 정책은 특정 상태에서 **가능한 행동들의 확률 분포**를 반환하는 정책입니다.

> **π(a|s) = P[A_t = a | S_t = s]**

- **특징:**
  - 주어진 상태 s에서 행동 a를 선택할 확률을 나타냄
  - 모든 가능한 행동에 대한 확률의 합은 1: ∑_a π(a|s) = 1
  - 더 유연하고 탐험적인 행동 가능

- **예시:** 포커 게임에서 같은 패를 가져도 때로는 베팅하고, 때로는 폴드하는 혼합 전략

## 2. 정책의 중요성과 역할

### 2.1. 강화학습에서의 위치

정책은 강화학습의 핵심 구성 요소 중 하나로, 다음과 같은 역할을 합니다:

1. **행동 선택:** 현재 상태에서 어떤 행동을 취할지 결정
2. **학습 방향:** 보상을 최대화하는 방향으로 정책을 개선
3. **탐험과 활용의 균형:** 새로운 전략 탐색과 현재 최선 전략 활용 사이의 균형

### 2.2. 최적 정책 (Optimal Policy)

**최적 정책 π***는 모든 상태에서 기대되는 누적 보상을 최대화하는 정책입니다.

> **π* = argmax_π V^π(s)** for all s ∈ S

여기서 V^π(s)는 정책 π를 따를 때 상태 s에서의 가치 함수입니다.

**최적 정책의 특징:**
- 모든 상태에서 최대 가치를 제공
- 유일하지 않을 수 있음 (여러 최적 정책 존재 가능)
- 벨만 최적성 방정식(Bellman Optimality Equation)을 만족

## 3. 정책 평가와 개선

### 3.1. 정책 평가 (Policy Evaluation)

주어진 정책 π에 대해 각 상태의 가치를 계산하는 과정입니다.

> **V^π(s) = E_π[G_t | S_t = s]**

여기서 G_t는 시점 t부터의 누적 할인 보상입니다.

### 3.2. 정책 개선 (Policy Improvement)

현재 정책보다 더 좋은 정책을 찾는 과정입니다.

> **π'(s) = argmax_a Q^π(s, a)**

이 과정을 통해 정책을 점진적으로 개선하여 최적 정책에 도달할 수 있습니다.

## 4. 정책 기반 방법들

### 4.1. 정책 반복 (Policy Iteration)

1. **정책 평가:** 현재 정책의 가치 함수 계산
2. **정책 개선:** 가치 함수를 기반으로 정책 개선
3. 수렴할 때까지 반복

### 4.2. 가치 반복 (Value Iteration)

정책 평가와 개선을 동시에 수행하여 최적 가치 함수와 최적 정책을 찾는 방법입니다.

### 4.3. 정책 경사 방법 (Policy Gradient Methods)

정책을 직접 매개변수화하고, 경사 상승법을 사용하여 정책을 개선하는 방법입니다.

---

## 예제 및 풀이 (Examples and Solutions)

### 예제 1: 간단한 격자 세계에서의 정책 설계

**문제:** 다음과 같은 3×3 격자 세계에서 에이전트가 시작점 S에서 목표점 G에 도달하는 최적의 결정론적 정책을 설계하시오. 각 칸에서 상, 하, 좌, 우로 이동할 수 있으며, 목표에 도달하면 +10의 보상, 각 이동마다 -1의 비용이 발생한다.

```
[S] [ ] [G]
[ ] [X] [ ]
[ ] [ ] [ ]
```

여기서 X는 장애물이고, S는 시작점, G는 목표점입니다.

**풀이:**

각 상태(위치)에 대해 최적의 행동을 결정해야 합니다. 목표는 최단 경로로 G에 도달하는 것입니다.

1. **상태 정의:** 각 격자 위치를 (행, 열)로 표현
   - S = (0,0), G = (0,2), X = (1,1)

2. **최적 경로 분석:** S에서 G까지의 최단 경로들:
   - 경로 1: S → (0,1) → G (2 이동, 총 보상: 10 - 2 = 8)
   - 경로 2: S → (1,0) → (2,0) → (2,1) → (2,2) → (1,2) → G (6 이동, 총 보상: 10 - 6 = 4)

3. **최적 정책 π*:**
   ```
   π*(0,0) = 우 (오른쪽)
   π*(0,1) = 우 (오른쪽)
   π*(0,2) = 목표 달성
   π*(1,0) = 우 (경로2 선택시)
   π*(1,2) = 상 (위)
   π*(2,0) = 상 (위)
   π*(2,1) = 상 (위)
   π*(2,2) = 상 (위)
   ```

**답:** 최적 정책은 상태 (0,0)에서 '우', 상태 (0,1)에서 '우'로 이동하는 것으로, 총 2번의 이동으로 목표에 도달할 수 있습니다.

### 예제 2: 확률적 정책의 기댓값 계산

**문제:** 어떤 상태 s에서 2개의 행동 A₁, A₂가 가능하고, 확률적 정책 π가 다음과 같이 정의되어 있다:
- π(A₁|s) = 0.7
- π(A₂|s) = 0.3

각 행동의 즉시 보상이 다음과 같을 때, 이 상태에서의 기대 보상을 계산하시오:
- R(s, A₁) = 5
- R(s, A₂) = 12

**풀이:**

확률적 정책에서의 기대 보상은 각 행동의 보상에 해당 행동을 선택할 확률을 곱한 값들의 합입니다.

> **E[R] = ∑_a π(a|s) × R(s, a)**

계산:
- A₁에 대한 기대 보상: 0.7 × 5 = 3.5
- A₂에 대한 기대 보상: 0.3 × 12 = 3.6

**답:** 총 기대 보상 = 3.5 + 3.6 = **7.1**

**해설:** 비록 A₂의 보상이 더 높지만(12 > 5), A₁을 선택할 확률이 더 높기 때문에 전체적인 기대 보상에서는 두 행동이 비슷한 기여를 합니다. 이는 확률적 정책이 탐험(exploration)과 활용(exploitation) 사이의 균형을 유지하는 방식을 보여줍니다.
