# AI의 편향성과 공정성 (Bias and Fairness in AI)

**책임감 있는 AI(Responsible AI)**의 핵심 요소인 **편향성(Bias)**과 **공정성(Fairness)**은 AI 시스템이 사회에 미치는 영향을 고려할 때 매우 중요한 문제입니다. AI 모델이 특정 그룹에 대해 불공평하거나 차별적인 예측을 내놓을 때, 이는 심각한 사회적, 윤리적 문제를 야기할 수 있습니다.

## 1. AI 편향성이란 무엇인가?

**AI 편향성**은 AI 모델이 학습한 데이터에 내재된 편견이나 불균형, 또는 잘못된 알고리즘 설계로 인해, 특정 인구 집단에 대해 체계적으로 불리하거나 부정확한 결과를 생성하는 경향을 의미합니다. 이는 모델이 현실 세계의 복잡성을 제대로 반영하지 못하고, 특정 패턴을 과도하게 일반화할 때 발생합니다.

### 편향성의 주요 원인

- **데이터 편향 (Data Bias):** 가장 흔한 원인으로, 학습 데이터 자체가 현실 세계의 편견을 반영하거나 특정 그룹을 과소/과대 표현하는 경우입니다.
  - **표본 편향 (Sample Bias):** 특정 집단의 데이터만 과도하게 수집하여 발생하는 편향. (예: 범죄 예측 모델을 특정 지역 데이터로만 학습)
  - **역사적 편향 (Historical Bias):** 데이터가 과거의 사회적 차별(성별, 인종 등)을 그대로 담고 있는 경우. (예: 과거 채용 데이터에서 남성 임원이 많았던 것을 학습하여, AI가 남성을 더 유능하다고 판단)
  - **선택 편향 (Selection Bias):** 데이터 수집 과정에서 특정 그룹이 누락되거나 잘못 선택되어 발생.
- **알고리즘 편향 (Algorithmic Bias):** 모델의 설계나 목적 함수(objective function)가 특정 결과를 선호하도록 만들어졌을 때 발생합니다. 예를 들어, 예측 정확도(accuracy)만을 최적화하도록 설계된 모델은 소수 집단에 대한 예측 오류를 무시하는 경향이 있을 수 있습니다.
- **사람의 편향 (Human Bias):** 모델을 개발하고 평가하는 과정에서 연구자나 개발자의 주관적인 판단이나 무의식적인 편견이 개입될 때 발생합니다.

## 2. 공정성이란 무엇인가?

**AI 공정성**은 AI 시스템의 예측이나 결정이 개인의 민감한 특성(sensitive attributes)인 성별, 인종, 나이, 종교 등에 의해 부당하게 영향을 받지 않아야 한다는 원칙입니다. 즉, 모델의 혜택과 피해가 모든 인구 집단에 공평하게 분배되어야 함을 의미합니다.

하지만 '공정성'은 모든 상황에 적용할 수 있는 단일한 정의가 존재하지 않으며, 상황과 맥락에 따라 다양한 수학적 정의가 존재합니다.

### 공정성의 주요 지표 (Metrics)

공정성 지표는 크게 **그룹 공정성**과 **개인 공정성**으로 나뉩니다.

#### A. 그룹 공정성 (Group Fairness)

- 다른 그룹 간에 모델의 성능이나 결과가 통계적으로 유사해야 한다는 개념입니다.
- **인구 통계학적 동등성 (Demographic Parity / Statistical Parity):**
  - 모델의 예측 결과(긍정적 예측 비율)가 모든 그룹에서 동일해야 합니다.
  - `P(Ŷ=1 | A=0) = P(Ŷ=1 | A=1)` (A는 민감 특성, Ŷ는 예측 결과)
  - 예: 남성 지원자와 여성 지원자의 대출 승인율이 같아야 한다.
  - 단점: 실제 그룹 간의 기본 비율(base rate)이 다른 경우, 오히려 불공정한 결과를 초래할 수 있습니다.
- **기회 균등 (Equal Opportunity):**
  - 실제 결과가 긍정(Positive)인 그룹 내에서, 모델이 긍정으로 예측할 확률(재현율, Recall)이 모든 그룹에서 동일해야 합니다.
  - `P(Ŷ=1 | Y=1, A=0) = P(Ŷ=1 | Y=1, A=1)` (Y는 실제 결과)
  - 예: 실제로 대출 상환 능력이 있는 사람들 중에서, 남성과 여성의 대출 승인율이 같아야 한다. '자격이 있는 사람에게 동등한 기회를 주자'는 관점입니다.
- **예측 확률 균등 (Equalized Odds):**
  - 기회 균등을 더 확장한 개념으로, 실제 결과가 긍정일 때와 부정일 때 모두에서 재현율(TPR)과 특이도(TNR)가 그룹 간에 동일해야 합니다.

#### B. 개인 공정성 (Individual Fairness)

- **"비슷한 개인은 비슷하게 대우받아야 한다"**는 원칙에 기반합니다.
- 개인 수준에서 공정성을 측정하기 때문에 그룹 공정성보다 더 엄격한 기준입니다.
- `d(x, z)`가 두 개인 `x`와 `z`의 유사도를 나타낼 때, 모델의 예측 `f(x)`와 `f(z)`도 유사해야 함을 의미합니다.

## 3. 편향성 완화 기법

AI 모델의 편향성을 줄이고 공정성을 높이기 위한 기술적인 접근법은 데이터 처리, 모델 학습, 후처리 단계에서 적용될 수 있습니다.

- **전처리 기법 (Pre-processing):**
  - 모델을 학습시키기 **전**에 데이터 자체를 수정하여 편향을 제거합니다.
  - **리샘플링 (Resampling):** 소수 집단 데이터를 오버샘플링(oversampling)하거나 다수 집단 데이터를 언더샘플링(undersampling)하여 데이터 불균형을 해소합니다.
  - **리와잉 (Reweighing):** 각 데이터 포인트에 가중치를 부여하여, 소수 집단 데이터가 학습에 더 큰 영향을 미치도록 조정합니다.
- **학습 중 처리 기법 (In-processing):**
  - 모델을 학습하는 **과정**에서 공정성 제약 조건을 추가합니다.
  - 모델의 손실 함수(loss function)에 정확도 항과 함께 공정성 지표(예: Demographic Parity)를 위반할 경우 페널티를 부과하는 항을 추가하여, 두 목표를 동시에 최적화합니다.
- **후처리 기법 (Post-processing):**
  - 이미 학습된 모델의 **예측 결과**를 수정하여 공정성을 보정합니다.
  - 예를 들어, 특정 그룹에 대해 더 관대한 예측 임계값(threshold)을 적용하여 그룹 간의 긍정 예측 비율을 맞출 수 있습니다.

공정성은 단순히 기술적인 문제 해결을 넘어, 사회적 합의와 윤리적 고려가 필요한 복합적인 문제입니다. 따라서 어떤 공정성 지표를 선택하고 적용할지는 해당 AI 서비스의 목적과 사회적 맥락을 충분히 고려하여 신중하게 결정해야 합니다.
