# 라그랑주 승수법 (Lagrange Multipliers)

**라그랑주 승수법(Lagrange Multipliers)**은 **제약 조건(constraints)이 있는 최적화 문제**를 푸는 강력한 방법입니다. 즉, 특정 조건을 만족시키면서 어떤 함수의 최댓값 또는 최솟값을 찾고자 할 때 사용됩니다.

예를 들어, "주어진 예산(제약 조건) 하에서 최대의 만족도(목적 함수)를 얻는 방법"과 같은 문제를 푸는 데 사용될 수 있습니다.

---

### 1. 문제 설정

라그랑주 승수법은 다음과 같은 형태의 문제를 해결합니다.

- **목적 함수:** $`f(x, y)`$ -> 이 함수의 최댓값 또는 최솟값을 찾고 싶다.
- **제약 조건:** $`g(x, y) = c`$ -> 단, 변수 $`x, y`$는 이 조건을 반드시 만족해야 한다.

### 2. 핵심 아이디어 (기하학적 해석)

최적화 문제의 해(최댓값 또는 최솟값이 되는 점)는 목적 함수 $`f(x, y)`$의 등고선(contour line)과 제약 조건 함수 $`g(x, y) = c`$의 그래프가 **서로 접하는(tangent)** 지점에서 발생합니다.

- **등고선:** 함수 값이 같은 지점들을 연결한 선.
- **그래디언트($`\nabla`$):** 등고선에 수직이며, 함수 값이 가장 가파르게 증가하는 방향을 가리키는 벡터.

두 곡선이 한 점에서 서로 접한다는 것은, 그 지점에서 두 곡선의 법선 벡터(normal vector)가 서로 평행하다는 의미입니다. 함수의 그래디언트는 등고선의 법선 벡터이므로, 결국 **두 함수의 그래디언트 벡터가 서로 평행**하다는 결론에 이릅니다.

두 벡터가 평행하다는 것은, 한 벡터가 다른 벡터의 스칼라배와 같다는 의미입니다. 이를 수식으로 표현하면 다음과 같습니다.

$`\nabla f(x, y) = \lambda \nabla g(x, y)`$

- $`\nabla f`$: 목적 함수 $`f`$의 그래디언트.
- $`\nabla g`$: 제약 조건 함수 $`g`$의 그래디언트.
- $`\lambda`$ (람다): **라그랑주 승수(Lagrange Multiplier)**라고 불리는 미지의 스칼라 상수.

### 3. 라그랑주 함수 (Lagrangian Function)

위의 아이디어를 실제 계산에 적용하기 위해, 다음과 같은 **라그랑주 함수** $`\mathcal{L}`$를 정의합니다.

$`\mathcal{L}(x, y, \lambda) = f(x, y) - \lambda(g(x, y) - c)`$

이 새로운 함수 $`\mathcal{L}`$의 모든 편미분 값이 0이 되는 지점을 찾으면, 원래 문제의 최적해 후보를 찾을 수 있습니다.

$`\nabla \mathcal{L}(x, y, \lambda) = 0`$

이를 각 변수에 대해 풀어쓰면 다음과 같은 연립방정식이 됩니다.
1.  $`\frac{\partial \mathcal{L}}{\partial x} = \frac{\partial f}{\partial x} - \lambda \frac{\partial g}{\partial x} = 0`$
2.  $`\frac{\partial \mathcal{L}}{\partial y} = \frac{\partial f}{\partial y} - \lambda \frac{\partial g}{\partial y} = 0`$
3.  $`\frac{\partial \mathcal{L}}{\partial \lambda} = -(g(x, y) - c) = 0 \implies g(x, y) = c`$

- 1번과 2번 식을 정리하면 $`\nabla f = \lambda \nabla g`$가 되어 기하학적 아이디어와 일치합니다.
- 3번 식은 원래의 제약 조건 그 자체입니다.

이 세 개의 연립방정식을 풀어 변수 $`x, y, \lambda`$를 구하면, 그 $`(x, y)`$가 바로 목적 함수 $`f`$의 최적해 후보가 됩니다.

### 4. 머신러닝에서의 응용

라그랑주 승수법은 머신러닝에서 제약 조건이 있는 최적화 문제를 푸는 데 이론적 기반을 제공합니다. 대표적인 예로는 **서포트 벡터 머신(SVM)**이 있습니다. SVM은 마진(margin)을 최대로 하는 결정 경계(decision boundary)를 찾는 동시에, 모든 데이터 포인트가 올바르게 분류되어야 한다는 제약 조건을 만족해야 합니다. 이 문제를 풀기 위해 라그랑주 승수법의 일반화된 형태인 **KKT(Karush-Kuhn-Tucker) 조건**이 사용됩니다.
