# 딥러닝 기반 시계열 모델 (Deep Learning-based Time Series Models)

전통적인 통계 기반 시계열 모델(ARIMA 등)은 데이터가 정상성을 가지거나 선형적인 관계를 보일 때 잘 작동하지만, 복잡한 비선형 패턴, 여러 변수 간의 상호작용, 장기적인 의존성을 학습하는 데에는 한계가 있습니다. **딥러닝 기반 시계열 모델**은 이러한 한계를 극복하기 위해 개발되었으며, 대규모 데이터셋에서 뛰어난 예측 성능을 보입니다.

## 1. 순환 신경망 (Recurrent Neural Networks, RNN)

**RNN**은 시계열 데이터와 같이 순서(sequence)가 있는 데이터를 처리하기 위해 고안된 신경망입니다.

- **핵심 구조:** RNN은 내부에 **순환하는 루프(loop)**를 가지고 있어, 이전 타임스텝(time step)의 정보가 현재 타임스텝의 계산에 영향을 미칩니다. 즉, 은닉 상태(hidden state)라는 형태로 과거의 정보를 기억하고, 이를 다음 단계로 계속 전달합니다.
- **작동 방식:** 각 타임스텝에서 모델은 현재의 입력과 이전 타임스텝의 은닉 상태를 함께 받아 새로운 은닉 상태를 만들고 예측값을 출력합니다.
- **한계:**
  - **기울기 소실/폭주 문제 (Vanishing/Exploding Gradient Problem):** 시퀀스의 길이가 길어질수록, 역전파 과정에서 기울기가 점차 사라지거나(vanishing) 기하급수적으로 커져(exploding) 장기적인 의존성(long-term dependency)을 학습하기 어렵습니다. 예를 들어, 문장의 맨 앞에 나온 단어가 맨 뒤에 나올 단어에 미치는 영향을 학습하기 힘듭니다.

## 2. LSTM (Long Short-Term Memory)

**LSTM**은 RNN의 장기 의존성 문제를 해결하기 위해 등장한 발전된 형태의 순환 신경망입니다.

- **핵심 구조:** RNN의 단순한 은닉 상태 구조와 달리, LSTM은 **셀 상태(Cell State)**와 여러 개의 **게이트(Gate)**를 도입했습니다.
  - **셀 상태 (Cell State):** 컨베이어 벨트처럼 시퀀스 전체를 관통하며 중요한 정보를 장기간 기억하는 역할을 합니다.
  - **게이트 (Gate):** 시그모이드(sigmoid) 함수를 사용하여 0과 1 사이의 값을 출력하는 작은 신경망으로, 셀 상태의 정보를 얼마나 통과시킬지를 제어합니다.
    - **망각 게이트 (Forget Gate):** 과거의 정보 중 어떤 것을 잊어버릴지 결정합니다.
    - **입력 게이트 (Input Gate):** 현재의 정보 중 어떤 것을 셀 상태에 저장할지 결정합니다.
    - **출력 게이트 (Output Gate):** 셀 상태의 정보 중 어떤 것을 현재의 은닉 상태(출력)로 내보낼지 결정합니다.
- **장점:** 이러한 게이트 구조 덕분에 불필요한 정보는 잊고 중요한 정보는 오랫동안 기억할 수 있어, RNN보다 훨씬 긴 시퀀스의 의존성을 효과적으로 학습할 수 있습니다.

## 3. GRU (Gated Recurrent Unit)

**GRU**는 2014년에 제안된 모델로, LSTM의 복잡한 구조를 간소화하면서 비슷한 성능을 내도록 설계되었습니다.

- **핵심 구조:** LSTM의 셀 상태와 은닉 상태를 하나의 은닉 상태로 통합하고, 게이트의 수도 2개로 줄였습니다.
  - **리셋 게이트 (Reset Gate):** 과거의 정보를 얼마나 무시할지 결정합니다.
  - **업데이트 게이트 (Update Gate):** LSTM의 망각 게이트와 입력 게이트의 역할을 합친 것으로, 과거의 정보를 얼마나 유지하고, 현재의 새로운 정보를 얼마나 반영할지를 결정합니다.
- **장점:** LSTM보다 파라미터 수가 적어 계산 효율성이 높고, 데이터가 적은 경우 과대적합의 위험이 적을 수 있습니다. 많은 경우 LSTM과 비슷한 성능을 보여주기 때문에 널리 사용됩니다.

## 4. 딥러닝 모델의 응용

- **다변량 시계열 예측 (Multivariate Time Series Forecasting):**
  - 주가 예측 시, 과거 주가 데이터뿐만 아니라 거래량, 금리, 뉴스 기사 등 여러 관련 변수(feature)들을 함께 입력으로 사용하여 더 정확한 예측을 수행할 수 있습니다.
- **장기 예측 (Long-term Forecasting):**
  - **Seq2Seq (Sequence-to-Sequence) 모델:** 인코더(Encoder)가 입력 시퀀스(과거 데이터)를 하나의 문맥 벡터(context vector)로 압축하고, 디코더(Decoder)가 이 벡터를 바탕으로 출력 시퀀스(미래 예측값)를 생성하는 구조입니다.
  - **어텐션 메커니즘 (Attention Mechanism):** 디코더가 예측을 수행할 때, 인코더의 모든 타임스텝 정보 중에서 현재 예측에 가장 중요한 부분에 더 집중(attention)하도록 하여 장기 예측의 정확도를 높입니다.
- **트랜스포머 (Transformer) 기반 모델:**
  - 최근에는 RNN 계열의 순차적인 처리 방식에서 벗어나, 어텐션 메커니즘만으로 시계열 데이터 내의 복잡한 관계를 모델링하는 트랜스포머 기반의 아키텍처(예: Informer, Autoformer)도 시계열 예측 분야에서 높은 성능을 보이고 있습니다. 이들은 특히 매우 긴 시퀀스의 의존성을 파악하는 데 강점을 가집니다.
