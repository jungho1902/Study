# 활성화 함수(Activation Function)의 미분

활성화 함수의 도함수는 역전파(Backpropagation) 과정에서 그래디언트를 계산하는 데 필수적입니다. 각 뉴런의 출력에 대한 손실(error)의 그래디언트를 계산할 때, 활성화 함수의 미분값이 연쇄 법칙(chain rule)에 따라 곱해지기 때문입니다.

---

### 1. 시그모이드 함수 (Sigmoid Function)

시그모이드 함수는 출력을 0과 1 사이로 압축하여 확률처럼 해석할 수 있게 만들어, 초창기 신경망에서 널리 사용되었습니다.

- **함수:** $`\sigma(x) = \frac{1}{1 + e^{-x}}`$

- **미분:** 시그모이드 함수의 도함수는 자기 자신을 이용하여 간단하게 표현할 수 있는 특징이 있습니다.
  $`\sigma'(x) = \sigma(x)(1 - \sigma(x))`$

- **증명 (몫의 법칙과 연쇄 법칙 사용):**
  1.  $`\sigma(x) = (1 + e^{-x})^{-1}`$ 로 씁니다.
  2.  연쇄 법칙을 적용합니다: $`\sigma'(x) = -1(1 + e^{-x})^{-2} \cdot (e^{-x})'`$
  3.  $`(e^{-x})'`$는 다시 연쇄 법칙에 의해 $`e^{-x} \cdot (-1) = -e^{-x}`$ 입니다.
  4.  $`\sigma'(x) = -1(1 + e^{-x})^{-2} \cdot (-e^{-x}) = \frac{e^{-x}}{(1 + e^{-x})^2}`$
  5.  이 식을 다음과 같이 변형합니다:
      $`\sigma'(x) = \frac{1 + e^{-x} - 1}{(1 + e^{-x})^2} = \frac{1 + e^{-x}}{(1 + e^{-x})^2} - \frac{1}{(1 + e^{-x})^2}`$
      $`= \frac{1}{1 + e^{-x}} - (\frac{1}{1 + e^{-x}})^2`$
  6.  $`\sigma(x)`$로 치환하면:
      $`\sigma'(x) = \sigma(x) - \sigma(x)^2 = \sigma(x)(1 - \sigma(x))`$

- **특징:**
  - 미분값의 최댓값은 $`x=0`$일 때 0.25입니다.
  - $`x`$의 절대값이 커질수록 미분값이 0에 가까워지는 **그래디언트 소실(Vanishing Gradient)** 문제가 발생하여, 깊은 신경망에서의 학습을 어렵게 할 수 있습니다.

---

### 2. ReLU (Rectified Linear Unit)

ReLU는 현대 딥러닝에서 가장 널리 사용되는 활성화 함수 중 하나입니다. 단순한 형태와 빠른 계산 속도, 그래디언트 소실 문제 완화 등의 장점을 가집니다.

- **함수:** $`\text{ReLU}(x) = \max(0, x)`$
  - $`f(x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \le 0 \end{cases}`$

- **미분:**
  - $`f'(x) = \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x < 0 \end{cases}`$
  - $`x=0`$에서는 수학적으로 미분 불가능하지만(첨점), 실제 구현에서는 0 또는 1로 정의하여 사용합니다. (Subgradient)

- **특징:**
  - **장점:**
    - 양수 구간에서 미분값이 항상 1이므로, 그래디언트가 소실되지 않고 잘 전파됩니다.
    - 연산이 매우 간단하여 계산 비용이 적습니다.
  - **단점:**
    - 입력값이 음수이면 미분값이 0이 되어, 해당 뉴런의 가중치가 더 이상 업데이트되지 않는 **'죽은 ReLU(Dying ReLU)'** 문제가 발생할 수 있습니다.

---

### 3. 하이퍼볼릭 탄젠트 (Tanh)

Tanh는 시그모이드와 유사하게 S자 곡선을 가지지만, 출력 범위가 -1에서 1 사이입니다.

- **함수:** $`\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}`$
- **미분:** $`\tanh'(x) = 1 - \tanh^2(x)`$
- **특징:** 출력이 0을 중심으로 분포하여(zero-centered) 시그모이드보다 학습 효율이 좋은 경향이 있지만, 그래디언트 소실 문제는 여전히 존재합니다.
