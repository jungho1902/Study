# 파트 10: AI 기반 로봇 응용 (AI-Powered Robotic Applications)

## 10.2. 로봇 매니퓰레이션 (Robot Manipulation)

### 10.2.3. AI 기반 제어 (AI-based Control)

**들어가며**

전통적인 로봇 제어는 정확한 물리 모델과 수학적 계산에 기반하여 정교하게 설계되었습니다. 하지만 현실 세계는 예측 불가능한 변수와 불확실성으로 가득 차 있으며, 모든 상황에 대한 모델을 수립하는 것은 거의 불가능합니다. 유리컵을 잡을 때와 종이컵을 잡을 때 힘을 다르게 조절하는 것처럼, 로봇도 다양한 상황에 '적응'하는 능력이 필요합니다. AI 기반 제어는 바로 이 문제에 대한 해법을 제시합니다. 데이터를 통해 스스로 학습함으로써, 명시적으로 프로그래밍하기 어려운 복잡하고 다양한 매니퓰레이션 기술을 로봇이 습득하게 하는 것입니다.

이 장에서는 대표적인 AI 기반 제어 기법인 **모방 학습(Imitation Learning)**과 **강화학습(Reinforcement Learning)**에 대해 알아봅니다.

---

**1. 모방 학습 (Imitation Learning)**

모방 학습은 '전문가(Expert)의 시연(Demonstration)을 보고 따라 배우는' 방식입니다. 여기서 전문가는 보통 인간 조작자입니다. 인간이 직접 로봇 팔을 움직여 작업을 수행하면, 로봇은 이때의 센서 데이터(관절 각도, 카메라 이미지 등)와 조작 명령(모터 토크 등)의 관계를 학습합니다.

- **주요 알고리즘:**

  - **행동 복제 (Behavior Cloning, BC):**
    - **개념:** 가장 간단한 모방 학습 방식으로, 전문가의 행동을 그대로 복제하는 것을 목표로 합니다. 전문가의 '상황(Observation)'을 입력(X)으로, '행동(Action)'을 정답(Y)으로 하는 지도학습(Supervised Learning) 문제입니다.
    - **예시:** `(카메라 이미지, 관절 각도) -> (모터 토크)`의 관계를 학습하는 딥러닝 모델을 훈련시킵니다.
    - **장점:** 구현이 매우 간단하고 직관적입니다.
    - **단점:**
      - **분포 불일치 (Distribution Mismatch):** 학습 데이터는 전문가의 완벽한 경로만을 포함합니다. 만약 로봇이 작은 실수로 경로를 벗어나면, 한 번도 보지 못한 상황에 놓이게 되어 어떻게 대처해야 할지 몰라 큰 오류로 이어질 수 있습니다.
      - **인과관계 혼동 (Causality Confusion):** 행동의 '결과'가 아닌 '상황'만을 보고 모방하므로, 어떤 행동이 왜 중요한지 근본적인 원인을 학습하지 못합니다.

  - **DAgger (Dataset Aggregation):**
    - **개념:** 행동 복제의 분포 불일치 문제를 해결하기 위해 제안된 알고리즘입니다. '학습 -> 실행 -> 데이터 수집 -> 재학습'의 과정을 반복합니다.
    - **동작:**
      1. 전문가 데이터로 초기 정책(policy)을 학습합니다. (행동 복제와 동일)
      2. 학습된 정책으로 로봇을 실행시켜 궤적을 생성합니다.
      3. 로봇이 경로를 벗어나 마주한 새로운 상황들에 대해, 전문가에게 '이럴 땐 어떻게 해야 하는지' 다시 물어보고 정답(Action) 데이터를 받습니다.
      4. 이 새로운 데이터셋을 기존 데이터셋에 추가(Aggregation)하여 정책을 다시 학습합니다.
    - **장점:** 로봇이 겪을 수 있는 다양한 상황에 대한 대처법을 학습하므로, 행동 복제보다 훨씬 강건한 정책을 만들 수 있습니다.

---

**2. 강화학습 기반 매니퓰레이션 (Reinforcement Learning-based Manipulation)**

강화학습(파트 5 RL 연계)은 명시적인 전문가 시연 없이, 로봇이 스스로 **시행착오(Trial-and-Error)**를 통해 작업을 배우는 방식입니다. 로봇(에이전트)은 주어진 목표를 달성하기 위해 다양한 행동을 시도하고, 그 결과로 받는 **보상(Reward)**을 최대화하는 방향으로 자신의 정책을 점차 개선해 나갑니다.

- **주요 과제:**

  - **희소 보상 문제 (Sparse Reward Problem):**
    - **문제점:** 매니퓰레이션 작업에서는 '성공' 또는 '실패'와 같이, 작업의 마지막 순간에만 보상을 주는 경우가 많습니다. (예: 문고리를 돌려 문을 여는 데 성공했을 때만 +1 보상) 이 경우, 로봇은 우연히 성공하기 전까지 어떤 행동이 올바른 방향인지 전혀 힌트를 얻지 못해 학습이 매우 비효율적이거나 불가능해집니다.
    - **해결책:**
      - **보상 설계 (Reward Shaping):** 최종 목표에 가까워지는 중간 단계에 대해 추가적인 보상을 설계해 줍니다. (예: 문고리에 가까이 갈수록, 문고리를 돌리는 각도가 커질수록 작은 보상 제공)
      - **호기심 기반 탐험 (Curiosity-driven Exploration):** 외부 보상뿐만 아니라, 자신이 예측하지 못한 새로운 상태를 발견하는 것 자체에 '호기심'이라는 내적 보상을 주어 효율적인 탐험을 유도합니다.

  - **Sim-to-Real (시뮬레이션에서 현실로) 전달:**
    - **문제점:** 강화학습은 수백만 번 이상의 시행착오가 필요한데, 이를 실제 로봇으로 수행하면 시간과 비용이 엄청나고 로봇이 파손될 위험이 큽니다. 따라서 대부분 시뮬레이션 환경에서 학습을 진행합니다. 하지만 시뮬레이션 환경과 현실 세계의 물리적 차이(Reality Gap) 때문에, 시뮬레이션에서 완벽하게 학습된 정책이 실제 로봇에서는 잘 동작하지 않는 문제가 발생합니다.
    - **해결책:**
      - **도메인 랜덤화 (Domain Randomization):** 시뮬레이션 환경의 파라미터(물체의 질량, 마찰 계수, 조명 등)를 매번 무작위로 바꾸면서 학습시킵니다. 이를 통해 로봇은 다양한 환경 변화에 강건한 정책을 학습하게 되어, 현실 세계를 '한 번도 보지 못한 새로운 도메인' 중 하나로 여기고 잘 적응하게 됩니다.

**결론**

AI 기반 제어, 특히 모방 학습과 강화학습은 로봇 매니퓰레이션의 패러다임을 바꾸고 있습니다. 모방 학습은 인간의 직관적인 지식을 로봇에게 빠르게 전달하는 효과적인 방법이며, 강화학습은 인간이 해결책을 모르는 문제까지도 로봇이 스스로 풀어낼 가능성을 열어줍니다. Sim-to-Real과 같은 기술적 과제들이 해결되면서, AI 기반 제어는 공장 자동화를 넘어, 예측 불가능한 가정 환경이나 서비스 분야에서 로봇이 활약할 수 있게 하는 핵심 동력이 될 것입니다.
