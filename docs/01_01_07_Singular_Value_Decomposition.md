# 특이값 분해 (Singular Value Decomposition, SVD)

특이값 분해(SVD)는 선형대수에서 가장 중요하고 유용한 행렬 분해(matrix decomposition) 방법 중 하나입니다. SVD는 고유값 분해(eigendecomposition)가 정사각행렬에만 적용될 수 있는 것과 달리, **모든 m x n 크기의 행렬**에 적용할 수 있는 일반적인 방법입니다.

---

### 1. SVD의 정의

모든 m x n 행렬 $`A`$는 다음과 같이 세 개의 행렬의 곱으로 분해될 수 있습니다.

$`A = U \Sigma V^T`$

- **$`U`$**: m x m 크기의 직교 행렬(orthogonal matrix). $`U`$의 열벡터들은 **좌특이벡터(left-singular vectors)**라고 불리며, $`AA^T`$의 고유벡터(eigenvectors)로 구성됩니다. $`U^T U = I`$.

- **$`\Sigma`$** (시그마): m x n 크기의 대각 행렬(diagonal matrix). 주대각선 원소인 $`\sigma_1, \sigma_2, ...`$를 **특이값(singular values)**이라고 하며, 내림차순으로 정렬됩니다 ($`\sigma_1 \ge \sigma_2 \ge ... \ge 0`$). 특이값은 $`A^T A`$ (또는 $`AA^T`$)의 고유값들의 제곱근입니다.

- **$`V`$**: n x n 크기의 직교 행렬. $`V`$의 열벡터들은 **우특이벡터(right-singular vectors)**라고 불리며, $`A^T A`$의 고유벡터로 구성됩니다. $`V^T V = I`$.
  - $`V^T`$는 $`V`$의 전치 행렬입니다.

**기하학적 의미:**
SVD는 임의의 선형 변환 $`A`$를 세 가지 기본적인 변환으로 분해하는 것으로 해석할 수 있습니다.
1.  **회전 (Rotation):** $`V^T`$
2.  **스케일링 (Scaling):** $`\Sigma`$
3.  **또 다른 회전 (Rotation):** $`U`$

즉, 어떤 벡터에 변환 $`A`$를 적용하는 것은 그 벡터를 $`V^T`$로 회전시키고, $`\Sigma`$로 각 축에 따라 스케일링한 다음, 다시 $`U`$로 회전시키는 것과 같습니다.

---

### 2. SVD의 중요성 및 응용

SVD는 데이터 과학 및 머신러닝에서 매우 중요한 역할을 합니다.

- **차원 축소 (Dimensionality Reduction):** 큰 특이값들은 데이터의 분산이 큰, 즉 중요한 정보가 담긴 방향을 나타냅니다. 작은 특이값들과 그에 해당하는 특이벡터들을 제거함으로써, 원본 데이터의 정보를 최소한으로 손실하면서 데이터의 차원을 줄일 수 있습니다. 이는 주성분 분석(PCA)의 핵심 원리입니다.

- **데이터 압축 (Data Compression):** 위와 같은 원리로 이미지나 다른 데이터에서 중요도가 낮은 정보를 제거하여 데이터를 압축할 수 있습니다.

- **추천 시스템 (Recommender Systems):** 사용자-아이템 행렬의 빈 값을 추정하고 사용자의 잠재 선호를 파악하는 데 사용됩니다(잠재 요인 모델).

- **노이즈 제거 (Noise Reduction):** 작은 특이값들은 노이즈에 해당하는 경우가 많아, 이를 제거하여 데이터를 정제할 수 있습니다.

---

## 예제 및 풀이 (Examples and Solutions)

### 예제 1: 이미지 압축의 개념적 예

SVD의 수치적 계산은 복잡하므로, 여기서는 SVD가 어떻게 이미지 압축에 사용될 수 있는지 개념적으로 설명합니다.

**문제:** 512x512 픽셀의 흑백 이미지가 행렬 $`A`$로 표현되어 있다고 가정합시다. SVD를 사용하여 이 이미지 데이터를 어떻게 압축할 수 있는지 그 원리를 설명하시오.

**풀이:**

1.  **SVD 적용:** 먼저, 이미지 행렬 $`A`$ (512x512)에 SVD를 적용합니다.
    $`A = U \Sigma V^T`$
    - $`U`$는 512x512 행렬입니다.
    - $`\Sigma`$는 512x512 대각 행렬이며, 512개의 특이값을 가집니다.
    - $`V^T`$는 512x512 행렬입니다.
    현재 상태에서는 데이터를 전혀 압축하지 않았습니다. $`A`$를 표현하기 위해 $`U, \Sigma, V^T`$ 세 행렬의 모든 원소가 필요합니다.

2.  **저차원 근사 (Low-Rank Approximation):**
    SVD의 핵심은 $`A`$를 여러 개의 '랭크-1(rank-1)' 행렬의 합으로 표현할 수 있다는 점입니다.
    $`A = \sigma_1 \mathbf{u}_1 \mathbf{v}_1^T + \sigma_2 \mathbf{u}_2 \mathbf{v}_2^T + \cdots + \sigma_r \mathbf{u}_r \mathbf{v}_r^T`$
    (여기서 $`r`$은 0이 아닌 특이값의 개수, $`\mathbf{u}_i`$와 $`\mathbf{v}_i`$는 $`U`$와 $`V`$의 열벡터)

    특이값($`\sigma_i`$)은 각 랭크-1 행렬이 원본 이미지 $`A`$에 얼마나 중요한지를 나타내는 '가중치' 역할을 합니다. 일반적으로 특이값은 빠르게 감소하므로, 처음 몇 개의 항이 이미지의 대부분의 정보를 담고 있습니다.

3.  **압축 수행:**
    상위 $`k`$개의 특이값과 그에 해당하는 특이벡터들만 사용하여 원본 행렬 $`A`$를 근사(approximate)합니다. 여기서 $`k`$는 512보다 훨씬 작은 값(예: 50)입니다.
    $`A_k = U_k \Sigma_k V_k^T = \sum_{i=1}^{k} \sigma_i \mathbf{u}_i \mathbf{v}_i^T`$
    - $`A_k`$는 원본 이미지 $`A`$와 매우 유사한 근사 이미지입니다.
    - $`U_k`$: 512x_k_ 행렬
    - $`\Sigma_k`$: _k_x_k_ 행렬
    - $`V_k^T`$: _k_x512 행렬

4.  **저장 공간 비교:**
    - **원본:** 512 x 512 = 262,144개의 숫자를 저장해야 합니다.
    - **압축:** $`U_k, \Sigma_k, V_k^T`$를 저장하면 됩니다. 필요한 숫자의 개수는 (512 x _k_) + _k_ + (k x 512) = 1025_k_ 개입니다.
    - 만약 $`k=50`$이라면, 1025 x 50 = 51,250개의 숫자만 저장하면 되므로, 원본 대비 약 80%의 데이터를 압축하는 효과를 얻습니다.

**결론:** SVD는 이미지의 핵심적인 정보(큰 특이값에 해당하는 부분)만을 남기고 덜 중요한 정보(작은 특이값에 해당하는 부분)를 버려서, 시각적으로는 큰 차이가 없으면서도 저장 공간을 크게 줄이는 데이터 압축을 가능하게 합니다.
