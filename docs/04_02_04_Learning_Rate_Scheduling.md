# 학습률 스케줄링 (Learning Rate Scheduling)

## 1. 학습률 스케줄링의 개념

**학습률(Learning Rate)**은 경사 하강법에서 가중치를 업데이트할 때, 그래디언트 방향으로 얼마나 큰 보폭으로 이동할지를 결정하는 매우 중요한 하이퍼파라미터입니다.

- **학습률이 너무 크면:** 최적해를 지나쳐 발산(diverge)할 수 있습니다.
- **학습률이 너무 작으면:** 학습 속도가 매우 느리고, 지역 최적해(local minimum)에 갇히기 쉽습니다.

**학습률 스케줄링(Learning Rate Scheduling)**은 학습 과정 동안 미리 정해진 계획에 따라 학습률을 동적으로 조절하는 기법입니다. 일반적으로 **학습 초기에는 큰 학습률**을 사용하여 빠르게 최적해 근처로 이동하고, **학습이 진행됨에 따라 학습률을 점차 줄여**가면서 모델이 최적해에 안정적으로 수렴하도록 돕습니다.

## 2. 주요 학습률 스케줄링 방법

### 2.1. 스텝 감쇠 (Step Decay)
- **개념:** 가장 간단한 스케줄링 방법으로, 특정 학습 횟수(epoch)마다 학습률을 일정한 비율(예: 0.1)로 줄입니다.
- **예시:** 30 에포크마다 학습률을 10배씩 줄인다. (0.1 -> 0.01 -> 0.001)
- **장점:** 구현이 간단하고 직관적입니다.
- **단점:** 언제, 얼마나 줄일지에 대한 하이퍼파라미터 설정이 중요합니다.

### 2.2. 지수 감쇠 (Exponential Decay)
- **개념:** 매 스텝(또는 에포크)마다 학습률에 특정 감쇠율(decay rate)을 곱하여 지수적으로 감소시킵니다.
- **수식:** `new_lr = initial_lr * (decay_rate ^ step)`
- **특징:** 학습률이 부드럽게 감소하지만, 스텝 감쇠보다 더 빠르게 줄어들 수 있습니다.

### 2.3. 코사인 어닐링 (Cosine Annealing)
- **개념:** 학습률이 코사인(cosine) 함수 곡선을 따라 부드럽게 감소하고 다시 증가하는 것을 반복하는 방식입니다.
- **동작 방식:**
  - 학습 초기에는 큰 학습률로 시작하여, 코사인 곡선을 따라 점차 최소 학습률까지 부드럽게 감소합니다.
  - 이 과정(주기)이 끝나면 다시 학습률을 초기값으로 "재시작(restart)"하여, 모델이 지역 최적해에서 벗어나 더 나은 최적해를 탐색하도록 유도할 수 있습니다 (SGDR: Stochastic Gradient Descent with Restarts).
- **장점:**
  - 학습률이 부드럽게 변하여 안정적인 수렴을 돕습니다.
  - 재시작 기법을 통해 지역 최적해 탈출을 시도할 수 있어, 모델의 일반화 성능을 높이는 데 효과적입니다.
  - 최근 많은 연구에서 좋은 성능을 보이고 있습니다.

### 2.4. 학습률 워밍업 (Learning Rate Warmup)
- **개념:** 학습 초기에 모델의 가중치가 무작위로 초기화되어 매우 불안정한 상태이므로, 처음부터 큰 학습률을 사용하면 학습이 불안정해질 수 있습니다. 이를 방지하기 위해, **학습 시작 후 몇 번의 에포크 동안은 매우 작은 학습률에서 시작하여 점차 목표 학습률까지 선형적으로 증가**시키는 방법입니다.
- **효과:** 학습 초기의 불안정성을 줄여 모델이 안정적으로 학습을 시작하도록 돕습니다.
- **적용:** 주로 다른 스케줄러(예: 코사인 어닐링)와 함께 사용됩니다. 워밍업이 끝난 후, 메인 스케줄러가 동작을 시작합니다.

## 3. 어떤 스케줄러를 선택해야 하는가?

- **간단한 시작:** **스텝 감쇠**는 이해하고 구현하기 쉬워 좋은 출발점이 될 수 있습니다.
- **안정적이고 좋은 성능:** **코사인 어닐링**과 **학습률 워밍업**을 함께 사용하는 것이 현재 많은 딥러닝 모델에서 표준처럼 사용되며, 안정적이면서도 높은 성능을 보이는 경향이 있습니다.
- **적응형 옵티마이저와의 관계:** Adam과 같은 적응형 옵티마이저는 자체적으로 파라미터별 학습률을 조절하지만, 전역적인 학습률 스케줄링을 함께 사용하면 더 좋은 성능을 얻을 수 있는 경우가 많습니다.

적절한 학습률 스케줄러를 선택하고 하이퍼파라미터를 튜닝하는 것은 성공적인 딥러닝 모델 학습을 위한 중요한 과정입니다.
