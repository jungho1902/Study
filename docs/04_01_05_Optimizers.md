# 옵티마이저 (Optimizers)

## 1. 옵티마이저의 역할

**옵티마이저(Optimizer)**는 손실 함수(Loss Function)의 값을 최소화하기 위해 신경망의 가중치(Weight)와 편향(Bias)을 어떻게 업데이트할지를 결정하는 알고리즘입니다.

역전파 알고리즘이 손실 함수의 그래디언트(기울기)를 계산하면, 옵티마이저는 이 그래디언트를 사용하여 효과적으로 가중치를 수정하는 역할을 담당합니다. 즉, '어떤 방향으로, 얼마나 큰 보폭으로 나아갈 것인가'를 결정합니다. 좋은 옵티마이저를 사용하면 모델을 더 빠르고 안정적으로 수렴시킬 수 있습니다.

## 2. 경사 하강법 (Gradient Descent) 및 변형

### 2.1. (배치) 경사 하강법 (Batch Gradient Descent)
- **개념:** 전체 훈련 데이터셋을 사용하여 한 번에 하나의 그래디언트를 계산하고, 이를 통해 가중치를 업데이트합니다.
- **장점:**
  - 전체 데이터의 정보를 사용하므로, 그래디언트가 안정적이고 전역 최적해(Global Minimum)를 찾을 가능성이 높습니다 (볼록 함수의 경우).
- **단점:**
  - 전체 데이터를 메모리에 올려야 하므로, 데이터셋이 매우 클 경우 메모리 부족 문제가 발생할 수 있습니다.
  - 한 번의 업데이트에 모든 데이터를 계산해야 하므로 시간이 매우 오래 걸립니다.

### 2.2. 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)
- **개념:** 전체 데이터셋이 아닌, **하나의 데이터 샘플**을 무작위로 선택하여 그래디언트를 계산하고 가중치를 업데이트합니다.
- **장점:**
  - 업데이트가 매우 빠릅니다.
  - 그래디언트의 노이즈(Noise)가 심하여, 지역 최적해(Local Minimum)를 탈출하고 더 나은 최적해를 찾을 가능성이 있습니다.
- **단점:**
  - 그래디언트가 매우 불안정하여 수렴 과정이 진동할 수 있고, 정확한 최적해를 찾기 어려울 수 있습니다.

### 2.3. 미니배치 경사 하강법 (Mini-batch Gradient Descent)
- **개념:** (배치) 경사 하강법과 SGD의 절충안으로, 전체 데이터셋을 **미니배치(mini-batch)**라는 작은 그룹으로 나누어 각 미니배치마다 그래디언트를 계산하고 가중치를 업데이트합니다.
- **장점:**
  - SGD보다 그래디언트가 안정적이어서 수렴이 부드럽습니다.
  - 배치 경사 하강법보다 업데이트 속도가 빠릅니다.
  - GPU와 같은 병렬 컴퓨팅 환경에서 효율적으로 동작합니다.
- **현대 딥러닝에서는 'SGD'라고 하면 보통 이 미니배치 경사 하강법을 의미하는 경우가 많습니다.**

---

## 3. SGD의 개선: Momentum

### 3.1. 모멘텀 (Momentum)
- **개념:** 경사 하강법에 **관성(Momentum)**의 개념을 추가한 것입니다. 이전의 그래디언트가 이동했던 방향을 일정 비율 기억하면서 현재 그래디언트와 합쳐 다음 업데이트에 반영합니다.
- **동작 방식:**
  - 현재 그래디언트 방향과 이전 이동 방향이 비슷하면, 이동에 가속도를 붙여 더 빠르게 나아갑니다.
  - 방향이 다르면 속도를 줄여 진동을 억제합니다.
- **장점:**
  - SGD보다 수렴 속도가 빠르고, 진동을 줄여 더 안정적으로 최적해에 도달할 수 있습니다.

### 3.2. 네스테로프 가속 경사 (Nesterov Accelerated Gradient, NAG)
- **개념:** 모멘텀을 한 단계 발전시킨 알고리즘입니다. 모멘텀에 의해 **미리 한 걸음 나아간 위치**에서 그래디언트를 계산하고, 이를 바탕으로 실제 업데이트 방향을 결정합니다.
- **장점:**
  - 모멘텀보다 더 똑똑하게 다음 방향을 예측하므로, 수렴 속도가 더 빠른 경향이 있습니다.

---

## 4. 적응적 학습률 (Adaptive Learning Rate) 옵티마이저

각 파라미터(가중치)마다 다른 학습률을 적용하여 학습을 최적화하는 방법입니다. 자주 업데이트되는 파라미터는 학습률을 작게, 드물게 업데이트되는 파라미터는 학습률을 크게 만듭니다.

### 4.1. Adagrad (Adaptive Gradient)
- **개념:** 각 파라미터에 대해, **학습 과정 동안 변화가 많았던 파라미터는 학습률을 작게 만들고, 변화가 적었던 파라미터는 학습률을 크게** 만듭니다.
- **단점:**
  - 학습이 오래 진행되면 모든 파라미터의 학습률이 0에 가까워져 더 이상 학습이 진행되지 않는 문제가 발생할 수 있습니다.

### 4.2. RMSprop (Root Mean Square Propagation)
- **개념:** Adagrad의 학습률이 계속 감소하는 문제를 해결하기 위해 제안되었습니다. 그래디언트 제곱 값의 **지수 이동 평균(Exponential Moving Average)**을 사용하여 최근의 그래디언트 정보에 더 큰 가중치를 둡니다.
- **장점:**
  - Adagrad의 문제를 해결하여, 학습이 멈추는 현상을 방지합니다.

### 4.3. Adam (Adaptive Moment Estimation)
- **개념:** **RMSprop과 Momentum을 결합**한 것과 같은 방식입니다. 각 파라미터의 그래디언트 지수 이동 평균(모멘텀처럼)과 제곱된 그래디언트의 지수 이동 평균(RMSprop처럼)을 함께 사용합니다.
- **장점:**
  - 각 파라미터마다 적응적인 학습률을 가지면서, 모멘텀처럼 관성을 이용해 빠르게 수렴합니다.
  - 성능이 우수하고 안정적이어서, 현재 가장 널리 사용되는 옵티마이저 중 하나입니다.
  - 하이퍼파라미터 튜닝이 비교적 용이합니다.

---

## 5. 옵티마이저 비교 및 선택 가이드

| 옵티마이저 | 주요 특징 | 장점 | 단점 |
|---|---|---|---|
| **SGD** | 기본 경사 하강법 (미니배치) | 간단함, 메모리 효율적 | 수렴 속도 느림, 진동 가능성 |
| **Momentum** | 이전 그래디언트 방향(관성) 반영 | SGD보다 빠른 수렴, 진동 억제 | 하이퍼파라미터(모멘텀 계수) 추가 |
| **Adagrad** | 각 파라미터별 학습률 조정 (변화량 기반) | 변화가 적은 파라미터 학습 촉진 | 학습이 길어지면 학습률이 0에 수렴 |
| **RMSprop** | Adagrad의 학습률 감소 문제 해결 | Adagrad보다 안정적인 학습 | |
| **Adam** | **Momentum + RMSprop** | 빠르고 안정적, 대부분의 경우 성능 우수 | 복잡한 계산, 메모리 사용량 증가 |

**일반적인 선택 가이드:**
- **Adam**이 대부분의 문제에서 좋은 성능을 보여주므로, **가장 먼저 시도해볼 만한 기본 옵티마이저**입니다.
- 데이터셋이 매우 희소(sparse)한 경우에는 Adagrad나 RMSprop과 같은 적응적 학습률 옵티마이저가 좋은 선택일 수 있습니다.
- 때로는 잘 튜닝된 SGD+Momentum이 Adam보다 더 좋은 일반화 성능을 보일 때도 있습니다.
- 모델과 데이터셋의 특성에 따라 최적의 옵티마이저는 달라질 수 있으므로, 여러 옵티마이저를 실험해보는 것이 좋습니다.
