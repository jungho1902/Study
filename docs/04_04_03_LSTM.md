# LSTM (Long Short-Term Memory)

## 1. LSTM의 등장 배경

기본적인 RNN은 시퀀스가 길어질수록 과거의 정보가 희석되어 사라지는 **장기 의존성 문제(Long-Term Dependency Problem)**를 가집니다. 이는 역전파 시 발생하는 **기울기 소실(Vanishing Gradient)** 문제 때문입니다.

**LSTM(Long Short-Term Memory)**은 이러한 문제를 해결하기 위해 1997년 호크라이터(Hochreiter)와 슈미트후버(Schmidhuber)에 의해 제안된 RNN의 개선된 모델입니다. LSTM은 내부에 **'게이트(Gate)'**라는 독창적인 메커니즘을 도입하여, 중요한 정보는 오래 기억하고 불필요한 정보는 잊어버리는 것을 학습할 수 있습니다.

![LSTM Cell Diagram](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/600px-The_LSTM_cell.png)
*(이미지 출처: 위키미디어)*

## 2. LSTM의 핵심 구조: 셀 상태와 게이트

LSTM의 핵심은 **셀 상태(Cell State)**와 3개의 **게이트(Gate)**입니다.

### 2.1. 셀 상태 (Cell State, $C_t$)
- 셀 상태는 LSTM의 '기억'의 핵심으로, 컨베이어 벨트처럼 신경망 전체를 가로지르며 정보를 전달합니다.
- 이 셀 상태 덕분에 정보가 큰 변화 없이 먼 과거에서 미래까지 쉽게 흘러갈 수 있습니다.
- 게이트들은 이 셀 상태에 정보를 추가하거나 제거하는 역할을 제어합니다.

### 2.2. 게이트 (Gates)
게이트는 정보가 얼마나 통과할지를 결정하는 장치입니다. 각 게이트는 **시그모이드(Sigmoid) 함수**와 **element-wise 곱셈**으로 구성됩니다.
- **시그모이드 함수:** 입력값을 0과 1 사이의 값으로 변환합니다. 0은 '정보를 전혀 통과시키지 말라'는 의미이고, 1은 '정보를 모두 통과시키라'는 의미입니다.

LSTM에는 3가지 주요 게이트가 있습니다.

#### 1) 망각 게이트 (Forget Gate)
- **역할:** 과거의 정보를 얼마나 '잊어버릴지' 결정합니다.
- **동작:** 이전 시점의 은닉 상태($h_{t-1}$)와 현재 시점의 입력($x_t$)을 받아, 시그모이드 함수를 통과시켜 0과 1 사이의 값을 출력합니다. 이 값이 이전 셀 상태($C_{t-1}$)에 곱해집니다.
- **결과:** 값이 0에 가까우면 해당 정보를 잊어버리고, 1에 가까우면 정보를 온전히 기억합니다.
  $$ f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) $$

#### 2) 입력 게이트 (Input Gate)
- **역할:** 현재 시점의 새로운 정보를 셀 상태에 얼마나 '저장할지' 결정합니다.
- **동작:**
  1.  **어떤 정보를 저장할지 결정:** 시그모이드 함수가 어떤 값을 업데이트할지 정합니다 ($i_t$).
  2.  **어떤 새로운 정보를 만들지 결정:** `tanh` 함수가 새로운 후보 정보($\tilde{C}_t$)를 생성합니다.
  3.  두 결과를 곱하여, 실제로 셀 상태에 추가할 새로운 정보를 결정합니다.
  $$ i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) $$
  $$ \tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) $$

#### 3) 출력 게이트 (Output Gate)
- **역할:** 업데이트된 셀 상태($C_t$) 중에서 어떤 정보를 현재 시점의 출력(은닉 상태, $h_t$)으로 내보낼지 결정합니다.
- **동작:**
  1.  **어떤 부분을 출력할지 결정:** 시그모이드 함수가 출력할 부분을 정합니다 ($o_t$).
  2.  **셀 상태 값 변환:** `tanh` 함수를 통과시켜 셀 상태 값을 -1과 1 사이로 변환합니다.
  3.  두 결과를 곱하여 최종 은닉 상태($h_t$)를 계산합니다.
  $$ o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) $$
  $$ h_t = o_t * \tanh(C_t) $$

## 3. 셀 상태 업데이트 과정

최종적으로, 현재 시점의 셀 상태 $C_t$는 다음과 같이 업데이트됩니다.

$$ C_t = f_t * C_{t-1} + i_t * \tilde{C}_t $$

- **$f_t * C_{t-1}$:** 이전 셀 상태($C_{t-1}$)에서 잊어버릴 정보는 잊어버리고(망각 게이트),
- **$i_t * \tilde{C}_t$:** 새로운 정보($\tilde{C}_t$) 중에서 저장할 정보만 저장합니다(입력 게이트).

## 4. LSTM의 효과

이러한 게이트 구조 덕분에 LSTM은 그래디언트가 셀 상태를 통해 효과적으로 전달되도록 하여 **기울기 소실 문제를 크게 완화**합니다. 이를 통해 RNN이 어려움을 겪었던 **장기 의존성 학습**을 성공적으로 수행할 수 있게 되었고, 자연어 처리, 시계열 예측 등 다양한 분야에서 뛰어난 성능을 보이고 있습니다.
