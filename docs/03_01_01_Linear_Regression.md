# 3.1.1. 선형 회귀 (Linear Regression)

**선형 회귀(Linear Regression)**는 지도 학습(Supervised Learning)의 가장 기본적인 알고리즘 중 하나로, **독립 변수(independent variable)**와 **종속 변수(dependent variable)** 사이의 선형 관계를 모델링하는 기법입니다. 즉, 주어진 데이터로부터 가장 잘 맞는 직선(또는 평면)의 방정식을 찾는 과정입니다.

- **독립 변수 (X)**: 예측에 사용되는 입력 데이터 (Feature)
- **종속 변수 (Y)**: 예측하고자 하는 연속적인 값 (Label)

예를 들어, '공부 시간(X)'에 따른 '시험 성적(Y)'을 예측하거나, '집의 평수(X)'에 따른 '집값(Y)'을 예측하는 문제에 사용될 수 있습니다.

## 1. 기본 개념

### 1.1. 가설 함수 (Hypothesis Function)

선형 회귀는 데이터가 선형적인 관계를 가질 것이라고 가정합니다. 이 가정을 수학적인 함수로 표현한 것이 **가설 함수(Hypothesis Function)**이며, 직선의 방정식과 동일합니다.

- **단순 선형 회귀 (Simple Linear Regression)**: 독립 변수가 하나일 때
  > **H(x) = Wx + b**

- **다중 선형 회귀 (Multiple Linear Regression)**: 독립 변수가 여러 개일 때
  > **H(x₁, x₂, ..., xₙ) = W₁x₁ + W₂x₂ + ... + Wₙxₙ + b**

여기서 각 기호는 다음을 의미합니다.
- **W (Weight)**: 가중치, 직선의 기울기. 독립 변수가 종속 변수에 얼마나 영향을 미치는지를 나타냅니다.
- **b (bias)**: 편향, 직선의 y절편. 기본적인 출력값을 조절합니다.

머신러닝 모델의 목표는 주어진 데이터를 가장 잘 설명하는 최적의 **W**와 **b**를 찾는 것입니다.

## 2. 비용 함수 (Cost Function) / 손실 함수 (Loss Function)

최적의 W와 b를 찾기 위해서는 "모델이 얼마나 데이터를 잘 설명하는가"를 측정할 기준이 필요합니다. 이때 사용되는 것이 **비용 함수(Cost Function)**입니다.

비용 함수는 모델의 예측값(H(x))과 실제값(y) 사이의 오차를 측정하는 함수입니다. 선형 회귀에서는 이 오차를 계산하기 위해 주로 **평균 제곱 오차(Mean Squared Error, MSE)**를 사용합니다.

> **Cost(W, b) = (1/m) * Σ [ H(x⁽ⁱ⁾) - y⁽ⁱ⁾ ]²**

- **m**: 전체 데이터의 개수
- **H(x⁽ⁱ⁾)**: i번째 데이터 x에 대한 모델의 예측값
- **y⁽ⁱ⁾**: i번째 데이터 x에 대한 실제 정답 값

MSE는 실제값과 예측값의 차이(오차)를 제곱하여 평균을 낸 것입니다. 오차를 제곱하기 때문에,
- 오차의 크기를 양수로 통일할 수 있습니다.
- 오차가 클수록 패널티를 더 크게 부여하여 모델이 더 민감하게 반응하도록 만듭니다.

**선형 회귀의 목표는 바로 이 비용 함수(Cost)의 값을 최소화하는 W와 b를 찾는 것입니다.**

## 3. 경사 하강법 (Gradient Descent)

**경사 하강법(Gradient Descent)**은 비용 함수를 최소화하는 W와 b를 찾기 위해 사용되는 대표적인 최적화 알고리즘입니다.

비용 함수는 W와 b에 대한 2차 함수(아래로 볼록한 포물선 형태)이므로, 경사 하강법을 통해 전역 최솟값(Global Minimum)을 안정적으로 찾을 수 있습니다.

**동작 원리:**
1. **초기값 설정**: W와 b를 임의의 값으로 초기화합니다. (보통 0으로 설정)
2. **그래디언트 계산**: 현재 위치에서 비용 함수의 **기울기(Gradient)**를 계산합니다. 기울기는 특정 지점에서 함수 값이 가장 가파르게 증가하는 방향을 나타냅니다.
3. **가중치 업데이트**: 기울기의 **반대 방향**으로 W와 b를 아주 조금씩 이동시킵니다. 비용을 줄여야 하므로, 기울기가 가장 가파른 방향의 반대로 움직이는 것입니다.
   > **W := W - α * (∂/∂W) Cost(W, b)**
   > **b := b - α * (∂/∂b) Cost(W, b)**
   - **α (Learning Rate, 학습률)**: W와 b를 업데이트할 때 이동하는 보폭(step)의 크기를 결정합니다. 너무 크면 최솟값을 지나쳐 버릴 수 있고(발산), 너무 작으면 학습 속도가 매우 느려집니다. 적절한 학습률을 설정하는 것이 중요합니다.
4. **반복**: 비용 함수의 값이 더 이상 줄어들지 않거나(수렴), 지정된 횟수만큼 반복할 때까지 2~3번 과정을 계속합니다.

이 과정을 통해 경사 하강법은 점진적으로 비용 함수의 최솟값 지점으로 이동하며, 최종적으로 비용을 최소화하는 최적의 W와 b를 찾게 됩니다.
