# 주요 트랜스포머 기반 모델

트랜스포머 아키텍처의 등장은 **사전 훈련(Pre-training)**과 **미세 조정(Fine-tuning)**이라는 새로운 패러다임을 열었습니다. 대규모의 레이블 없는 텍스트 데이터로 모델을 미리 학습시켜 언어 자체에 대한 깊은 이해를 갖게 한 후, 특정 작업(Task)에 맞는 소량의 데이터로 모델을 추가 학습시켜 높은 성능을 달성하는 방식입니다.

이러한 패러다임을 기반으로 수많은 트랜스포머 모델이 개발되었으며, 그 중 대표적인 모델은 BERT와 GPT입니다.

---

## 1. BERT (Bidirectional Encoder Representations from Transformers)

- **개발사:** 구글 (2018)
- **핵심 아이디어:** 트랜스포머의 **인코더(Encoder)** 구조만을 사용하여, 문장의 **양방향 문맥**을 동시에 학습합니다.
- **사전 훈련 방식:**
  1.  **마스크 언어 모델 (Masked Language Model, MLM):**
      - 입력 문장의 일부 단어를 무작위로 `[MASK]` 토큰으로 바꾼 뒤, 모델이 원래 단어가 무엇이었는지를 예측하도록 학습합니다.
      - 이를 통해 모델은 단어의 양쪽 문맥(왼쪽과 오른쪽)을 모두 고려하여 단어의 의미를 깊이 있게 이해하게 됩니다.
      - 예: "나는 [MASK]에 간다" -> 모델이 "학교"를 예측하도록 학습
  2.  **다음 문장 예측 (Next Sentence Prediction, NSP):**
      - 두 개의 문장을 주고, 두 번째 문장이 첫 번째 문장의 바로 다음에 이어지는 문장인지 아닌지를 맞추도록 학습합니다.
      - 이를 통해 문장 간의 관계를 이해하는 능력을 기릅니다.

- **특징:**
  - **양방향성(Bidirectionality):** MLM 덕분에 진정한 의미의 양방향 문맥 학습이 가능하며, 이는 BERT의 뛰어난 성능의 핵심 원동력입니다.
  - **전이 학습(Transfer Learning)에 강함:** 사전 훈련된 BERT 모델은 분류, 개체명 인식, 질의응답 등 다양한 다운스트림 태스크(Downstream Task)에 미세 조정하여 강력한 성능을 발휘합니다.
  - 주로 문맥을 이해하고 표현을 생성하는 **자연어 이해(Natural Language Understanding, NLU)** 작업에 특화되어 있습니다.

---

## 2. GPT (Generative Pre-trained Transformer)

- **개발사:** OpenAI (2018~)
- **핵심 아이디어:** 트랜스포머의 **디코더(Decoder)** 구조만을 사용하여, **단방향(왼쪽에서 오른쪽) 문맥**을 기반으로 다음 단어를 예측하도록 학습합니다.
- **사전 훈련 방식:**
  - **표준 언어 모델 (Standard Language Model):**
      - 주어진 단어 시퀀스를 바탕으로, 다음에 올 단어를 예측하는 방식으로 학습합니다.
      - 예: "나는 학교에" -> 모델이 "간다"를 예측하도록 학습
- **특징:**
  - **자기회귀(Autoregressive):** 이전에 자신이 생성한 단어를 다음 입력으로 사용하여 순차적으로 텍스트를 생성합니다.
  - **생성 능력(Generative Power):** 이러한 구조 덕분에 매우 자연스럽고 일관성 있는 문장이나 문단을 생성하는 데 매우 뛰어난 능력을 보입니다.
  - GPT-2, GPT-3, GPT-4로 발전하면서 모델의 크기와 데이터 양을 기하급수적으로 늘려, 매우 복잡하고 창의적인 텍스트 생성, 요약, 번역, 코드 생성 등 **자연어 생성(Natural Language Generation, NLG)** 작업에서 전례 없는 성능을 보여주고 있습니다.

---

## 3. BERT vs GPT 비교

| 특징 | BERT | GPT |
|---|---|---|
| **사용 구조** | 인코더 만 | 디코더 만 |
| **방향성** | 양방향 | 단방향 (Left-to-Right) |
| **사전 학습 목적** | 마스킹된 단어 예측 | 다음 단어 예측 |
| **주요 장점** | 문매 이해, 분류 | 텍스트 생성 |
| **적합한 작업** | NLU (감성분석, QA, NER) | NLG (대화, 요약, 번역) |

**예시 비교:**

문장: "The cat sat on the [MASK]."
- **BERT:** 양쪽 문맥을 모두 고려하여 "mat", "chair" 등을 예측
- **GPT:** 왼쪽 문맥만 보고 다음 단어를 순차적으로 생성

---

## 4. T5와 BART: 인코더-디코더 모델

BERT와 GPT가 각각 인코더와 디코더에 집중했다면, 트랜스포머의 전체 구조를 활용하는 모델들도 있습니다.

### 3.1. T5 (Text-to-Text Transfer Transformer)
- **개발사:** 구글 (2019)
- **핵심 아이디어:** 모든 자연어 처리 문제를 **"Text-to-Text"** 형식으로 통일합니다.
- **동작 방식:**
  - 번역, 요약, 분류 등 모든 작업을 "입력 텍스트를 받아 출력 텍스트를 생성하는" 문제로 변환합니다.
  - 예: 분류 문제 -> "classify: 이 영화 정말 재밌다." 라는 텍스트를 입력하면 "positive" 라는 텍스트를 출력하도록 학습.
  - 트랜스포머의 인코더-디코더 구조를 그대로 사용합니다.

### 3.2. BART (Bidirectional and Auto-Regressive Transformers)
- **개발사:** 페이스북 (메타) (2019)
- **핵심 아이디어:** BERT의 양방향 인코더와 GPT의 자기회귀 디코더를 결합한 **Denoising Autoencoder**입니다.
- **사전 훈련 방식:**
  - 원본 텍스트에 노이즈(토큰 삭제, 순서 섞기 등)를 추가하여 손상된 텍스트를 만듭니다.
  - 인코더는 이 손상된 텍스트를 입력받고, 디코더는 원본 텍스트를 복원하도록 학습합니다.
  - 이를 통해 BERT와 GPT의 장점을 모두 취하여, 자연어 이해와 생성 작업 모두에서 좋은 성능을 보입니다.

---

## 5. 최근 대형 언어 모델의 진화

### 5.1. GPT 시리즈의 진화
- **GPT-1 (2018):** 117M 파라미터, 트랜스포머 기반 언어모델의 가능성 입증
- **GPT-2 (2019):** 1.5B 파라미터, 다양한 NLP 작업에서 고품질 텍스트 생성
- **GPT-3 (2020):** 175B 파라미터, 별도 학습 없이 Few-shot 학습로 다양한 작업 수행
- **GPT-4 (2023):** 멀티모달(텍스트 + 이미지) 지원, 더 정교하고 안전한 응답

### 5.2. 기타 주요 모델들
- **PaLM (Google):** 540B 파라미터, 수학 및 코드 생성에 특화
- **LaMDA (Google):** 대화에 특화된 모델
- **ChatGPT/InstructGPT:** 인간 피드백 기반 강화학습(RLHF) 적용

## 6. 모델 선택 가이드

### 6.1. 이해(Understanding) 작업
- **추천 모델:** BERT, RoBERTa, DeBERTa
- **예시:** 감성분석, 문서 분류, 개체명 인식

### 6.2. 생성(Generation) 작업
- **추천 모델:** GPT 시리즈, T5, BART
- **예시:** 대화, 요약, 코드 생성, 창의적 글쓰기

### 6.3. 다역할(Multi-task) 작업
- **추천 모델:** T5, BART, GPT-3/4
- **예시:** 번역, 요약, QA 등 다양한 작업을 하나의 모델로 처리

이러한 다양한 트랜스포머 기반 모델들은 각각의 고유한 강점을 가지고 있으며, 사용자의 요구사항과 작업의 특성에 따라 적절한 모델을 선택하는 것이 중요합니다. 이들은 현대 AI 기술 발전의 핵심적인 역할을 하고 있습니다.
