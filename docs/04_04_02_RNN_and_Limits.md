# RNN의 기본 구조와 한계

## 1. 순환 신경망 (Recurrent Neural Network, RNN)의 기본 구조

**순환 신경망(RNN)**은 순차 데이터(Sequential Data)를 처리하기 위해 특별히 설계된 인공 신경망입니다. RNN의 가장 큰 특징은 내부에 **순환하는(recurrent) 구조**를 가지고 있다는 점입니다.

### 1.1. 동작 원리
RNN은 각 시점(time step)마다 다음과 같은 과정을 거칩니다.

1.  **입력:** 현재 시점 $t$의 입력 데이터 $x_t$와 이전 시점 $t-1$의 **은닉 상태(hidden state)** $h_{t-1}$을 함께 입력으로 받습니다.
2.  **은닉 상태 업데이트:** 두 입력을 사용하여 현재 시점 $t$의 은닉 상태 $h_t$를 계산합니다. 이 은닉 상태는 과거의 정보를 요약하여 담고 있는 '기억'과 같은 역할을 합니다.
    $$ h_t = f_W(h_{t-1}, x_t) $$
    (여기서 $f_W$는 가중치 $W$를 사용하는 비선형 활성화 함수, 예를 들어 `tanh`를 의미합니다.)
3.  **출력:** 계산된 현재 은닉 상태 $h_t$를 사용하여 현재 시점 $t$의 최종 출력 $y_t$를 계산합니다.

이 과정에서 사용되는 가중치($W_{hh}$, $W_{xh}$, $W_{hy}$)는 모든 시점에서 **동일하게 공유**됩니다. 이 덕분에 RNN은 가변적인 길이의 시퀀스를 처리할 수 있습니다.

**시각적 표현:**
- **펼쳐진(Unrolled) 구조:**
  ```
     ... --> h_t-1 --> [RNN Cell] --> h_t --> [RNN Cell] --> h_t+1 --> ...
               ^          |            ^          |
               |          v            |          v
              x_t-1       y_t-1         x_t        y_t
  ```

### 1.2. RNN의 역할
- **정보의 압축:** $h_t$는 시점 0부터 $t$까지의 모든 입력 정보를 요약하여 압축하는 역할을 합니다.
- **문맥 이해:** 이 '기억' 메커니즘을 통해 RNN은 단어의 순서, 시계열의 추세 등 데이터의 문맥(context)을 이해할 수 있습니다.

---

## 2. RNN의 한계: 기울기 소실/폭주 문제

기본적인 RNN 구조는 이론적으로는 강력하지만, 실제로는 중요한 한계를 가지고 있습니다.

### 2.1. 장기 의존성 문제 (Long-Term Dependency Problem)
RNN은 시퀀스가 길어질수록, 즉 시점 간의 거리가 멀어질수록 과거의 정보를 제대로 기억하지 못하는 문제가 있습니다. 예를 들어, "오늘 아침에 구름이 많이 꼈다. ... (긴 문장) ... 그래서 나는 우산을 챙겼다." 라는 문장에서 '우산'이라는 단어는 '구름'이라는 먼 과거의 정보와 관련이 있습니다. 하지만 기본적인 RNN은 이러한 **장기 의존성**을 학습하기 매우 어렵습니다.

### 2.2. 원인: 기울기 소실 및 폭주 (Vanishing & Exploding Gradient)
이 문제의 근본적인 원인은 역전파(Backpropagation) 과정에 있습니다.

- **역전파 과정:** RNN은 시간을 거슬러 역전파(Backpropagation Through Time, BPTT)를 수행합니다. 이 과정에서 동일한 가중치 행렬($W_{hh}$)이 반복적으로 곱해집니다.
- **기울기 소실 (Vanishing Gradient):**
  - 만약 가중치 행렬의 값들이 대부분 1보다 작으면, 역전파 과정에서 그래디언트가 반복적으로 곱해지면서 **점점 0에 가까워집니다.**
  - 이로 인해 먼 과거의 정보까지 그래디언트가 제대로 전달되지 않아, 장기 의존성 학습이 실패하게 됩니다.
- **기울기 폭주 (Exploding Gradient):**
  - 반대로, 가중치 행렬의 값들이 대부분 1보다 크면, 그래디언트가 반복적으로 곱해지면서 **기하급수적으로 커져 발산(NaN)**하게 됩니다.
  - 이는 학습을 매우 불안정하게 만듭니다. (기울기 폭주는 '기울기 클리핑(Gradient Clipping)'이라는 기법으로 어느 정도 해결 가능합니다.)

이러한 기울기 소실 문제는 RNN이 장기적인 패턴을 학습하는 것을 방해하는 치명적인 단점입니다. 이 문제를 해결하기 위해 **LSTM(Long Short-Term Memory)**과 **GRU(Gated Recurrent Unit)**와 같은 개선된 RNN 모델이 제안되었습니다.
