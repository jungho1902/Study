# 3.1.3. 보팅 (Voting) 및 배깅 (Bagging)

**앙상블 학습(Ensemble Learning)**은 단일 모델이 아닌 **여러 개의 모델(분류기)**을 사용하여 예측을 결합함으로써, 더 정확하고 안정적인 최종 모델을 만드는 기법입니다. "집단 지성(Wisdom of the crowd)"의 원리와 유사하게, 개별적인 약한 학습기(Weak Learner)들이 모여 하나의 강력한 학습기(Strong Learner)를 구성합니다.

보팅과 배깅은 대표적인 앙상블 학습 기법입니다.

## 1. 보팅 (Voting)

**보팅(Voting)**은 서로 다른 알고리즘을 가진 여러 개의 분류기 모델을 사용하여, 각 모델의 예측 결과를 투표를 통해 최종 예측으로 결정하는 방식입니다.

예를 들어, 로지스틱 회귀, SVM, k-NN 세 개의 모델을 사용하여 어떤 데이터를 예측할 때, 두 모델이 'A' 클래스로 예측하고 한 모델이 'B' 클래스로 예측했다면, 다수결에 따라 최종 예측은 'A'가 됩니다.

### 1.1. 하드 보팅 (Hard Voting)

- 가장 간단한 투표 방식으로, 각 분류기가 예측한 클래스 레이블을 그대로 받아 **다수결**로 최종 클래스를 결정합니다.
- '직접 투표'라고도 불립니다.

### 1.2. 소프트 보팅 (Soft Voting)

- 각 분류기가 각 클래스에 대한 **예측 확률**을 출력하면, 이 확률들의 평균을 내어 가장 높은 확률을 가진 클래스를 최종 예측으로 결정합니다.
- '간접 투표'라고도 불립니다.
- 일반적으로 분류기들의 성능이 비슷할 때, 소프트 보팅이 하드 보팅보다 더 좋은 성능을 내는 경향이 있습니다. 이는 모델이 예측에 대해 얼마나 확신하는지에 대한 정보를 활용하기 때문입니다.

## 2. 배깅 (Bagging - Bootstrap Aggregating)

**배깅(Bagging)**은 **같은 유형의 알고리즘**을 사용하지만, 훈련 데이터를 무작위로 다르게 샘플링하여 여러 개의 모델을 학습시키는 방식입니다.

배깅은 두 가지 핵심 아이디어로 구성됩니다.

### 2.1. 부트스트랩 (Bootstrap)

- 훈련 데이터셋에서 **중복을 허용하여(복원 추출)** 원본 데이터셋과 동일한 크기의 여러 서브 데이터셋을 만드는 샘플링 방식입니다.
- 예를 들어, 원본 데이터가 {1, 2, 3, 4}일 때, 부트스트랩 샘플은 {2, 4, 2, 1}과 같이 중복된 데이터를 포함할 수 있습니다.
- 이 과정을 통해 각 모델은 서로 약간씩 다른 데이터셋으로 학습하게 되어, 모델 간의 다양성을 확보할 수 있습니다.

### 2.2. 통합 (Aggregating)

- 부트스트랩된 각 데이터셋으로 학습된 여러 모델들의 예측을 하나로 합치는 과정입니다.
- **분류(Classification)** 문제에서는 **투표(Voting)**를 통해 최종 클래스를 결정합니다. (주로 하드 보팅)
- **회귀(Regression)** 문제에서는 각 모델의 예측값의 **평균(Averaging)**을 내어 최종 예측값을 결정합니다.

**핵심 효과**: 배깅은 각 모델의 예측에서 발생하는 **분산(Variance)**을 줄이는 데 매우 효과적입니다. 결정 트리와 같이 분산이 높은(불안정한) 모델에 배깅을 적용하면, 모델이 훈련 데이터의 노이즈에 과대적합되는 것을 방지하고 일반화 성능을 크게 향상시킬 수 있습니다.

**병렬 학습**: 배깅은 각 모델이 독립적으로 학습되므로, 여러 CPU 코어를 사용하여 병렬로 학습을 진행할 수 있다는 장점이 있습니다.

**랜덤 포레스트(Random Forest)**는 배깅을 기반으로 한 결정 트리의 앙상블 모델로, 배깅의 대표적인 예시입니다.
