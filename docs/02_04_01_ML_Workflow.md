# ë¨¸ì‹ ëŸ¬ë‹ ì›Œí¬í”Œë¡œìš° (Machine Learning Workflow)

ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ëŠ” ë‹¨ìˆœíˆ ì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒí•˜ê³  ì½”ë“œ ëª‡ ì¤„ì„ ì‘ì„±í•˜ëŠ” ê²ƒ ì´ìƒì˜ ì²´ê³„ì ì´ê³  ì¢…í•©ì ì¸ ê³¼ì •ì…ë‹ˆë‹¤. ì„±ê³µì ì¸ ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œì˜ ëª…í™•í•œ ì´í•´ë¶€í„° ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œì˜ ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ê¹Œì§€ ì´ì–´ì§€ëŠ” **ìƒëª…ì£¼ê¸°(Lifecycle)**ë¥¼ ê°€ì§‘ë‹ˆë‹¤.

ì´ ì „ì²´ ê³¼ì •ì„ **ë¨¸ì‹ ëŸ¬ë‹ ì›Œí¬í”Œë¡œìš°(ML Workflow)** ë˜ëŠ” **ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸(ML Pipeline)**ì´ë¼ê³  í•˜ë©°, ì´ëŠ” ë°ì´í„° ê³¼í•™ í”„ë¡œì íŠ¸ì˜ ì²´ê³„ì ì¸ ë°©ë²•ë¡ ì„ ì œê³µí•©ë‹ˆë‹¤. ê° ë‹¨ê³„ëŠ” ì„œë¡œ ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°ë˜ì–´ ìˆìœ¼ë©°, ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” ë°˜ë³µì ì´ê³  ìˆœí™˜ì ì¸ íŠ¹ì„±ì„ ë³´ì…ë‹ˆë‹¤.

---

## 1. ë¬¸ì œ ì •ì˜ (Problem Definition)

### 1.1. ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œì˜ ì´í•´

ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ì˜ ì„±ê³µì€ ì²« ë²ˆì§¸ ë‹¨ê³„ì¸ **ë¬¸ì œ ì •ì˜**ì—ì„œ ì‹œì‘ë©ë‹ˆë‹¤. ì´ ë‹¨ê³„ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ í•µì‹¬ ì§ˆë¬¸ë“¤ì— ë‹µí•´ì•¼ í•©ë‹ˆë‹¤:

**í•µì‹¬ ì§ˆë¬¸ë“¤:**
- **ë¬´ì—‡ì„ í•´ê²°í•˜ë ¤ê³  í•˜ëŠ”ê°€?** êµ¬ì²´ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œë‚˜ ëª©í‘œ ì •ì˜
- **í˜„ì¬ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ í•´ê²°í•˜ê³  ìˆëŠ”ê°€?** ê¸°ì¡´ í•´ê²° ë°©ë²•ì˜ í•œê³„ì  íŒŒì•…
- **ë¨¸ì‹ ëŸ¬ë‹ì´ ì •ë§ í•„ìš”í•œê°€?** ë¬¸ì œì˜ ë³µì¡ì„±ê³¼ ë°ì´í„° ê°€ìš©ì„± í‰ê°€
- **ì„±ê³µì„ ì–´ë–»ê²Œ ì¸¡ì •í•  ê²ƒì¸ê°€?** ëª…í™•í•œ í‰ê°€ ê¸°ì¤€ê³¼ KPI ì„¤ì •

### 1.2. ë¨¸ì‹ ëŸ¬ë‹ ë¬¸ì œë¡œ ë³€í™˜

ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œë¥¼ êµ¬ì²´ì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ë¬¸ì œë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤:

**ë¬¸ì œ ìœ í˜• ë¶„ë¥˜:**
- **íšŒê·€ ë¬¸ì œ (Regression):** ì—°ì†ì ì¸ ìˆ˜ì¹˜ ì˜ˆì¸¡ (ì˜ˆ: ì£¼íƒ ê°€ê²©, ë§¤ì¶œ ì˜ˆì¸¡)
- **ë¶„ë¥˜ ë¬¸ì œ (Classification):** ë²”ì£¼í˜• ê²°ê³¼ ì˜ˆì¸¡ (ì˜ˆ: ì´ë©”ì¼ ìŠ¤íŒ¸ íŒë³„, ì´ë¯¸ì§€ ë¶„ë¥˜)
- **í´ëŸ¬ìŠ¤í„°ë§ (Clustering):** ìœ ì‚¬í•œ ë°ì´í„° ê·¸ë£¹í™” (ì˜ˆ: ê³ ê° ì„¸ë¶„í™”)
- **ê°•í™”í•™ìŠµ (Reinforcement Learning):** ìµœì  í–‰ë™ ì •ì±… í•™ìŠµ (ì˜ˆ: ê²Œì„ AI, ì¶”ì²œ ì‹œìŠ¤í…œ)

### 1.3. í‰ê°€ ì§€í‘œ ì„¤ì •

í”„ë¡œì íŠ¸ì˜ ì„±ê³µì„ ì¸¡ì •í•  ìˆ˜ ìˆëŠ” **ê°ê´€ì ì´ê³  ì •ëŸ‰ì ì¸ ì§€í‘œ**ë¥¼ ë¯¸ë¦¬ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤:

**ê¸°ìˆ ì  ì§€í‘œ ì˜ˆì‹œ:**
- ë¶„ë¥˜: ì •í™•ë„(Accuracy), F1-score, ROC-AUC
- íšŒê·€: RMSE, MAE, R-squared
- í´ëŸ¬ìŠ¤í„°ë§: Silhouette Score, Davies-Bouldin Index

**ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ ì˜ˆì‹œ:**
- ë§¤ì¶œ ì¦ê°€ìœ¨, ë¹„ìš© ì ˆê°ì•¡, ê³ ê° ë§Œì¡±ë„ ê°œì„  ì •ë„

---

## 2. ë°ì´í„° ìˆ˜ì§‘ (Data Collection)

### 2.1. ë°ì´í„° ì†ŒìŠ¤ ì‹ë³„

í•„ìš”í•œ ë°ì´í„°ì˜ ì¢…ë¥˜ì™€ ì¶œì²˜ë¥¼ íŒŒì•…í•˜ê³  ìˆ˜ì§‘ ì „ëµì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤:

**ë°ì´í„° ì†ŒìŠ¤ ìœ í˜•:**
- **ë‚´ë¶€ ë°ì´í„°:** ê¸°ì—… ë°ì´í„°ë² ì´ìŠ¤, ë¡œê·¸ íŒŒì¼, CRM ì‹œìŠ¤í…œ
- **ì™¸ë¶€ ë°ì´í„°:** ê³µê°œ ë°ì´í„°ì…‹, API, ì›¹ í¬ë¡¤ë§
- **ì‹¤ì‹œê°„ ë°ì´í„°:** ì„¼ì„œ ë°ì´í„°, ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°
- **ë¼ë²¨ë§ ë°ì´í„°:** ìˆ˜ë™ìœ¼ë¡œ ìƒì„±ëœ ì •ë‹µ ë°ì´í„°

### 2.2. ë°ì´í„° í’ˆì§ˆ ê³ ë ¤ì‚¬í•­

**ë°ì´í„° í’ˆì§ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- **ì™„ì •ì„±(Completeness):** ëˆ„ë½ëœ ë°ì´í„°ê°€ ìˆëŠ”ê°€?
- **ì •í™•ì„±(Accuracy):** ë°ì´í„°ê°€ ì‹¤ì œë¥¼ ì •í™•íˆ ë°˜ì˜í•˜ëŠ”ê°€?
- **ì¼ê´€ì„±(Consistency):** ë°ì´í„° í˜•ì‹ì´ í†µì¼ë˜ì–´ ìˆëŠ”ê°€?
- **ì ì‹œì„±(Timeliness):** ë°ì´í„°ê°€ ìµœì‹  ìƒíƒœì¸ê°€?
- **ê´€ë ¨ì„±(Relevance):** í•´ê²°í•˜ë ¤ëŠ” ë¬¸ì œì™€ ê´€ë ¨ì´ ìˆëŠ”ê°€?

---

## 3. íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ë° ì „ì²˜ë¦¬ (EDA & Preprocessing)

ì´ ë‹¨ê³„ëŠ” ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ì—ì„œ **ê°€ì¥ ë§ì€ ì‹œê°„(ë³´í†µ ì „ì²´ì˜ 60-80%)**ì„ ì°¨ì§€í•˜ëŠ” ì¤‘ìš”í•œ ê³¼ì •ì…ë‹ˆë‹¤.

### 3.1. íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (Exploratory Data Analysis, EDA)

**ê¸°ë³¸ ë°ì´í„° íŒŒì•…:**
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ë°ì´í„° ê¸°ë³¸ ì •ë³´ í™•ì¸
df.info()
df.describe()
df.head()

# ê²°ì¸¡ì¹˜ í™•ì¸
missing_data = df.isnull().sum()
print(missing_data[missing_data > 0])

# ë°ì´í„° ë¶„í¬ ì‹œê°í™”
plt.figure(figsize=(15, 10))
for i, column in enumerate(df.select_dtypes(include=[np.number]).columns):
    plt.subplot(3, 3, i+1)
    df[column].hist(bins=30, alpha=0.7)
    plt.title(f'{column} Distribution')
    plt.xlabel(column)
    plt.ylabel('Frequency')
plt.tight_layout()
plt.show()
```

**ìƒê´€ê´€ê³„ ë¶„ì„:**
```python
# ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
correlation_matrix = df.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Feature Correlation Matrix')
plt.show()

# íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„
target_correlation = df.corr()['target'].sort_values(ascending=False)
print("Target variable correlations:")
print(target_correlation)
```

### 3.2. ë°ì´í„° ì „ì²˜ë¦¬ (Data Preprocessing)

**ê²°ì¸¡ì¹˜ ì²˜ë¦¬:**
```python
from sklearn.impute import SimpleImputer, KNNImputer

# ìˆ˜ì¹˜í˜• ë°ì´í„°: í‰ê· /ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
numeric_imputer = SimpleImputer(strategy='mean')
df[numeric_columns] = numeric_imputer.fit_transform(df[numeric_columns])

# ë²”ì£¼í˜• ë°ì´í„°: ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´
categorical_imputer = SimpleImputer(strategy='most_frequent')
df[categorical_columns] = categorical_imputer.fit_transform(df[categorical_columns])

# KNN ê¸°ë°˜ ê²°ì¸¡ì¹˜ ëŒ€ì²´ (ê³ ê¸‰)
knn_imputer = KNNImputer(n_neighbors=5)
df_imputed = knn_imputer.fit_transform(df)
```

**ì´ìƒì¹˜ ì²˜ë¦¬:**
```python
# IQR ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1

# ì´ìƒì¹˜ ë²”ìœ„ ì„¤ì •
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# ì´ìƒì¹˜ ì œê±° ë˜ëŠ” ë³€í™˜
df_cleaned = df[~((df < lower_bound) | (df > upper_bound)).any(axis=1)]

# ë˜ëŠ” Winsorization (ì´ìƒì¹˜ë¥¼ ì„ê³„ê°’ìœ¼ë¡œ ë³€í™˜)
from scipy.stats.mstats import winsorize
df['feature'] = winsorize(df['feature'], limits=[0.05, 0.05])
```

### 3.3. íŠ¹ì„± ê³µí•™ (Feature Engineering)

**ìƒˆë¡œìš´ íŠ¹ì„± ìƒì„±:**
```python
# ë‚ ì§œ/ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ
df['year'] = pd.to_datetime(df['date']).dt.year
df['month'] = pd.to_datetime(df['date']).dt.month
df['day_of_week'] = pd.to_datetime(df['date']).dt.dayofweek

# ìˆ˜ì¹˜í˜• íŠ¹ì„±ì˜ ë³€í™˜
df['price_per_sqft'] = df['price'] / df['square_feet']
df['age'] = 2024 - df['year_built']

# ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ì¡°í•©
df['location_type'] = df['city'] + '_' + df['property_type']

# êµ¬ê°„í™” (Binning)
df['price_range'] = pd.cut(df['price'], 
                          bins=[0, 100000, 300000, 500000, float('inf')],
                          labels=['Low', 'Medium', 'High', 'Luxury'])
```

**íŠ¹ì„± ìŠ¤ì¼€ì¼ë§:**
```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

# í‘œì¤€í™” (í‰ê·  0, í‘œì¤€í¸ì°¨ 1)
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df[numeric_columns])

# ì •ê·œí™” (0-1 ë²”ìœ„)
minmax_scaler = MinMaxScaler()
df_normalized = minmax_scaler.fit_transform(df[numeric_columns])

# ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§ (ì´ìƒì¹˜ì— ê°•í•¨)
robust_scaler = RobustScaler()
df_robust = robust_scaler.fit_transform(df[numeric_columns])
```

**ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©:**
```python
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
import pandas as pd

# Label Encoding (ìˆœì„œí˜• ë²”ì£¼)
label_encoder = LabelEncoder()
df['education_level'] = label_encoder.fit_transform(df['education_level'])

# One-Hot Encoding (ëª…ëª©í˜• ë²”ì£¼)
df_encoded = pd.get_dummies(df, columns=['city', 'category'], drop_first=True)

# ê³ ì¹´ë””ë„ë¦¬í‹° ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬
from category_encoders import TargetEncoder
target_encoder = TargetEncoder()
df['city_encoded'] = target_encoder.fit_transform(df['city'], df['target'])
```

### 3.4. ë°ì´í„° ë¶„í•  (Data Splitting)

```python
from sklearn.model_selection import train_test_split

# ê¸°ë³¸ ë¶„í•  (Train: 60%, Validation: 20%, Test: 20%)
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
)

print(f"Training set: {X_train.shape}")
print(f"Validation set: {X_val.shape}")
print(f"Test set: {X_test.shape}")
```

---

## 4. ëª¨ë¸ ì„ íƒ (Model Selection)

### 4.1. ë¬¸ì œ ìœ í˜•ë³„ ì í•©í•œ ì•Œê³ ë¦¬ì¦˜

**íšŒê·€ ë¬¸ì œ:**
```python
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor

# ë‹¤ì–‘í•œ íšŒê·€ ëª¨ë¸ í›„ë³´
models = {
    'Linear Regression': LinearRegression(),
    'Ridge': Ridge(alpha=1.0),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'XGBoost': XGBRegressor(random_state=42)
}
```

**ë¶„ë¥˜ ë¬¸ì œ:**
```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

# ë‹¤ì–‘í•œ ë¶„ë¥˜ ëª¨ë¸ í›„ë³´
models = {
    'Logistic Regression': LogisticRegression(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(kernel='rbf', random_state=42),
    'Naive Bayes': GaussianNB()
}
```

### 4.2. ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ êµ¬ì¶•

```python
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.metrics import accuracy_score, mean_squared_error

# ë¶„ë¥˜ ë¬¸ì œì˜ ë² ì´ìŠ¤ë¼ì¸
dummy_classifier = DummyClassifier(strategy='most_frequent')
dummy_classifier.fit(X_train, y_train)
dummy_pred = dummy_classifier.predict(X_val)
baseline_accuracy = accuracy_score(y_val, dummy_pred)

print(f"Baseline Accuracy: {baseline_accuracy:.4f}")
print("ëª¨ë“  ëª¨ë¸ì€ ì´ ì„±ëŠ¥ì„ ë„˜ì–´ì•¼ ì˜ë¯¸ê°€ ìˆìŠµë‹ˆë‹¤.")
```

---

## 5. ëª¨ë¸ í›ˆë ¨ (Model Training)

### 5.1. ê¸°ë³¸ ëª¨ë¸ í›ˆë ¨

```python
from sklearn.metrics import classification_report, mean_squared_error, r2_score

# ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ í•¨ìˆ˜
def train_and_evaluate_model(model, X_train, y_train, X_val, y_val, model_name):
    # ëª¨ë¸ í›ˆë ¨
    model.fit(X_train, y_train)
    
    # ì˜ˆì¸¡
    train_pred = model.predict(X_train)
    val_pred = model.predict(X_val)
    
    # í‰ê°€ (ë¶„ë¥˜ì˜ ê²½ìš°)
    train_acc = accuracy_score(y_train, train_pred)
    val_acc = accuracy_score(y_val, val_pred)
    
    print(f"\n{model_name} Results:")
    print(f"Training Accuracy: {train_acc:.4f}")
    print(f"Validation Accuracy: {val_acc:.4f}")
    print(f"Overfitting Check: {train_acc - val_acc:.4f}")
    
    return model, val_pred

# ëª¨ë“  ëª¨ë¸ í›ˆë ¨ ë° ë¹„êµ
results = {}
for name, model in models.items():
    trained_model, predictions = train_and_evaluate_model(
        model, X_train, y_train, X_val, y_val, name
    )
    results[name] = {
        'model': trained_model,
        'predictions': predictions
    }
```

### 5.2. ì•™ìƒë¸” ê¸°ë²•

```python
from sklearn.ensemble import VotingClassifier, StackingClassifier

# Voting ì•™ìƒë¸”
voting_classifier = VotingClassifier(
    estimators=[
        ('rf', RandomForestClassifier(n_estimators=100)),
        ('svc', SVC(probability=True)),
        ('lr', LogisticRegression())
    ],
    voting='soft'  # 'hard' for majority voting
)

voting_classifier.fit(X_train, y_train)
voting_pred = voting_classifier.predict(X_val)
voting_acc = accuracy_score(y_val, voting_pred)

print(f"Voting Ensemble Accuracy: {voting_acc:.4f}")
```

---

## 6. ëª¨ë¸ í‰ê°€ (Model Evaluation)

### 6.1. ë¶„ë¥˜ ëª¨ë¸ í‰ê°€

```python
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score
import matplotlib.pyplot as plt

# í˜¼ë™ í–‰ë ¬ (Confusion Matrix)
cm = confusion_matrix(y_val, val_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

# ë¶„ë¥˜ ë¦¬í¬íŠ¸
print("\nClassification Report:")
print(classification_report(y_val, val_pred))

# ROC Curve (ì´ì§„ ë¶„ë¥˜ì˜ ê²½ìš°)
from sklearn.metrics import roc_curve, auc
fpr, tpr, _ = roc_curve(y_val, model.predict_proba(X_val)[:, 1])
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
```

### 6.2. íšŒê·€ ëª¨ë¸ í‰ê°€

```python
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# íšŒê·€ í‰ê°€ ì§€í‘œ
mae = mean_absolute_error(y_val, val_pred)
mse = mean_squared_error(y_val, val_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_val, val_pred)

print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"R-squared Score: {r2:.4f}")

# ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ì‹œê°í™”
plt.figure(figsize=(10, 6))
plt.scatter(y_val, val_pred, alpha=0.6)
plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values')
plt.show()

# ì”ì°¨ ë¶„ì„ (Residual Analysis)
residuals = y_val - val_pred
plt.figure(figsize=(10, 6))
plt.scatter(val_pred, residuals, alpha=0.6)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.show()
```

---

## 7. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (Hyperparameter Tuning)

### 7.1. Grid Search

```python
from sklearn.model_selection import GridSearchCV

# Random Forest í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì˜ˆì‹œ
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_grid=param_grid,
    cv=5,  # 5-fold cross-validation
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

print(f"Best parameters: {grid_search.best_params_}")
print(f"Best cross-validation score: {grid_search.best_score_:.4f}")

# ìµœì  ëª¨ë¸ë¡œ ì˜ˆì¸¡
best_model = grid_search.best_estimator_
best_pred = best_model.predict(X_val)
best_accuracy = accuracy_score(y_val, best_pred)
print(f"Best model validation accuracy: {best_accuracy:.4f}")
```

### 7.2. Random Search

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# ëœë¤ ì„œì¹˜ë¥¼ ìœ„í•œ í™•ë¥  ë¶„í¬ ì •ì˜
param_distributions = {
    'n_estimators': randint(50, 300),
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10),
    'max_features': ['sqrt', 'log2', None]
}

random_search = RandomizedSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_distributions=param_distributions,
    n_iter=100,  # 100ë²ˆ ë¬´ì‘ìœ„ ìƒ˜í”Œë§
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42,
    verbose=1
)

random_search.fit(X_train, y_train)
print(f"Best random search parameters: {random_search.best_params_}")
```

### 7.3. Bayesian Optimization (ê³ ê¸‰)

```python
# pip install scikit-optimize í•„ìš”
from skopt import BayesSearchCV
from skopt.space import Real, Categorical, Integer

# Bayesian optimizationì„ ìœ„í•œ ê²€ìƒ‰ ê³µê°„ ì •ì˜
search_spaces = {
    'n_estimators': Integer(50, 300),
    'max_depth': Categorical([5, 10, 15, 20, None]),
    'min_samples_split': Integer(2, 20),
    'min_samples_leaf': Integer(1, 10),
    'learning_rate': Real(0.01, 0.3, prior='log-uniform')
}

bayes_search = BayesSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    search_spaces=search_spaces,
    n_iter=50,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42
)

bayes_search.fit(X_train, y_train)
print(f"Best Bayesian optimization parameters: {bayes_search.best_params_}")
```

---

## 8. ëª¨ë¸ ë°°í¬ ë° ëª¨ë‹ˆí„°ë§ (Deployment & Monitoring)

### 8.1. ëª¨ë¸ ì €ì¥ ë° ë²„ì „ ê´€ë¦¬

```python
import joblib
import pickle
from datetime import datetime

# ëª¨ë¸ ì €ì¥
model_filename = f"best_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
joblib.dump(best_model, model_filename)

# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ë„ í•¨ê»˜ ì €ì¥
preprocessing_filename = f"preprocessor_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
joblib.dump(scaler, preprocessing_filename)

# ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì €ì¥
model_metadata = {
    'model_type': 'RandomForestClassifier',
    'features': list(X_train.columns),
    'training_accuracy': train_acc,
    'validation_accuracy': val_acc,
    'hyperparameters': best_model.get_params(),
    'training_date': datetime.now().isoformat()
}

import json
with open(f"model_metadata_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json", 'w') as f:
    json.dump(model_metadata, f, indent=2)
```

### 8.2. ëª¨ë¸ ì„œë¹™ì„ ìœ„í•œ API êµ¬ì¶•

```python
from flask import Flask, request, jsonify
import pandas as pd

app = Flask(__name__)

# ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë¡œë“œ
model = joblib.load('best_model_20241213_143022.pkl')
preprocessor = joblib.load('preprocessor_20241213_143022.pkl')

@app.route('/predict', methods=['POST'])
def predict():
    try:
        # JSON ë°ì´í„° ë°›ê¸°
        data = request.get_json()
        
        # DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame([data])
        
        # ì „ì²˜ë¦¬ ì ìš©
        df_processed = preprocessor.transform(df)
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        prediction = model.predict(df_processed)[0]
        probability = model.predict_proba(df_processed)[0].max()
        
        return jsonify({
            'prediction': int(prediction),
            'confidence': float(probability),
            'status': 'success'
        })
        
    except Exception as e:
        return jsonify({
            'error': str(e),
            'status': 'error'
        }), 400

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({'status': 'healthy'})

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)
```

### 8.3. ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
import numpy as np
from datetime import datetime, timedelta
import matplotlib.pyplot as plt

class ModelMonitor:
    def __init__(self, model, threshold=0.05):
        self.model = model
        self.threshold = threshold
        self.performance_history = []
        self.data_drift_history = []
        
    def log_prediction(self, features, prediction, actual=None):
        """ì˜ˆì¸¡ê³¼ ì‹¤ì œ ê²°ê³¼ë¥¼ ë¡œê¹…"""
        log_entry = {
            'timestamp': datetime.now(),
            'features': features,
            'prediction': prediction,
            'actual': actual
        }
        self.performance_history.append(log_entry)
        
    def calculate_data_drift(self, new_data, reference_data):
        """ë°ì´í„° ë“œë¦¬í”„íŠ¸ ê°ì§€ (ë‹¨ìˆœ í†µê³„ ê¸°ë°˜)"""
        drift_scores = {}
        
        for column in reference_data.columns:
            if reference_data[column].dtype in ['int64', 'float64']:
                # í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ë¹„êµ
                ref_mean = reference_data[column].mean()
                new_mean = new_data[column].mean()
                ref_std = reference_data[column].std()
                
                # ì •ê·œí™”ëœ í‰ê·  ì°¨ì´
                if ref_std > 0:
                    drift_score = abs(new_mean - ref_mean) / ref_std
                    drift_scores[column] = drift_score
                    
        return drift_scores
        
    def check_model_performance(self, recent_days=7):
        """ìµœê·¼ ì„±ëŠ¥ í™•ì¸"""
        recent_data = [
            entry for entry in self.performance_history
            if entry['timestamp'] > datetime.now() - timedelta(days=recent_days)
            and entry['actual'] is not None
        ]
        
        if len(recent_data) < 10:
            print("ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        predictions = [entry['prediction'] for entry in recent_data]
        actuals = [entry['actual'] for entry in recent_data]
        
        accuracy = sum([p == a for p, a in zip(predictions, actuals)]) / len(predictions)
        
        print(f"ìµœê·¼ {recent_days}ì¼ ì„±ëŠ¥: {accuracy:.4f}")
        
        if accuracy < self.threshold:
            print("âš ï¸  ì„±ëŠ¥ ì €í•˜ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸ ì¬í•™ìŠµì„ ê²€í† í•˜ì„¸ìš”.")
            
        return accuracy

# ì‚¬ìš© ì˜ˆì‹œ
monitor = ModelMonitor(best_model, threshold=0.85)

# ìƒˆë¡œìš´ ì˜ˆì¸¡ ë¡œê¹…
new_features = X_test.iloc[0:1]
prediction = model.predict(new_features)[0]
actual = y_test.iloc[0]

monitor.log_prediction(new_features.values[0], prediction, actual)
```

---

## ì˜ˆì œ ë° í’€ì´ (Examples and Solutions)

### ì˜ˆì œ 1: ì£¼íƒ ê°€ê²© ì˜ˆì¸¡ í”„ë¡œì íŠ¸

**ë¬¸ì œ:** ë¶€ë™ì‚° íšŒì‚¬ì—ì„œ ì£¼íƒì˜ íŠ¹ì„±ì„ ë°”íƒ•ìœ¼ë¡œ ê°€ê²©ì„ ì˜ˆì¸¡í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ë‹¤ìŒ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì™„ì „í•œ ë¨¸ì‹ ëŸ¬ë‹ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬í˜„í•´ë³´ì„¸ìš”.

**ë°ì´í„° íŠ¹ì„±:**
- `sqft_living`: ê±°ì£¼ ë©´ì  (í‰ë°©í”¼íŠ¸)
- `bedrooms`: ì¹¨ì‹¤ ê°œìˆ˜
- `bathrooms`: ìš•ì‹¤ ê°œìˆ˜
- `floors`: ì¸µ ìˆ˜
- `condition`: ì§‘ ìƒíƒœ (1-5ì )
- `grade`: ê±´ì¶• ë“±ê¸‰ (1-13ì )
- `yr_built`: ê±´ì¶• ì—°ë„
- `price`: ê°€ê²© (ì˜ˆì¸¡ ëŒ€ìƒ)

**í’€ì´:**

**1ë‹¨ê³„: ë¬¸ì œ ì •ì˜**
```python
# ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œ: ì£¼íƒ ê°€ê²©ì„ ì •í™•íˆ ì˜ˆì¸¡í•˜ì—¬ ì ì • ê°€ê²© ì±…ì •
# ML ë¬¸ì œ: íšŒê·€ ë¬¸ì œ (ì—°ì†ì ì¸ ê°€ê²© ì˜ˆì¸¡)
# ì„±ê³µ ì§€í‘œ: RMSE < $50,000, RÂ² > 0.85
```

**2ë‹¨ê³„: ë°ì´í„° ìˆ˜ì§‘ ë° íƒìƒ‰**
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ)
np.random.seed(42)
n_samples = 1000

data = {
    'sqft_living': np.random.normal(2000, 500, n_samples),
    'bedrooms': np.random.randint(1, 6, n_samples),
    'bathrooms': np.random.normal(2.5, 0.8, n_samples),
    'floors': np.random.choice([1, 1.5, 2, 2.5, 3], n_samples),
    'condition': np.random.randint(1, 6, n_samples),
    'grade': np.random.randint(3, 14, n_samples),
    'yr_built': np.random.randint(1950, 2024, n_samples)
}

# ê°€ê²© ìƒì„± (íŠ¹ì„±ë“¤ê³¼ ì„ í˜• ê´€ê³„ + ë…¸ì´ì¦ˆ)
price = (
    data['sqft_living'] * 150 +
    data['bedrooms'] * 10000 +
    data['bathrooms'] * 15000 +
    data['floors'] * 20000 +
    data['condition'] * 25000 +
    data['grade'] * 30000 +
    (data['yr_built'] - 1950) * 1000 +
    np.random.normal(0, 50000, n_samples)
)

data['price'] = np.abs(price)  # ê°€ê²©ì€ ì–‘ìˆ˜
df = pd.DataFrame(data)

# ê¸°ë³¸ ë°ì´í„° íƒìƒ‰
print("=== ë°ì´í„° ê¸°ë³¸ ì •ë³´ ===")
print(df.info())
print("\n=== ê¸°ìˆ  í†µê³„ ===")
print(df.describe())

# íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬ í™•ì¸
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.hist(df['price'], bins=30, alpha=0.7, color='skyblue')
plt.title('Price Distribution')
plt.xlabel('Price ($)')
plt.ylabel('Frequency')

plt.subplot(1, 2, 2)
plt.hist(np.log(df['price']), bins=30, alpha=0.7, color='lightcoral')
plt.title('Log(Price) Distribution')
plt.xlabel('Log(Price)')
plt.ylabel('Frequency')
plt.tight_layout()
plt.show()
```

**3ë‹¨ê³„: íŠ¹ì„± ê³µí•™ ë° ì „ì²˜ë¦¬**
```python
# ìƒˆë¡œìš´ íŠ¹ì„± ìƒì„±
df['house_age'] = 2024 - df['yr_built']
df['price_per_sqft'] = df['price'] / df['sqft_living']
df['bedroom_bathroom_ratio'] = df['bedrooms'] / (df['bathrooms'] + 0.1)  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€

# ì´ìƒì¹˜ ì œê±° (IQR ë°©ë²•)
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

df_clean = remove_outliers(df, 'price')
print(f"ì´ìƒì¹˜ ì œê±° í›„ ë°ì´í„° í¬ê¸°: {df_clean.shape[0]} (ì œê±°ëœ í–‰: {df.shape[0] - df_clean.shape[0]})")

# íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
features = ['sqft_living', 'bedrooms', 'bathrooms', 'floors', 'condition', 'grade', 'house_age']
X = df_clean[features]
y = df_clean['price']

# ë°ì´í„° ë¶„í• 
from sklearn.model_selection import train_test_split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

print(f"í›ˆë ¨ ì„¸íŠ¸: {X_train.shape}")
print(f"ê²€ì¦ ì„¸íŠ¸: {X_val.shape}")
print(f"í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {X_test.shape}")

# íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)
```

**4ë‹¨ê³„: ëª¨ë¸ ì„ íƒ ë° í›ˆë ¨**
```python
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# ì—¬ëŸ¬ ëª¨ë¸ ì •ì˜
models = {
    'Linear Regression': LinearRegression(),
    'Ridge': Ridge(alpha=1.0),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)
}

# ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€
results = {}

for name, model in models.items():
    # í›ˆë ¨
    if 'Linear' in name or 'Ridge' in name:
        model.fit(X_train_scaled, y_train)
        train_pred = model.predict(X_train_scaled)
        val_pred = model.predict(X_val_scaled)
    else:
        model.fit(X_train, y_train)
        train_pred = model.predict(X_train)
        val_pred = model.predict(X_val)
    
    # í‰ê°€
    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))
    val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))
    train_r2 = r2_score(y_train, train_pred)
    val_r2 = r2_score(y_val, val_pred)
    
    results[name] = {
        'train_rmse': train_rmse,
        'val_rmse': val_rmse,
        'train_r2': train_r2,
        'val_r2': val_r2,
        'overfitting': train_rmse - val_rmse
    }
    
    print(f"\n=== {name} ===")
    print(f"Training RMSE: ${train_rmse:,.2f}")
    print(f"Validation RMSE: ${val_rmse:,.2f}")
    print(f"Training RÂ²: {train_r2:.4f}")
    print(f"Validation RÂ²: {val_r2:.4f}")
    print(f"Overfitting (ì°¨ì´): ${train_rmse - val_rmse:,.2f}")

# ìµœì  ëª¨ë¸ ì„ íƒ
best_model_name = min(results.keys(), key=lambda x: results[x]['val_rmse'])
print(f"\nğŸ† ìµœì  ëª¨ë¸: {best_model_name}")
```

**5ë‹¨ê³„: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**
```python
from sklearn.model_selection import GridSearchCV

# Random Forest íŠœë‹ (ì˜ˆì‹œ)
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10]
}

rf_model = RandomForestRegressor(random_state=42)
grid_search = GridSearchCV(
    rf_model, param_grid, 
    cv=5, scoring='neg_root_mean_squared_error',
    n_jobs=-1, verbose=1
)

grid_search.fit(X_train, y_train)
print(f"ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
print(f"ìµœì  CV ì ìˆ˜: ${-grid_search.best_score_:,.2f} RMSE")

# ìµœì  ëª¨ë¸ë¡œ ìµœì¢… ì˜ˆì¸¡
best_rf_model = grid_search.best_estimator_
test_pred = best_rf_model.predict(X_test)
test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
test_r2 = r2_score(y_test, test_pred)

print(f"\n=== ìµœì¢… í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ===")
print(f"Test RMSE: ${test_rmse:,.2f}")
print(f"Test RÂ²: {test_r2:.4f}")

# ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ í™•ì¸
if test_rmse < 50000 and test_r2 > 0.85:
    print("âœ… ëª©í‘œ ì„±ëŠ¥ ë‹¬ì„±!")
else:
    print("âŒ ëª©í‘œ ì„±ëŠ¥ ë¯¸ë‹¬ì„±. ì¶”ê°€ ê°œì„  í•„ìš”.")
```

**6ë‹¨ê³„: ëª¨ë¸ í•´ì„**
```python
# íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
feature_importance = pd.DataFrame({
    'feature': features,
    'importance': best_rf_model.feature_importances_
}).sort_values('importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(data=feature_importance, x='importance', y='feature')
plt.title('Feature Importance')
plt.xlabel('Importance Score')
plt.tight_layout()
plt.show()

print("=== íŠ¹ì„± ì¤‘ìš”ë„ ìˆœìœ„ ===")
for i, row in feature_importance.iterrows():
    print(f"{row['feature']}: {row['importance']:.4f}")
```

**7ë‹¨ê³„: ëª¨ë¸ ë°°í¬ ì¤€ë¹„**
```python
import joblib

# ìµœì¢… ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì €ì¥
joblib.dump(best_rf_model, 'house_price_model.pkl')
joblib.dump(scaler, 'house_price_scaler.pkl')

# ì˜ˆì¸¡ í•¨ìˆ˜ ì‘ì„±
def predict_house_price(sqft_living, bedrooms, bathrooms, floors, condition, grade, house_age):
    """ì£¼íƒ ê°€ê²© ì˜ˆì¸¡ í•¨ìˆ˜"""
    # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
    input_data = pd.DataFrame({
        'sqft_living': [sqft_living],
        'bedrooms': [bedrooms],
        'bathrooms': [bathrooms],
        'floors': [floors],
        'condition': [condition],
        'grade': [grade],
        'house_age': [house_age]
    })
    
    # ì˜ˆì¸¡ ìˆ˜í–‰
    prediction = best_rf_model.predict(input_data)[0]
    
    return prediction

# í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
sample_price = predict_house_price(2000, 3, 2.5, 2, 4, 8, 10)
print(f"ì˜ˆì¸¡ëœ ì£¼íƒ ê°€ê²©: ${sample_price:,.2f}")
```

**í•´ì„¤:**
ì´ ì˜ˆì œëŠ” ì™„ì „í•œ ë¨¸ì‹ ëŸ¬ë‹ ì›Œí¬í”Œë¡œìš°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤:
1. **ë¬¸ì œ ì •ì˜**: íšŒê·€ ë¬¸ì œë¡œ ëª…í™•íˆ ì •ì˜í•˜ê³  ì„±ê³µ ì§€í‘œ ì„¤ì •
2. **ë°ì´í„° íƒìƒ‰**: EDAë¥¼ í†µí•œ ë°ì´í„° ì´í•´
3. **íŠ¹ì„± ê³µí•™**: ìƒˆë¡œìš´ íŠ¹ì„± ìƒì„± ë° ì „ì²˜ë¦¬
4. **ëª¨ë¸ ì„ íƒ**: ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ í‰ê°€
5. **ìµœì í™”**: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ìœ¼ë¡œ ì„±ëŠ¥ ê°œì„ 
6. **í•´ì„**: íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ìœ¼ë¡œ ëª¨ë¸ ì´í•´
7. **ë°°í¬ ì¤€ë¹„**: ì‹¤ì œ ì‚¬ìš©ì„ ìœ„í•œ í•¨ìˆ˜ ë° ì €ì¥

ê° ë‹¨ê³„ëŠ” ë‹¤ìŒ ë‹¨ê³„ì˜ ê¸°ì´ˆê°€ ë˜ë©°, í•„ìš”ì— ë”°ë¼ ì´ì „ ë‹¨ê³„ë¡œ ëŒì•„ê°€ ê°œì„ í•˜ëŠ” **ë°˜ë³µì  ê³¼ì •**ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

---

## í•µì‹¬ ìš”ì•½ (Key Takeaways)

### ì›Œí¬í”Œë¡œìš°ì˜ íŠ¹ì§•
- **ë°˜ë³µì ì´ê³  ìˆœí™˜ì **: ì„ í˜•ì  ê³¼ì •ì´ ì•„ë‹Œ í”¼ë“œë°±ì„ í†µí•œ ì§€ì†ì  ê°œì„ 
- **ë°ì´í„° ì¤‘ì‹¬**: ì „ì²´ ì‹œê°„ì˜ 60-80%ê°€ ë°ì´í„° ê´€ë ¨ ì‘ì—…ì— ì†Œìš”
- **ì‹¤í—˜ì  ì ‘ê·¼**: ê°€ì„¤ ì„¤ì • â†’ ì‹¤í—˜ â†’ í‰ê°€ â†’ ê°œì„ ì˜ ê³¼í•™ì  ë°©ë²•ë¡ 
- **í˜‘ì—… í•„ìˆ˜**: ë„ë©”ì¸ ì „ë¬¸ê°€, ë°ì´í„° ê³¼í•™ì, ì—”ì§€ë‹ˆì–´ ê°„ ê¸´ë°€í•œ í˜‘ì¡°

### ì„±ê³µ ìš”ì¸
1. **ëª…í™•í•œ ë¬¸ì œ ì •ì˜**: ëª¨í˜¸í•œ ëª©í‘œëŠ” ì‹¤íŒ¨ì˜ ì§€ë¦„ê¸¸
2. **ì–‘ì§ˆì˜ ë°ì´í„°**: "Garbage in, Garbage out"
3. **ì ì ˆí•œ í‰ê°€**: ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œì™€ ì—°ê²°ëœ í‰ê°€ ì§€í‘œ
4. **ì§€ì†ì  ëª¨ë‹ˆí„°ë§**: ë°°í¬ëŠ” ëì´ ì•„ë‹Œ ì‹œì‘
5. **íŒ€ì›Œí¬**: ë‹¤ì–‘í•œ ì—­í• ì˜ ì „ë¬¸ê°€ë“¤ê³¼ì˜ íš¨ê³¼ì  ì†Œí†µ

### ì¼ë°˜ì ì¸ ì‹¤ìˆ˜ì™€ í•´ê²°ì±…
- **ë°ì´í„° ë¦¬í‚¤ì§€**: ë¯¸ë˜ ì •ë³´ê°€ ëª¨ë¸ì— í¬í•¨ë˜ì§€ ì•Šë„ë¡ ì£¼ì˜
- **ê³¼ì í•©**: êµì°¨ ê²€ì¦ê³¼ ì •ê·œí™” ê¸°ë²• í™œìš©
- **í‰ê°€ í¸í–¥**: ì¸µí™” ì¶”ì¶œê³¼ ì ì ˆí•œ ê²€ì¦ ì„¸íŠ¸ êµ¬ì„±
- **ìŠ¤ì¼€ì¼ë§ ëˆ„ë½**: ë‹¤ì–‘í•œ ë²”ìœ„ì˜ íŠ¹ì„±ë“¤ì— ëŒ€í•œ ì •ê·œí™”
- **ë¹„ì¦ˆë‹ˆìŠ¤ ë§¥ë½ ë¬´ì‹œ**: ê¸°ìˆ ì  ì„±ëŠ¥ë§Œì´ ì•„ë‹Œ ì‹¤ì œ ê°€ì¹˜ ê³ ë ¤
